[
  {
    "objectID": "posts/jupyter-test/index.html",
    "href": "posts/jupyter-test/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "import numpy as np\na = np.arange(15).reshape(3, 5)\na\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])"
  },
  {
    "objectID": "posts/jupyter-test/index.html#numpy",
    "href": "posts/jupyter-test/index.html#numpy",
    "title": "Post With Code",
    "section": "",
    "text": "import numpy as np\na = np.arange(15).reshape(3, 5)\na\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])"
  },
  {
    "objectID": "posts/jupyter-test/index.html#matplotlib",
    "href": "posts/jupyter-test/index.html#matplotlib",
    "title": "Post With Code",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nx = np.arange(10)\ny = 2.5 * np.sin(x / 20 * np.pi)\nyerr = np.linspace(0.05, 0.2, 10)\n\nplt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')\nplt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')\nplt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,\n             label='uplims=True, lolims=True')\n\nupperlimits = [True, False] * 5\nlowerlimits = [False, True] * 5\nplt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,\n             label='subsets of uplims and lolims')\n\nplt.legend(loc='lower right')\nplt.show(fig)"
  },
  {
    "objectID": "posts/jupyter-test/index.html#plotly",
    "href": "posts/jupyter-test/index.html#plotly",
    "title": "Post With Code",
    "section": "Plotly",
    "text": "Plotly\n\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport plotly.express as px\nimport plotly.io as pio\ngapminder = px.data.gapminder()\ngapminder2007 = gapminder.query(\"year == 2007\")\nfig = px.scatter(gapminder2007, \n                 x=\"gdpPercap\", y=\"lifeExp\", color=\"continent\", \n                 size=\"pop\", size_max=60,\n                 hover_name=\"country\")\nfig.show()"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#datasets",
    "href": "posts/20230428_inpars/index.html#datasets",
    "title": "InPars Data Augmentation for Information Retrieval using Large Language Models.pdf",
    "section": "4.1 Datasets",
    "text": "4.1 Datasets\n\nMS MARCO\nMicrosoftが出したBingの実際のユーザログをベースとした大型データセット。880万のドキュメントと50万のドキュメントとクエリーのペアがある。各クエリーが平均的に1ドキュメントに対応する。\n\n\nTREC-DL\nMS MARCOと同じドキュメントを持っているが、クエリーは54件のみ、また、各クエリーについてアノテーションしたドキュメントが多い。\n\n\nRobust04\n新聞領域のデータセット。52万のドキュメントがあり、249クエリーがある。各クエリーに対して、平均的に1250件のドキュメントをアノテーションした。\n\n\nNatural Questions\n260万件のWikipediaテキストをベースとしたQuestion Answer データセット。QuestionはGoogleの検索エンジンのログから作ったもの。\n\n\nTREC-COVID\nコロナの情報に関するデータセット"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#training-data-generation",
    "href": "posts/20230428_inpars/index.html#training-data-generation",
    "title": "InPars Data Augmentation for Information Retrieval using Large Language Models.pdf",
    "section": "4.2 Training Data Generation",
    "text": "4.2 Training Data Generation\n各学習データは（Query, Positive document, Negative document）のTripleによって構成させる。\n10万のドキュメントをサンプリングし、GPT3のCurieでQueryを生成させる。最終的にLog probabilityが上位の1万件のペアのみ学習データとして使う。\n生成する際に温度とTopーPのパラメータ設定は結果に影響しない。\n長さが300文字のドキュメントは捨てられる。\nBM25で検索した1000件の中でランダムに1件を抽出し、それをNegative Documentとする。（このやり方で多くのノイズを入れてしまうのでは？）\nQuery生成する際にPromptの書き方：\n一般方法（Vanilla)：\nMS MARCOからランダムに3つのデータを抽出し、それを例として、FewーshotでQueryを生成させる。\nGBQ（Guided by Bad Questions）：\n一般方法と同じように、MS MARCOからランダムに3つのデータを抽出する。しかし、MS MARCOのQueryが漠然すぎるため、それをBad questionとして提示する。Documentを読んで、より関連するGood Questionを手動で作った。（Document, Good Question, Bad Question)で例を提示し、生成したGood QuestionをQueryとする。\n4.3 Retrieval Methods\n２段階の検索にしている。まずBM25で上位1000のドキュメントを取り出す。その次、MonoT5を使ってRerankingをする。\nMonoT5はTransfromerのEncoderとDecoder両方とも使っているモデル。IR領域のCrossーEncoderモデルである。今回はサイズは220Mと3Bのモデルをテストした。\n各データセットで作った偽データでFine-tuningした。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "自然言語処理技術ブログ",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nInPars Data Augmentation for Information Retrieval using Large Language Models.pdf\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "このブログでは、自分が学んだ知識や経験を共有し、皆さんと一緒に成長していくことを目指しています。どうぞよろしくお願いいたします。"
  },
  {
    "objectID": "about.html#自己紹介",
    "href": "about.html#自己紹介",
    "title": "About",
    "section": "自己紹介",
    "text": "自己紹介\n私は2017年から社会人として働き始め、これまでに数々のデータサイエンスプロジェクトを経験してきました。自然言語処理に強い関心を持ち、KaggleやSignateなどでコンペに参加しています。詳細はLinkedInをご覧ください。"
  },
  {
    "objectID": "about.html#このブログの目的",
    "href": "about.html#このブログの目的",
    "title": "About",
    "section": "このブログの目的",
    "text": "このブログの目的\nブログを書く目的は主に以下の3つです。\n\n学んだ知識の理解を深めるため： 学んだことを他人に教えることは、自分自身の理解を深めることに繋がります。自分が理解できていない内容は他人に教えられないため、教えることを通じて自然と理解が深まります。\nプロフィールを充実させるため： 今後自己紹介が必要になった際に、このブログのリンクを共有することで相手に自分の興味や技術スタックを知ってもらえるようになります。\n他人の役に立つ情報を提供するため： これまでインターネット上の無料コンテンツの恩恵を受けてきました。私もコンテンツの消費者だけでなく、提供者としても活躍したいと考えています。"
  }
]