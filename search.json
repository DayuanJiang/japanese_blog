[
  {
    "objectID": "posts/20230430_HyDE/index.html",
    "href": "posts/20230430_HyDE/index.html",
    "title": "HyDE 論文解読 Draft",
    "section": "",
    "text": "論文URL：https://arxiv.org/abs/2204.07496"
  },
  {
    "objectID": "posts/20230430_HyDE/index.html#related-works",
    "href": "posts/20230430_HyDE/index.html#related-works",
    "title": "HyDE 論文解読 Draft",
    "section": "2 Related works",
    "text": "2 Related works"
  },
  {
    "objectID": "posts/20230430_HyDE/index.html#methology",
    "href": "posts/20230430_HyDE/index.html#methology",
    "title": "HyDE 論文解読 Draft",
    "section": "3 Methology",
    "text": "3 Methology"
  },
  {
    "objectID": "posts/20230430_HyDE/index.html#experiments",
    "href": "posts/20230430_HyDE/index.html#experiments",
    "title": "HyDE 論文解読 Draft",
    "section": "4 Experiments",
    "text": "4 Experiments\n仮想なDocumentはInstructGPTで生成した。生成したDocumentをContrieverを用いてEmbeddingに変換した。\nテストのデータとしては、MS-MARCOをベースとしたTREC DL19 DL20があり、BEIRからもLow-resourceのデータセットをいくつ利用した。また、英語以外、韓国語、日本語等データセットも使った。\n\n\n\nweb search query sets\n\n\n\n\n\nlow-resource datasets\n\n\n\n\n\nnon-English retrieval\n\n\n結果を見ると、教師なしの領域でHyDEは全面的に以前のContrieverを超えた。また、教師あるのモデルから比較しても遜色しない精度を出した。"
  },
  {
    "objectID": "posts/20230430_HyDE/index.html#analysis",
    "href": "posts/20230430_HyDE/index.html#analysis",
    "title": "HyDE 論文解読 Draft",
    "section": "5 Analysis",
    "text": "5 Analysis\n\n当たり前だが、仮想なドキュメントを生成するLLMによって最終の精度が違う。また、HyDEは教師なしの手法だが、教師ありのRetrieverの精度も向上できる。"
  },
  {
    "objectID": "posts/20230430_HyDE/index.html#実装",
    "href": "posts/20230430_HyDE/index.html#実装",
    "title": "HyDE 論文解読 Draft",
    "section": "実装",
    "text": "実装\nHyDEはすでにLangChainで実装されている。\nfrom langchain.llms import OpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.chains import HypotheticalDocumentEmbedder\n\nbase_embeddings = OpenAIEmbeddings()\nllm = OpenAI()\n\n# Load with `web_search` prompt\nembeddings = HypotheticalDocumentEmbedder.from_llm(llm, base_embeddings, \"web_search\")\n\n# Now we can use it as any embedding class!\nresult = embeddings.embed_query(\"Where is the Taj Mahal?\")"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html",
    "href": "posts/20230429_promptagator/index.html",
    "title": "Promptagator 論文解読",
    "section": "",
    "text": "論文URL：https://arxiv.org/abs/2209.11755"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#prompt-base-query-generation",
    "href": "posts/20230429_promptagator/index.html#prompt-base-query-generation",
    "title": "Promptagator 論文解読",
    "section": "3.1 Prompt-base query generation",
    "text": "3.1 Prompt-base query generation\nLLMはFLANを利用した。Promptの形式としては、HotpotQAを例として説明すると以下になる。\nEvidence: passage 1 \nVexed question: query 1\n...\nEvidence: passage k\nVexed question: query k\nEvidence: target passage\n下表のようにタスクごとに違うPromptを設定した。（Promptの中の0と 1の意味が不明）\n\nPromptで使用した例は最大8個にし、例の長さによって調整している。文書が長い場合は、必要に応じて切断している。\n各コーパスから最大100万のドキュメントを抽出し、各ドキュメントで8個のQueryを生成している。LLMはFLAN137Bを使った。生成する際に0.7の温度を使った。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#consistency-filtering-using-only-generated-data",
    "href": "posts/20230429_promptagator/index.html#consistency-filtering-using-only-generated-data",
    "title": "Promptagator 論文解読",
    "section": "3.2 Consistency filtering using only generated data",
    "text": "3.2 Consistency filtering using only generated data\n生成したQueryに対して、生成元がその答案を含む必要がある。今までの研究で、その原則で生成したQueryをフィルタリングすることは重要であることがしめされている。\n過去の研究の中で外部の質問応答モデルを用いて実現していたが、この研究では生成したデータで初期のRetrieverを学習されている。各生成したQueryに対して、Retrieverが検索したTopKの中に生成元のドキュメントが含まれていない場合は、そのQueryを除外する。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#few-shot-promptagator-retriever",
    "href": "posts/20230429_promptagator/index.html#few-shot-promptagator-retriever",
    "title": "Promptagator 論文解読",
    "section": "3.3 Few-shot promptagator retriever",
    "text": "3.3 Few-shot promptagator retriever\nDural EncoderのRetrieverを利用している。ベースモデルはT5で、それをC4（Common Crawlのweb crawlコーパス）データセットを使って、Contriverが使用したindependent cropping taskでさらに学習させた。（independent cropping taskとは、同じ文書の異なる部分のペアをPositive example、異なる文書のテキストのペアをNegative exampleとして、教師なしでRetrieverを学習する手法）\nその後、生成されたQueryとDocumentのペアを使って継続的に学習させる。学習時にBatch内のQueryとDocumentのペアをシャッフルしてNegative exampleとする。また、一定のStep数を学習した後、それを初期のRetrieverとして生成されたQueryのフィルタリングを行う。フィルタリングした後、継続的に学習させる。\nまた、Promptagator++というRerankerも提案した。学習データがRetrieverと同じだが、モデルはもっと精度が高く、推論時間が長いCross-attention modelを使った。Retrieverから取得した上位の200件のDocumentから31個のDocumentをサンプリングして、Negative exampleとして使っている。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#zero-shot-promptagator-retriever",
    "href": "posts/20230429_promptagator/index.html#zero-shot-promptagator-retriever",
    "title": "Promptagator 論文解読",
    "section": "3.4 Zero-shot promptagator retriever",
    "text": "3.4 Zero-shot promptagator retriever\nZero-shotでQueryを生成する場合は以下の形式でPromptを書いた：\nf'{d} Read the passage and generate a query.'"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#implementation",
    "href": "posts/20230429_promptagator/index.html#implementation",
    "title": "Promptagator 論文解読",
    "section": "4.1 Implementation",
    "text": "4.1 Implementation\nQueryを生成する際に温度を0.7にした。\n生成したQueryをフィルタリングする際にKを1にした。\nDual Encodersは同じT5-base v1.1 のEncoder(110M)を使っている。Encoderのトップの層を平均し、768次元のEmbeddingへ投影した。\nPromptagator++のRerankerもT5-base v1.1 のEncoder(110M)を使っているが、Cross AttentionのEncoderにしている。\nFine-tuningする際にの具体的なBatch sizeとStepsが下表の通り：\n\n\n\nModel type\nDataset size\nBatch size\nFine tune steps\n\n\n\n\nDual encoder\nBig(&gt;500k)\n6k\n5k\n\n\nDual encoder\nSmall(&lt;=500k)\n128\n1k\n\n\nReranker\nBig(&gt;500k)\n64\n20k\n\n\nReranker\nSmall(&lt;=500k)\n64\n5k"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#main-results",
    "href": "posts/20230429_promptagator/index.html#main-results",
    "title": "Promptagator 論文解読",
    "section": "4.2 Main Results",
    "text": "4.2 Main Results\n\n表の前半ではRetrieverの比較が行われている。Zero-shotのPromptagatorはすでに大多数のMS MARCOでFine-tuningした教師ありのRetrieverと同等な精度を出している。Few-shotのPromptagatorはさらに教師ありのRetrieverより高い精度を出している。\n後半ではRetriever+Rerankerの組み合わせの比較になる。Retrieverと同じ傾向で、Zero-shotでかなり良い精度を出している。Few-shotになるとさらに更に精度が3%向上し、Sotaになっている。\nまた、Promptagatorのもう一つ優れている点はモデルのサイズである。他のモデルは大体3Bの大きさだが、Promptagatorはわずか110Mのみである。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#abalation-study",
    "href": "posts/20230429_promptagator/index.html#abalation-study",
    "title": "Promptagator 論文解読",
    "section": "4.3 Abalation Study",
    "text": "4.3 Abalation Study\n\nQueryフィルタリングの効果 Figure2の左図は、Queryを一回フィルタリングした効果をしめしている。大多数のデータセットにとって、Queryフィルタリングが有効だが、逆効果のものも存在する。NFCorpus and SciFactは小さいデータセットなので、フィルタリングで過学習している可能性がある。\nまた、詳細にフィルタリングされた例をみると、多くケースはQueryは一般化過ぎて多数のドキュメントにマッチングされていること、もしくは単純にQueryが間違っていることがわかる。\n生成したQueryで人間のデータを代替することができるか？ Figure2の真ん中の図は、8-shotのPromptagatorは5万件の人間がラベリングしたデータと同じ効果であることを示している。\nPromptagatorのQuery生成が効いているか？ Figure2の右図のGenQはBEIR論文の中で提案されたモデル、NQ-QGenはこの論文提案した方法でNQデータセット学習したモデル、NQ-QGenとGenQの違いはQuery生成の部分のみ。NQ-QGenの精度は2.7%高いため、提案したQuery生成の方法が有効だと言える。\n\nFLANの影響 PromptagatorのLLMはFLANを利用している。FLANの学習データの中にNQとQuaroデータが含まれいている。その影響を検証するため､それらを除いたデータセットでFLANを学習し、その結果を比較した(さすがGoogle Research、金ならあるの感じ)。その結果、精度は若干低下したが、以前の研究よりは高い精度を達成している。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#qualitative-analysis",
    "href": "posts/20230429_promptagator/index.html#qualitative-analysis",
    "title": "Promptagator 論文解読",
    "section": "4.4 Qualitative Analysis",
    "text": "4.4 Qualitative Analysis\n\nQueryの最初のWordの分布を調査した。Few-shotが生成したQueryの分布は実際のQueryの分布と同じであることがわかる。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#neural-retrieval-models",
    "href": "posts/20230429_promptagator/index.html#neural-retrieval-models",
    "title": "Promptagator 論文解読",
    "section": "Neural retrieval models",
    "text": "Neural retrieval models\nNeural retrieval modelはrepresentation based modelとinteraction based modelの2つに分類できる。\nRepresentation based modelはQueryとDocument両方とも分散表現に変換し、分散表現の類似度で相関性を測っている。最近の研究は以下のことにフォーカスしている：\n\nより良い前学習するタスクやモデルのアーキテクチャを開発する\nMulti-Vectorでより分散表現の表現性能を向上させる\nよりよいNegative sample手法を開発する\nドメイン横断での汎化性能を向上させる\n\nInteraction based modelだと、QueryとDocumentを一緒に処理するため、精度が高い。一方、計算コストが高いため、Rerankerとして使われることが多い。それに関する研究は以下のものがある：\n\nInteraction based modelを蒸留し、Representation based modelとして使う。\nInteractionを最後にさせること。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#prompt-based-query-generation",
    "href": "posts/20230429_promptagator/index.html#prompt-based-query-generation",
    "title": "Promptagator 論文解読",
    "section": "Prompt-based query generation",
    "text": "Prompt-based query generation\nUPR：直接LLMSを使ってRerankを行う\nInPars：GPT3でQueryを生成し、T5のRerankerを学習する"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#retrievers-with-late-interactions",
    "href": "posts/20230429_promptagator/index.html#retrievers-with-late-interactions",
    "title": "Promptagator 論文解読",
    "section": "Retrievers with late interactions",
    "text": "Retrievers with late interactions\nDual encoder modelの効率が良いが、QueryとDocumentの相互作用は最後の内積のみであるため、性能が弱い。ColBERTとSPLADEは最後のToken-level interactionを使ったため、計算コストを少し犠牲して性能を向上させた。"
  },
  {
    "objectID": "posts/20230428_inpars/index.html",
    "href": "posts/20230428_inpars/index.html",
    "title": "InPars 論文解読",
    "section": "",
    "text": "論文URL：https://arxiv.org/abs/2202.05144"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#introduction",
    "href": "posts/20230428_inpars/index.html#introduction",
    "title": "InPars 論文解読",
    "section": "1. Introduction",
    "text": "1. Introduction\nLLMは高性能を誇るものの、情報検索（IR）への応用が制限されている理由として、大規模な計算量が必要であることと、コストが高いことが挙げられる。GPT-3の埋め込みサービスを利用する場合、すべてのテキストを少なくとも1回は処理する必要があり、件数が多い場合、コストが膨大になることが問題となる。\nまた、学習データにも課題が存在し、現存するデータが商用利用に適さないものが多く、また、既存のデータを用いて学習したモデルが他の領域に汎用性を持たないという問題がある。\nこの論文では、検索推論にLLMを直接使用するのではなく、LLMを用いて擬似データ（Pseudo data）を生成し、そのデータを使ってランキングモデルを学習する手法を提案している。"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#related-work",
    "href": "posts/20230428_inpars/index.html#related-work",
    "title": "InPars 論文解読",
    "section": "2. Related work",
    "text": "2. Related work\nこれまで情報検索（IR）領域におけるデータ生成の研究では、BM25を用いて類似度が高いドキュメントをペアとしてモデルを学習する研究が存在する。また、教師ありのモデルでテキストからQueryを生成し、それとテキストのペアを学習データとする研究もあった。\nこの研究の特徴として、モデルを学習させなく、LLMからFew-shotでデータを生成したこと（ちょっと新奇性が足りていない気がする）。"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#our-method-inpars",
    "href": "posts/20230428_inpars/index.html#our-method-inpars",
    "title": "InPars 論文解読",
    "section": "3. Our Method: InPars",
    "text": "3. Our Method: InPars\n以下はInParsのステップ：\n\n複数のドキュメントとクエリーのペアが存在すると仮定する。それを例として、Few-shotでLLMでドキュメントに関連するクエリーを生成してもらう。(\n詳細は4.2で紹介する）\n生成したドキュメントとクエリーのペアについて、LLMが出力する際に出している結果のLog probabilityが上位のものを選択する。ちなみに、このステップが大きく精度を改善した。\n生成したqとdのペアを学習データとしてRerankのモデルをFine-tuningする。（詳細は4.3で紹介する）"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#experimental-setup",
    "href": "posts/20230428_inpars/index.html#experimental-setup",
    "title": "InPars 論文解読",
    "section": "4. Experimental Setup",
    "text": "4. Experimental Setup\n\n4.1 Datasets\n今回使用したデータセットは以下：\n\nMS MARCO：Microsoftが出したBingの実際のユーザログをベースとした大型データセット。880万のドキュメントと50万のドキュメントとクエリーのペアがある。各クエリーが平均的に1ドキュメントに対応する。\nTREC-DL：MS MARCOと同じドキュメントを持っているが、クエリーは54件のみである。また、各クエリーについてアノテーションしたドキュメントが多い。\nRobust04：新聞領域のデータセット。52万のドキュメントがあり、249クエリーがある。各クエリーに対して、平均的に1250件のドキュメントをアノテーションした。\nNatural Questions：260万件のWikipediaテキストをベースとしたQuestion Answer データセット。QuestionはGoogleの検索エンジンのログから作ったもの。\nTREC-COVID：コロナの情報に関するデータセット\n\n\n\n4.2 Training Data Generation\n各学習データは（Query, Positive document, Negative document）のTripleによって構成させる。その生成のステップは以下：\n\n10万のドキュメントをサンプリングし、GPT3のCurieでQueryを生成させる。\n最終的にLog probabilityが上位の1万件のペアのみ学習データとして使う。\nBM25で検索した1000件の中でランダムに1件を抽出し、それをNegative Documentとする。（このやり方で多くのノイズを入れてしまうのでは？）\n\n以下は２点の補足：\n\n生成する際に温度とTopーPのパラメータ設定は結果に有意の影響しない。\n長さが300文字のドキュメントは捨てられる。\n\nQuery生成する際にPromptの書き方は2つを利用した（Figure２)：\n\n一般方法（Vanilla)：MS MARCOからランダムに3つのデータを抽出し、それを例として、FewーshotでQueryを生成させる。\nGBQ（Guided by Bad Questions）：一般方法と同じように、MS MARCOからランダムに3つのデータを抽出する。しかし、MS MARCOのQueryが漠然すぎるため、それをBad questionとして提示する。Documentを読んで、より関連するGood Questionを手動で作った。（Document, Good Question, Bad Question)で例を提示し、生成したGood QuestionをQueryとする。\n\n\n\n\n4.3 Retrieval Methods\n２段階の検索を採用している。まずBM25で上位1000のドキュメントを取り出す。その次、MonoT5を使ってRerankingをする。\nMonoT5はTransformerのEncoderとDecoder両方とも使っているモデルで、Cross-Encoderモデルである。今回の実験では、サイズは220Mと3Bのモデルでテストした。\n各データセットにおいて作成された擬似データでMonoT5をFine-tuningした。"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#results",
    "href": "posts/20230428_inpars/index.html#results",
    "title": "InPars 論文解読",
    "section": "5 Results",
    "text": "5 Results\n\n7,8行目を見ると、BM25やContriever等の以前のUnsupervised結果より優れていることがわかる。また、16行目はMS MARCOでFine-tuningした後さらに擬似データでFine-tuningした結果。幾つかのデータセットで単純にMS MARCOでFine-tuningするより良い結果が出ている。"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#ablation-study-and-analysis",
    "href": "posts/20230428_inpars/index.html#ablation-study-and-analysis",
    "title": "InPars 論文解読",
    "section": "6 Ablation Study and Analysis",
    "text": "6 Ablation Study and Analysis\n6.1 Prompt Selection and Source Corpus\n比較対象が混乱のため、何が言いたいかがわからなかった。\n6.2 Model Size Impact on IR Metrics\n当たり前だけど、Questionを生成するモデルのサイズが大きいほど結果がよくなる。\n6.3 Filtering by the Most Likely Questions\nTop1万件のデータを利用することにより精度が向上した。\n6.4 Was GPT-3 Trained on Supervised IR Data?\n生成したQuestionがどのぐらい実際のMS MARCOデータにあるかで、GPT3の学習データにMS MARCOのデータを含まれているかを検証した。結果としては多くても10％前後のため影響がない。（検証のやり方が正しいかが疑問）"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#conclusion-and-future-work",
    "href": "posts/20230428_inpars/index.html#conclusion-and-future-work",
    "title": "InPars 論文解読",
    "section": "7 Conclusion and Future Work",
    "text": "7 Conclusion and Future Work\n本文は、LLMを利用してQueryを生成して、生成したQueryとドキュメントのペアを学習データでモデルを学習する手法を提案した。\n今後の改善点としては、\n\n擬似データでDense RetrieverをFine-tuningする（今回はRerankerのみ）\nデータを生成する際に作った”BAD question”をNegative exampleとして使用する\n擬似データの数を増やす\n（Query, Document)のペアを探すもっと良い手法を開発する"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "自然言語処理技術ブログ",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nHyDE 論文解読 Draft\n\n\n\n\n\n\n\nNLP\n\n\nInformation_retrieval\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPromptagator 論文解読\n\n\n\n\n\n\n\nNLP\n\n\nInformation_retrieval\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\n\n\n\n\n  \n\n\n\n\nInPars 論文解読\n\n\n\n\n\n\n\nNLP\n\n\nInformation_retrieval\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "このブログでは、自分が学んだ知識や経験を共有し、皆さんと一緒に成長していくことを目指しています。どうぞよろしくお願いいたします。"
  },
  {
    "objectID": "about.html#自己紹介",
    "href": "about.html#自己紹介",
    "title": "About",
    "section": "自己紹介",
    "text": "自己紹介\n私は2017年から社会人として働き始め、これまでに数々のデータサイエンスプロジェクトを経験してきました。自然言語処理に強い関心を持ち、KaggleやSignateなどでコンペに参加しています。詳細はLinkedInをご覧ください。"
  },
  {
    "objectID": "about.html#このブログの目的",
    "href": "about.html#このブログの目的",
    "title": "About",
    "section": "このブログの目的",
    "text": "このブログの目的\nブログを書く目的は主に以下の3つです。\n\n学んだ知識の理解を深めるため： 学んだことを他人に教えることは、自分自身の理解を深めることに繋がります。自分が理解できていない内容は他人に教えられないため、教えることを通じて自然と理解が深まります。\nプロフィールを充実させるため： 今後自己紹介が必要になった際に、このブログのリンクを共有することで相手に自分の興味や技術スタックを知ってもらえるようになります。\n他人の役に立つ情報を提供するため： これまでインターネット上の無料コンテンツの恩恵を受けてきました。私もコンテンツの消費者だけでなく、提供者としても活躍したいと考えています。"
  }
]