[
  {
    "objectID": "draft/20230529_llama_index/index.html",
    "href": "draft/20230529_llama_index/index.html",
    "title": "blog",
    "section": "",
    "text": "# https://gpt-index.readthedocs.io/en/latest/guides/tutorials/terms_definitions_tutorial.html#improvement-2-refining-better-prompts\n\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\nimport pandas as pd\n# https://zenodo.org/record/2783642\nq_df = pd.read_csv(\"Questions.csv\")\na_df = pd.read_csv(\"Answers.csv\")\nprint(\"q_df.shape:\", q_df.shape)\nprint(\"a_df.shape:\", a_df.shape)\nq_df.columns = [c.strip() for c in q_df.columns]\na_df.columns = [c.strip() for c in a_df.columns]\ndf = q_df.merge(a_df, on=\"AID\")\ndf.columns = [\"query\",\"AID\",\"document\"]\n\nq_df.shape: (427, 2)\na_df.shape: (79, 2)\n\n\n\ndisplay(q_df.head(1))\ndisplay(a_df.head(1))\ndisplay(df.head(1))\n\n\n\n\n\n\n\n\nText\nAID\n\n\n\n\n0\n履修している授業で先生が資料をアップロードしているはずだが、コース上に資料が見当たらない。\nA001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAID\nText\n\n\n\n\n0\nA001\n資料が見つからない場合は、以下の点を確認してください。&lt;br&gt;&lt;br&gt;&lt;br&gt;【受講生編】&lt;...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nquery\nAID\ndocument\n\n\n\n\n0\n履修している授業で先生が資料をアップロードしているはずだが、コース上に資料が見当たらない。\nA001\n資料が見つからない場合は、以下の点を確認してください。&lt;br&gt;&lt;br&gt;&lt;br&gt;【受講生編】&lt;...\n\n\n\n\n\n\n\nテストケース - 違うVectorstore - 違うEmbedding手法 - 違うライブラリー\n\nfrom langchain.vectorstores import FAISS, Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom tqdm.auto import tqdm\nembedding = OpenAIEmbeddings()\n\n\n# raw_doc_embeddings = embedding.embed_documents(a_df[\"Text\"])\nraw_query_embeddings = embedding.embed_documents(q_df[\"Text\"])\n\n\nmetadata = a_df[[\"AID\"]].to_dict(orient=\"records\")\n\n\nfaiss_vectorstore = FAISS.from_texts(\n    texts = a_df[\"Text\"].tolist(),\n    embedding=embedding,\n    metadatas=metadata,\n    )\n\nchroma_vectorstore = Chroma.from_texts(\n    texts = a_df[\"Text\"].tolist(),\n    embedding=embedding,\n    metadatas = metadata,\n)\n\nUsing embedded DuckDB without persistence: data will be transient\n\n\n\nquery_list = list(zip(q_df[\"Text\"], q_df[\"AID\"]))\n\n\nDOC_NUM = len(a_df)\ndef get_query_result_rank(txt, aid, vectorstore):\n    search_result = vectorstore.similarity_search(txt, k=DOC_NUM)\n    rank_result = [r.metadata[\"AID\"] for r in search_result]\n    return rank_result.index(aid) + 1\n\ndef mrr(df):\n    return (1 / df[\"rank\"]).mean()\n\ndef recall(df, k):\n    return (df[\"rank\"] &lt;= k).mean()\n\ndef evaluate(query_list, vectorstore, get_rank_func=get_query_result_rank):\n    result_list = []\n    for query, aid, query_embedding in tqdm(query_list):\n        rank = get_rank_func(query, aid, vectorstore)\n        result_list.append((query, rank))\n    result_df = pd.DataFrame(result_list, columns=[\"query\", \"rank\"])\n    return result_df, mrr(result_df), recall(result_df, 1), recall(result_df, 5)\n\n\nfaiss_result = evaluate(query_list, faiss_vectorstore)\nchrome_result = evaluate(query_list, chroma_vectorstore)\n\n\n\n\n\n\n\n\nprint(faiss_result[1:])\nprint(chrome_result[1:])\n\n(0.6853719607667331, 0.550351288056206, 0.8665105386416861)\n(0.6853569484566387, 0.550351288056206, 0.8665105386416861)\n\n\n\nfrom llama_index import GPTVectorStoreIndex, Document\ndocuments = [Document(text=row[\"Text\"], extra_info={\"AID\": row[\"AID\"]}) for _, row in a_df.iterrows()]\nvec_store = GPTVectorStoreIndex.from_documents(documents)\n\n\nquery_engine = vec_store.as_retriever(similarity_top_k=DOC_NUM)\n\n\ndef get_query_result_rank_lmidx(text, aid, vectorstore):\n    search_result = vectorstore.retrieve(text)\n    rank_result = [r.node.extra_info[\"AID\"] for r in search_result]\n    return rank_result.index(aid) + 1\n\n\nlmidx_result = evaluate(query_list, query_engine, get_query_result_rank_lmidx)\n\n\n\n\n\nprint(faiss_result[1:])\nprint(chrome_result[1:])\nprint(lmidx_result[1:])\n\n(0.6853719607667331, 0.550351288056206, 0.8665105386416861)\n(0.6853569484566387, 0.550351288056206, 0.8665105386416861)\n(0.6862287549832872, 0.550351288056206, 0.8501170960187353)\n\n\n\nfrom langchain import ElasticVectorSearch\nfrom langchain.embeddings import OpenAIEmbeddings\n\nembedding = OpenAIEmbeddings()\nelastic_vector_search = ElasticVectorSearch(\nelasticsearch_url=\"http://localhost:9200\",\nindex_name=\"test_index\",\nembedding=embedding\n)\n\n\nelastic_vectostore = ElasticVectorSearch.from_texts(\n    texts = a_df[\"Text\"].tolist(),\n    embedding=embedding,\n    metadatas = metadata,\n    elasticsearch_url=\"http://localhost:9200\",\n)\n\n\nelastic_result = evaluate(query_list, elastic_vectostore, get_query_result_rank)\n\n\n\n\n\nprint(faiss_result[1:])\nprint(chrome_result[1:])\nprint(lmidx_result[1:])\nprint(elastic_result[1:])\n\n(0.6853719607667331, 0.550351288056206, 0.8665105386416861)\n(0.6853569484566387, 0.550351288056206, 0.8665105386416861)\n(0.6862287549832872, 0.550351288056206, 0.8501170960187353)\n(0.6853931924624378, 0.550351288056206, 0.8688524590163934)\n\n\n\nimport requests\n# download japanese stopwords\nurl = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-ja/master/stopwords-ja.txt\"\nr = requests.get(url)\nstopwords = r.text.split(\"\\n\")\n\n\nimport MeCab\n# parser = MeCab.Tagger(\"-Owakati\")\nparser = MeCab.Tagger()\ndef extract_nouns_verbs(text):\n    parser = MeCab.Tagger()\n    parsed_text = parser.parse(text)\n    lines = parsed_text.split('\\n')\n    nouns_verbs = []\n\n    for line in lines:\n        if '名詞' in line or '動詞' in line:\n            parts = line.split('\\t')\n            word = parts[0]\n            if not word.isascii():\n                nouns_verbs.append(word)\n    return nouns_verbs\n\ndef preprocess(text):\n    return [token for token in extract_nouns_verbs(text) if token not in stopwords]\n    # return parser.parse(text).strip().split()\n\ntokenized_texts = [preprocess(text) for text in a_df[\"Text\"]] \n\nfrom rank_bm25 import BM25Okapi\nbm25 = BM25Okapi(tokenized_texts)\n\n\n# for query, aid, embedding in query_list:\n#     tokenized_query = parser.parse(query).strip().split()\n#     rank_result = bm25.get_top_n(tokenized_query,a_df.AID , n=DOC_NUM)\n    \ndef get_query_result_rank(txt, aid, vectorstore):\n    search_result = vectorstore.similarity_search(txt, k=DOC_NUM)\n    rank_result = [r.metadata[\"AID\"] for r in search_result]\n    return rank_result.index(aid) + 1\n\ndef get_query_result_rank_bm25(txt, aid, bm25):\n    tokenized_query =  preprocess(txt)\n    rank_result = bm25.get_top_n(tokenized_query,a_df.AID , n=DOC_NUM)\n    return rank_result.index(aid) + 1\n\n\nbm25_result = evaluate(query_list, bm25, get_query_result_rank_bm25)\n\n\n\n\n\nprint(faiss_result[1:])\nprint(chrome_result[1:])\nprint(lmidx_result[1:])\nprint(elastic_result[1:])\nprint(bm25_result[1:])\n\n(0.6853719607667331, 0.550351288056206, 0.8665105386416861)\n(0.6853569484566387, 0.550351288056206, 0.8665105386416861)\n(0.6862287549832872, 0.550351288056206, 0.8501170960187353)\n(0.6853931924624378, 0.550351288056206, 0.8688524590163934)\n(0.6018858908976351, 0.45901639344262296, 0.7892271662763466)\n\n\n\nprint(faiss_result[1:])\nprint(chrome_result[1:])\nprint(lmidx_result[1:])\nprint(elastic_result[1:])\nprint(bm25_result[1:])\n\n(0.6853719607667331, 0.550351288056206, 0.8665105386416861)\n(0.6853569484566387, 0.550351288056206, 0.8665105386416861)\n(0.6862287549832872, 0.550351288056206, 0.8501170960187353)\n(0.6853931924624378, 0.550351288056206, 0.8688524590163934)\n(0.5582001979359493, 0.4379391100702576, 0.7002341920374707)\n\n\n\nprint((faiss_result[0][\"rank\"] != chrome_result[0][\"rank\"]).sum())\nprint((faiss_result[0][\"rank\"] != lmidx_result[0][\"rank\"]).sum())\nprint((faiss_result[0][\"rank\"] != elastic_result[0][\"rank\"]).sum())\nprint((faiss_result[0][\"rank\"] != bm25_result[0][\"rank\"]).sum())\n\n1\n153\n3\n265\n\n\n\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\nembeddings = HuggingFaceEmbeddings(model_name=\"distiluse-base-multilingual-cased-v2\")\n\n: \n\n\n: \n\n\n\n# save embeddings to pickle\nimport pickle\nwith open(\"embeddings.pkl\", \"wb\") as f:\n    pickle.dump(embeddings, f)\n\n\nfrom langchain.embeddings import TensorflowHubEmbeddings\n\nembedding_tfhub = TensorflowHubEmbeddings()\n\n\nquery_result = embedding_tfhub.embed_query(\"日本語\")\n\nCollecting tensorflow-hub\n  Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.6/100.6 kB 6.3 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.12.0 in /Users/dayuan.jiang/opt/anaconda3/envs/notion-db/lib/python3.9/site-packages (from tensorflow-hub) (1.23.5)\nRequirement already satisfied: protobuf&gt;=3.19.6 in /Users/dayuan.jiang/opt/anaconda3/envs/notion-db/lib/python3.9/site-packages (from tensorflow-hub) (3.20.3)\nInstalling collected packages: tensorflow-hub\nSuccessfully installed tensorflow-hub-0.13.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport torch\n\n\nimport tensorflow_hub\n\n\nconda install nomkl\n\nRetrieving notices: ...working... done\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /Users/dayuan.jiang/opt/anaconda3/envs/notion-db\n\n  added / updated specs:\n    - nomkl\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    blas-1.0                   |         openblas          45 KB\n    certifi-2023.5.7           |   py39hecd8cb5_0         153 KB\n    nomkl-3.0                  |                0          45 KB\n    ------------------------------------------------------------\n                                           Total:         243 KB\n\nThe following NEW packages will be INSTALLED:\n\n  blas               pkgs/main/osx-64::blas-1.0-openblas \n  nomkl              pkgs/main/osx-64::nomkl-3.0-0 \n\nThe following packages will be UPDATED:\n\n  certifi                          2022.12.7-py39hecd8cb5_0 --&gt; 2023.5.7-py39hecd8cb5_0 \n  openssl                                 1.1.1s-hca72f7f_0 --&gt; 1.1.1t-hca72f7f_0 \n\n\n\nDownloading and Extracting Packages\nblas-1.0             | 45 KB     |                                       |   0% \ncertifi-2023.5.7     | 153 KB    |                                       |   0% \n\nblas-1.0             | 45 KB     | ##################################### | 100% \ncertifi-2023.5.7     | 153 KB    | ##################################### | 100% \ncertifi-2023.5.7     | 153 KB    | ##################################### | 100% \n\nnomkl-3.0            | 45 KB     | #############1                        |  35% \n\nnomkl-3.0            | 45 KB     | ##################################### | 100% \n\n                                                                                \n                                                                                \n\n                                                                                \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n\nNote: you may need to restart the kernel to use updated packages."
  },
  {
    "objectID": "posts/20230509_react/index.html",
    "href": "posts/20230509_react/index.html",
    "title": "LangChain Agentの全面解説",
    "section": "",
    "text": "LangChainの中に最もハイレベルな概念としてはAgentです。以前の投稿の中でも話ましたが、LangChainはまだ未熟なライブラリなので、Agentの実装は複雑なものになっていますし、中身の挙動を説明するドキュメントもなかったので、本文ではAgentの使い方から、インプットからアウトプットまでの流れを説明していきます。"
  },
  {
    "objectID": "posts/20230509_react/index.html#reactを例にlangchainのagentを紹介する",
    "href": "posts/20230509_react/index.html#reactを例にlangchainのagentを紹介する",
    "title": "LangChain Agentの全面解説",
    "section": "ReActを例にLangChainのAgentを紹介する",
    "text": "ReActを例にLangChainのAgentを紹介する\nLangChainのAgentとは、簡単に言うとツールを利用できるLLMです。\n典型の例としては「ReAct」が挙げられます。去年出されている「ReAct: Synergizing Reasoning and Acting in Language Models」の論文の中で、思考だけではなく、思考に基づいて行動を起こし、さらに行動の結果から思考を行うLLMsの利用方法を提案した。そのやり方はReasoningとActingの結合なので、「ReAct」と名付けられました。\n実際の例で見ましょう。下記のコードはLangChainで定義したReActのAgentです。このAgentは検索と照応の2つのツールを持っています。人間と同じように、質問が投げられた後、Wikipediaで検査し、検索した結果からコピペー(照応)しながら答案を作ることができます。\n\nfrom langchain import OpenAI, Wikipedia\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.agents import AgentType\nfrom langchain.agents.react.base import DocstoreExplorer\nfrom dotenv import load_dotenv\n# set the environment variables\nload_dotenv()\n\ndocstore=DocstoreExplorer(Wikipedia())\ntools = [\n    Tool(\n        name=\"Search\",\n        func=docstore.search,\n        description=\"useful for when you need to ask with search\"\n    ),\n    Tool(\n        name=\"Lookup\",\n        func=docstore.lookup,\n        description=\"useful for when you need to ask with lookup\"\n    )\n]\n\nllm = OpenAI(temperature=0, model_name=\"text-davinci-003\")\nreact = initialize_agent(tools, llm, agent=AgentType.REACT_DOCSTORE, verbose=True)\n\nクリントンの奥さんが何をしているかを聞いてみましょう。\n\nreact.run(\"What do Bill Clinton's wife do for a living?\")\n\n\n\n&gt; Entering new AgentExecutor chain...\nThought: I need to search Bill Clinton and find his wife, then find what she does for a living.\nAction: Search[Bill Clinton]\nObservation: William Jefferson Clinton (né Blythe III; born August 19, 1946) is an American politician  who served as the 42nd president of the United States from 1993 to 2001. He previously served as governor of Arkansas from 1979 to 1981 and again from 1983 to 1992, and as attorney general of Arkansas from 1977 to 1979. A member of the Democratic Party, Clinton became known as a New Democrat, as many of his policies reflected a centrist \"Third Way\" political philosophy. He is the husband of Hillary Clinton, who was a U.S. senator from New York from 2001 to 2009, secretary of state from 2009 to 2013 and the Democratic nominee for president in the 2016 presidential election.\nClinton was born and raised in Arkansas and attended Georgetown University. He received a Rhodes Scholarship to study at University College, Oxford, and later graduated from Yale Law School. He met Hillary Rodham at Yale; they married in 1975. After graduating from law school, Clinton returned to Arkansas and won election as state attorney general, followed by two non-consecutive tenures as Arkansas governor. As governor, he overhauled the state's education system and served as chairman of the National Governors Association. Clinton was elected president in the 1992 presidential election, defeating incumbent Republican president George H. W. Bush and independent businessman Ross Perot. At 46 years old, he became the third-youngest president of the United States and the first president to be born in the Baby Boomer generation.\nClinton presided over the longest period of peacetime economic expansion in American history. He signed into law the North American Free Trade Agreement (NAFTA) and the Violent Crime Control and Law Enforcement Act, but failed to pass his plan for national health care reform. The Republican Party won unified control of Congress for the first time in 40 years in the 1994 elections, but Clinton was still comfortably re-elected in 1996, becoming the first Democrat since Franklin D. Roosevelt to win a second full term. Starting in the mid-1990s, he began an ideological evolution as he became much more conservative in his domestic policy, advocating for and signing the Personal Responsibility and Work Opportunity Act, the State Children's Health Insurance Program and financial deregulation measures. He appointed Ruth Bader Ginsburg and Stephen Breyer to the U.S. Supreme Court. During the last three years of Clinton's presidency, the Congressional Budget Office reported a budget surplus—the first such surplus since 1969. In foreign policy, Clinton ordered U.S. military intervention in the Bosnian and Kosovo wars, eventually signing the Dayton Peace agreement. He also called for the expansion of NATO in Eastern Europe and many former Warsaw Pact members joined NATO during his presidency. Clinton's foreign policy in the Middle East saw him sign the Iraq Liberation Act which gave aid to groups against Saddam Hussein. He also participated in the Oslo I Accord and Camp David Summit to advance the Israeli–Palestinian peace process, and assisted the Northern Ireland peace process.\nClinton's second term was dominated by the Monica Lewinsky scandal, which began in 1996, when he had a sexual relationship with 22-year-old Monica Lewinsky, an intern at the White House. In January 1998, news of the affair made tabloid headlines. This scandal escalated throughout the year, culminating on December 19 when Clinton was impeached by the House of Representatives, becoming the second U.S. president—the first since Andrew Johnson—to be impeached. The two impeachment articles that the House passed were centered around him using the powers of the presidency to obstruct the investigation and lying under oath. In 1999, Clinton's impeachment trial began in the Senate. He was acquitted on both charges as the Senate failed to cast 67 votes against him, which was necessary to meet the two-thirds conviction threshold prescribed by Article I, section 3, clause 6 of the U.S. Constitution.\nClinton left office in 2001 with the joint-highest approval rating of any U.S. president in the modern era, alongside Franklin D. Roosevelt and Ronald Reagan. His presidency has been ranked among the upper tier in historical rankings of U.S. presidents. However, his personal conduct and allegations of sexual assault have made him the subject of substantial scrutiny. Since leaving office, Clinton has been involved in public speaking and humanitarian work. He created the Clinton Foundation to address international causes such as the prevention of HIV/AIDS and global warming. In 2009, he was named the United Nations Special Envoy to Haiti. After the 2010 Haiti earthquake, Clinton and George W. Bush formed the Clinton Bush Haiti Fund. He has remained active in Democratic Party politics, campaigning for his wife's 2008 and 2016 presidential campaigns.\nThought: Bill Clinton's wife is Hillary Clinton. I need to search Hillary Clinton and find what she does for a living.\nAction: Search[Hillary Clinton]\nObservation: Hillary Diane Rodham Clinton (born October 26, 1947) is an American politician and diplomat who served as the 67th United States secretary of state under president Barack Obama from 2009 to 2013, as a United States senator representing New York from 2001 to 2009, and as the first lady of the United States as the wife of President Bill Clinton from 1993 to 2001. A member of the Democratic Party, she was the party's nominee for president in the 2016 presidential election, becoming the first woman to win a presidential nomination by a major U.S. political party. Clinton won the popular vote, but lost the Electoral College vote, thereby losing the election to Donald Trump.\nRaised in the Chicago suburb of Park Ridge, Rodham graduated from Wellesley College in 1969 and earned a Juris Doctor degree from Yale Law School in 1973. After serving as a congressional legal counsel, she moved to Arkansas and married future president Bill Clinton in 1975; the two had met at Yale. In 1977, Clinton co-founded Arkansas Advocates for Children and Families. She was appointed the first female chair of the Legal Services Corporation in 1978 and became the first female partner at Little Rock's Rose Law Firm the following year. The National Law Journal twice listed her as one of the hundred most influential lawyers in America. Clinton was the  First Lady of Arkansas from 1979 to 1981 and again from 1983 to 1992. As the first lady of the United States, Clinton advocated for healthcare reform. In 1994, her major initiative—the Clinton health care plan—failed to gain approval from Congress. In 1997 and 1999, Clinton played a leading role in advocating the creation of the State Children's Health Insurance Program, the Adoption and Safe Families Act, and the Foster Care Independence Act. Clinton advocated for gender equality at the 1995 UN conference on women. Her marital relationship came under public scrutiny during the Lewinsky scandal, which led her to issue a statement that reaffirmed her commitment to the marriage.\nIn 2000, Clinton was elected as the first female senator from New York and became the first First lady to simultaneously hold elected office, and then the first former First lady to serve in the Senate. She was re-elected in 2006 and chaired the Senate Democratic Steering and Outreach Committee from 2003 to 2007. During her Senate tenure, Clinton advocated for medical benefits for September 11 first responders. She supported the resolution authorizing the Iraq War in 2002, but opposed the surge of U.S. troops in 2007. In 2008, Clinton ran for president but was defeated by eventual winner Barack Obama in the Democratic primaries. Clinton was U.S. Secretary of State in the first term of the Obama administration from 2009 to 2013. During her tenure, Clinton established the Quadrennial Diplomacy and Development Review. She responded to the Arab Spring by advocating military intervention in Libya but was harshly criticized by Republicans for the failure to prevent or adequately respond to the 2012 Benghazi attack. Clinton helped to organize a diplomatic isolation and a regime of international sanctions against Iran in an effort to force it to curtail its nuclear program; this effort eventually led to the multinational JCPOA nuclear agreement in 2015. Her use of a private email server when she was Secretary of State was the subject of intense scrutiny; while no charges were filed against Clinton, the email controversy was the single most covered topic during the 2016 presidential election.\nClinton made a second presidential run in 2016, winning the Democratic nomination, and ran in the general election with Virginia senator Tim Kaine as her running mate. Clinton lost the presidential election to Republican opponent Donald Trump in the Electoral College, despite winning the popular vote by close to 3 million votes. Following her loss, she wrote her third memoir, What Happened, and launched Onward Together, a political action organization dedicated to fundraising for progressive political groups. Since February 2023, she has served on the faculty of the School of International and Public Affairs at Columbia University.\nThought: Hillary Clinton is a politician, diplomat, and lawyer. So the answer is politician, diplomat, and lawyer.\nAction: Finish[politician, diplomat, lawyer]\n\n&gt; Finished chain.\n\n\n'politician, diplomat, lawyer'\n\n\nご覧の通り、Agentが質問を受けた後、まず「先にクリントンの奥さんの名前を調べて、それから彼女の仕事を調べる」というプランを立てました。そして、そのプランに基づいて、Wikipediaでまずクリントンを検索し、その結果からヒラリーを特定し、さらにヒラリーの仕事を調べて、答案を作りました。\nこれで、LangChainのAgentの基本がわかりました。しかし、上記のことはLangChainが実装されているReActをCALLして利用しているだけです。自分でカスタマイズのAgentを作るにはどうすればよいかを、これから説明します。"
  },
  {
    "objectID": "posts/20230509_react/index.html#カスタマイズのagentを作る",
    "href": "posts/20230509_react/index.html#カスタマイズのagentを作る",
    "title": "LangChain Agentの全面解説",
    "section": "カスタマイズのAgentを作る",
    "text": "カスタマイズのAgentを作る\nAgentは3つの要素から構成されています。\n\nPromptTemplate: Agentの中の一番コアな部分です。このテンプレートでAgentの挙動を定義します。\nllm: Agentが利用するLLMです。\nOutptParser: LLMのアウトプットを解析し、AgentActionもしくはAgentFinishを生成するモジュールです。\n\n作られたAgentはAgentExecutorを通じで、以下のステップで実行します。\n\nユーザー入力とそれまでのステップをエージェントに渡す。\nエージェントがAgentFinishを返す場合、それを直接結果に返す。\nAgentがAgentActionを返した場合、それを使ってツールを呼び出し、Observationを取得します。\nAgentFinishが返されるまで、AgentActionとObservationをAgentに戻すことを繰り返します。\n\nこれから実際にカスタマイズ的なAgentを作りましょう。\nこのAgentは「Search」のツールでDBから情報を取得し、質問に答えることができます。 DBの中で「Hiroko」さんの家族に関する情報が入っています。\ncorpus = [\n    \"takuma is a teacher\",\n    \"hiroko's father is takuma\",\n    \"hiroko's mather is ayako\",\n    \"ayako is a doctor\",\n    \"hiroko is 10 years old\",\n]\nコードが100行ぐらいあります。こからステップ・バイ・ステップで説明するのでなので、一旦折りたたみます。下の矢印をクリックすると、コードが表示されます。\n\n\nClick here to show the agent definition code\nfrom langchain import OpenAI, LLMChain\nfrom langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\nfrom langchain.schema import AgentAction, AgentFinish\nfrom langchain.prompts import StringPromptTemplate\nfrom typing import List, Union\nimport re\nfrom dotenv import load_dotenv\n# set the environment variables\nload_dotenv()\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\n\ncorpus = [\n    \"takuma is a teacher\",\n    \"hiroko's father is takuma\",\n    \"hiroko's mather is ayako\",\n    \"ayako is a doctor\",\n    \"hiroko is 10 years old\",\n]\n        \nembedding = OpenAIEmbeddings()\nvectorstore = FAISS.from_texts(corpus, embedding)\n\ntools = [\n    Tool(\n        name=\"Search\",\n        func= lambda query: vectorstore.similarity_search(query, top_k=1)[0].page_content,\n        description=\"useful for when you need to ask with search\"\n    ),\n]\n\ntool_names = [tool.name for tool in tools]\ntemplate = \"\"\"Answer the following questions as best you can, You have access to the following tools:\n{tools}\n\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin! \nQuestion: {input}\n{agent_scratchpad}\"\"\"\n\nclass CustomPromptTemplate(StringPromptTemplate):\n    # The template to use\n    template: str\n    # The list of tools available\n    tools: List[Tool]\n    \n    def format(self, **kwargs) -&gt; str:\n        # Get the intermediate steps (AgentAction, Observation tuples)\n        # Format them in a particular way\n        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n        thoughts = \"\"\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n        # Set the agent_scratchpad variable to that value\n        kwargs[\"agent_scratchpad\"] = thoughts\n        # Create a tools variable from the list of tools provided\n        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n        # Create a list of tool names for the tools provided\n        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n        return self.template.format(**kwargs)\n    \nclass CustomOutputParser(AgentOutputParser):\n    \n    def parse(self, llm_output: str) -&gt; Union[AgentAction, AgentFinish]:\n        if \"Final Answer:\" in llm_output:\n            return AgentFinish(\n                # Return values is generally always a dictionary with a single `output` key\n                # It is not recommended to try anything else at the moment :)\n                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n                log=llm_output,\n            )\n        # Parse out the action and action input\n        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n        match = re.search(regex, llm_output, re.DOTALL)\n        if not match:\n            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n        action = match.group(1).strip()\n        action_input = match.group(2)\n        # Return the action and action input\n        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n\n\nllm = OpenAI(temperature=0, model_name=\"text-davinci-003\")\nprompt = CustomPromptTemplate(\n    template=template,\n    tools=tools,\n    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n    # This includes the `intermediate_steps` variable because that is needed\n    input_variables=[\"input\", \"intermediate_steps\"]\n)\n# LLM chain consisting of the LLM and a prompt\nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\noutput_parser = CustomOutputParser()\n\nagent = LLMSingleActionAgent(\n    llm_chain=llm_chain, \n    output_parser=output_parser,\n    stop=[\"\\nObservation:\"], \n    allowed_tools=tool_names\n)\n\n\n定義した後実行して見ましょう。\n\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\nagent_executor.run(\"What is hiroko's father's ocupation?\")\n\n\n\n&gt; Entering new AgentExecutor chain...\nThought: I need to find out what hiroko's father does for a living.\nAction: Search\nAction Input: \"Hiroko's father's occupation\"\n\nObservation:hiroko's father is takuma\n I need to find out what Takuma does for a living.\nAction: Search\nAction Input: \"Takuma's occupation\"\n\nObservation:takuma is a teacher\n I now know the final answer.\nFinal Answer: Takuma is a teacher.\n\n&gt; Finished chain.\n\n\n'Takuma is a teacher.'\n\n\nカスタマイズ的なが「ReAct」と同じように2回の検索によって結果を得ました。これはAgentExecutorを経由して得た結果です。その中でどのように動作しているかがこれからStep-by-stepで説明します。"
  },
  {
    "objectID": "posts/20230509_react/index.html#agentの動作を説明する",
    "href": "posts/20230509_react/index.html#agentの動作を説明する",
    "title": "LangChain Agentの全面解説",
    "section": "Agentの動作を説明する",
    "text": "Agentの動作を説明する\n\nツールを定義する\ncorpus = [\n    \"takuma is a teacher\",\n    \"hiroko's father is takuma\",\n    \"hiroko's mather is ayako\",\n    \"ayako is a doctor\",\n    \"hiroko is 10 years old\",\n]\n        \nembedding = OpenAIEmbeddings()\nvectorstore = FAISS.from_texts(corpus, embedding)\n\ntools = [\n    Tool(\n        name=\"Search\",\n        func= lambda query: vectorstore.similarity_search(query, top_k=1)[0].page_content,\n        description=\"useful for when you need to ask with search\"\n    ),\n]\n今回使うツールはDBからテキストを検索するツールです。ツールが使わる時、ツールのfuncがAgentActionよりコールされ、Observationが返されます。例えば、下記でtoolにHirokoさんの年齢を入れたら、toolはDBにあるドキュメントを検索し、それに関連するテキストを返します。\n\ntool = tools[0]\nquery = \"hiroko's age\"\nobservation = tool(query)\nprint(observation)\n\nhiroko is 10 years old\n\n\n\n\npromptを定義する\nつぎに、promptを定義します。そのために、まず最初のテンプレートを定義する必要があります。\ntool_names = [tool.name for tool in tools]\ntemplate = \"\"\"Answer the following questions as best you can, You have access to the following tools:\n{tools}\n\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin! \nQuestion: {input}\n{agent_scratchpad}\"\"\"\nテンプレートには4つの変数があります。\n\ntools: Agentが利用できるツールの詳細情報\ntool_names: ツールの名前のリスト\ninput: Agentに渡された質問\nagent_scratchpad: Agentの内部のメモ(次に説明)\n\nつぎに、実際にそれをベースとしてLangChainのPromptTemplateを定義し、初期化を行います。\nclass CustomPromptTemplate(StringPromptTemplate):\n    # The template to use\n    template: str\n    # The list of tools available\n    tools: List[Tool]\n    \n    def format(self, **kwargs) -&gt; str:\n        # Get the intermediate steps (AgentAction, Observation tuples)\n        # Format them in a particular way\n        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n        thoughts = \"\"\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n        # Set the agent_scratchpad variable to that value\n        kwargs[\"agent_scratchpad\"] = thoughts\n        # Create a tools variable from the list of tools provided\n        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n        # Create a list of tool names for the tools provided\n        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n        return self.template.format(**kwargs)\n\nprompt = CustomPromptTemplate(\n    template=template,\n    tools=tools,\n    input_variables=[\"input\", \"intermediate_steps\"]\n)\npromptを初期化する際にtoolsを渡したため、テンプレートに埋める時に[\"input\", \"intermediate_steps\"]があれば良いです。intermediate_stepsには途中の結果が全部は入っていて、それを使ってpromptにあるagent_scratchpadを埋めます。\nHirokoさんのお父さんの職業を聞く場合、最初のpromptはどんなものかを実際に見てみましょう。\n\nquery = \"What is hiroko's father's ocupation?\"\nformatted_prompt = prompt.format(input=query,  intermediate_steps=[])\n\nアウトプットは以下青字はインプットした情報です。\n\nAnswer the following questions as best you can, You have access to the following tools:\n\nSearch: useful for when you need to ask with search\n\nUse the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [Search] Action Input: the input to the action Observation: the result of the action … (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question\nBegin!  Question: What is hiroko’s father’s ocupation?\n\n\n\n最初のプランを建てる\nこのpromptをllmに渡すと、llmはpromptを補完します。そのアウトプットは以下のようになります。\n\noutput = llm(formatted_prompt)\nprint(output)\n\nThought: I need to find out what hiroko's father does for a living.\nAction: Search\nAction Input: \"Hiroko's father's occupation\"\nObservation: I found a website that lists Hiroko's father as a doctor.\nThought: I now know the final answer.\nFinal Answer: Hiroko's father is a doctor.\n\n\nここで、llmはpromptが決めたパターンに沿ってアウトプットを出しました。この中で、Observation:以降のものは全部捏造したものです。なぜかというと、ここまではまだDBに検索することをやっていなくて、llmはまだ何も知らないからです。ここでllmをやってもらいたいことはつぎのステップを決めてもらうだけです。 なので、outputのObservation:以降のものを全部切って、それをOutputParserに渡して、つぎのアクションを抽出してもらいます。\n\nclass CustomOutputParser(AgentOutputParser):\n    \n    def parse(self, llm_output: str) -&gt; Union[AgentAction, AgentFinish]:\n        if \"Final Answer:\" in llm_output:\n            return AgentFinish(\n                # Return values is generally always a dictionary with a single `output` key\n                # It is not recommended to try anything else at the moment :)\n                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n                log=llm_output,\n            )\n        # Parse out the action and action input\n        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n        match = re.search(regex, llm_output, re.DOTALL)\n        if not match:\n            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n        action = match.group(1).strip()\n        action_input = match.group(2).strip()\n        # Return the action and action input\n        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n\nparser = CustomOutputParser()\ntruncated_output = output.split(\"Observation:\")[0]\naction = parser.parse(truncated_output)\n\nactionには3つのフィールドがあります。\n\nlog: parserにインプットされたもの\ntool: parserが抽出したツールの名前\ntool_input: parserが抽出したツールにインプットするもの\n\n\nfor variable in [\"log\",\"tool\",\"tool_input\"]:\n    print(variable, \":\")\n    print(getattr(action, variable).strip())\n    print()\n\nlog :\nThought: I need to find out what hiroko's father does for a living.\nAction: Search\nAction Input: \"Hiroko's father's occupation\"\n\ntool :\nSearch\n\ntool_input :\nHiroko's father's occupation\n\n\n\n\n\n第一ステップを実行する\nこれでつぎのステップがわかったので、toolを実行します。この例でいうと、DBにHiroko's father's occupationを検索することです。その結果はactionを実行した後のobservationです。\n\ntool = tools[0]\nobservation = tool.run(action.tool_input)\nprint(observation)\n\nhiroko's father is takuma\n\n\n\n\n実行した結果を観察し、つぎのステップを決める\nこの第一ステップにより、Hirokoさんのお父さんはTakumaさんということがわかります。この中間結果をintermediate_stepsに追加して、再度llmに問い合わせする必要があります。\nまた、promptとllmとつないて、Chainを作ることができます。それで中間のステップが省くことができるのて、より便利になります。\n\nllm_chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n\nintermediate_steps = [(action, observation)]\nsecond_step_output = llm_chain.run(input=query, intermediate_steps=intermediate_steps)\n\n\n\n&gt; Entering new LLMChain chain...\nPrompt after formatting:\nAnswer the following questions as best you can, You have access to the following tools:\nSearch: useful for when you need to ask with search\n\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Search]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin! \nQuestion: What is hiroko's father's ocupation?\nThought: I need to find out what hiroko's father does for a living.\nAction: Search\nAction Input: \"Hiroko's father's occupation\"\n\nObservation: hiroko's father is takuma\nThought: \n\n&gt; Finished chain.\n\n\n\nprint(second_step_output)\n\n I need to find out what Takuma does for a living.\nAction: Search\nAction Input: \"Takuma's occupation\"\n\nObservation: Takuma is a fisherman.\nFinal Answer: Takuma is a fisherman.\n\n\n\n\n終了条件\nこれで1循環が終わりました。今までわかったこととしては、Hirokoさんのお父さんはTakumaさんということです。また、次にTakumaさんの職業を聞くことも決めました。 そのつぎのステップは今までと全く同じです。\n\ntruncated_output = second_step_output.split(\"Observation:\")[0]\naction = parser.parse(truncated_output)\nobservation = tool.run(action.tool_input)\nintermediate_steps += [(action, observation)]\nthird_step_output = llm_chain.run(input=query, intermediate_steps=intermediate_steps)\nprint(third_step_output)\n\n\n\n&gt; Entering new LLMChain chain...\nPrompt after formatting:\nAnswer the following questions as best you can, You have access to the following tools:\nSearch: useful for when you need to ask with search\n\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Search]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin! \nQuestion: What is hiroko's father's ocupation?\nThought: I need to find out what hiroko's father does for a living.\nAction: Search\nAction Input: \"Hiroko's father's occupation\"\n\nObservation: hiroko's father is takuma\nThought:  I need to find out what Takuma does for a living.\nAction: Search\nAction Input: \"Takuma's occupation\"\n\n\nObservation: takuma is a teacher\nThought: \n\n&gt; Finished chain.\n I now know the final answer.\nFinal Answer: Takuma is a teacher.\n\n\n今回のアウトプットはつぎのアクションがなくて、直接ObservationからFinal Answerが出たので、これをOutputParserに渡せばAgentFinishを抽出できます。AgentFinishが抽出した時点で、全体の処理が終わります。\n\naction = parser.parse(third_step_output)\nprint(action)\nprint()\nprint(\"Final Answer:\", action.return_values[\"output\"])\n\nAgentFinish(return_values={'output': 'Takuma is a teacher.'}, log=' I now know the final answer.\\nFinal Answer: Takuma is a teacher.')\n\nFinal Answer: Takuma is a teacher.\n\n\nこれで、Agentの最初から最後までの流れをひと通り解説を行いました。"
  },
  {
    "objectID": "posts/20230513_azure_openai/index.html",
    "href": "posts/20230513_azure_openai/index.html",
    "title": "Azure OpenAI Serviceの利用について",
    "section": "",
    "text": "最近、GPTを用いた提案を行う際のお作法についていくつかの問い合わせを受けています。実は、筆者もGPTを利用したプロジェクトを担当し、いくつかの問題でコケた経験があります。その経験をまとめてみたいと思います。"
  },
  {
    "objectID": "posts/20230513_azure_openai/index.html#azure-openai-serviceの申請",
    "href": "posts/20230513_azure_openai/index.html#azure-openai-serviceの申請",
    "title": "Azure OpenAI Serviceの利用について",
    "section": "Azure OpenAI Serviceの申請",
    "text": "Azure OpenAI Serviceの申請\nAzure OpenAI Service は、マイクロソフトがOpenAIを買収した後、Azureを基盤として提供しているOpenAIのAPIサービスです。Azure OpenAI Serviceは、OpenAI APIとAzureエンタープライズレベルのセキュリティ、コンプライアンス、リージョンの可用性を組み合わせています。\nつまり、Azure OpenAI Serviceを使うことで、セキュリティー等の面倒なものを全部Azureに委ね、こちらはOpenAIのAPIの利用に専念することができます。\nAzure OpenAI Serviceを使うために、いくつかの事前準備が必要です。\nその流れは以下です。階層構造は先決条件を意味しています。\n→Azureアカウント申請(即日)\n    → Azure OpenAI Service申請 (1〜2日)\n        → Opt-out申請 (2〜4日)\n        → GPT4申請(？日 後日補足)\n\n\n\n\n\n\nCaution\n\n\n\n()の中にあるのは時間の目安です。あくまでも目安なので、そうはならない可能性も十分あることをご理解ください。\n\n\n各申請フォームは以下です。\n\nAzure OpenAI Service申請フォーム\nOpt-out申請フォーム\nGPT4申請フォーム"
  },
  {
    "objectID": "posts/20230513_azure_openai/index.html#opt-outとは",
    "href": "posts/20230513_azure_openai/index.html#opt-outとは",
    "title": "Azure OpenAI Serviceの利用について",
    "section": "Opt-outとは",
    "text": "Opt-outとは\n一般的に、下図のように、GPTモデルへのすべての入力と出力はAzure側で30日間保存されます。これは、Azureが倫理違反等の不適切な行為の有無を審査するための措置です。そのデータの保存を禁止することはOpt-out申請の目的です。 詳細はここで確認できます。\n\n\n\ndata flow\n\n\n\n\n\n\n\n\nNote\n\n\n\nこのデータの保存はあくまでも審査目的なので、Azure OpenAI Serviceは顧客データを一切モデルの学習等に利用しない約束をホームページで明記されています。"
  },
  {
    "objectID": "posts/20230513_azure_openai/index.html#個人情報を扱う場合",
    "href": "posts/20230513_azure_openai/index.html#個人情報を扱う場合",
    "title": "Azure OpenAI Serviceの利用について",
    "section": "個人情報を扱う場合",
    "text": "個人情報を扱う場合\n\n\n\n\n\n\nCaution\n\n\n\n(筆者も法律のことは詳しくないので、以下の話は経験に過ぎなくて、もっと良いやり方があるかもしれません。)\n\n\nAzure OpenAI Service申請する際に、リージョンを選ぶ必要があります。個人情報を含むデータを処理する場合はWest Europeを選択したほうが無難です。\nなぜかどいうと、EUのGDPRは日本の個人情報保護法と互換性があるため、契約上の法務上の手続きがしやすいためです。国の機関である個人情報保護委員会は『日EU間・日英間のデータ越境移転について』で、日EU間のデータ越境移転について以下のように述べています。\n\n日EU間では、相互の円滑な個人データの移転を図る相互認証の枠組みが成立しており、互いのデータ保護制度を同等とみなし、両者間での自由な個人データ流通が可能となっています。\n\nしかし、West Europeをリージョンとして選択することにはデメリットもあります。それは最新のモデルがすぐに利用できないことです。例えば、現時点ではGPT4はまだWest Europeで利用できません。これから新しいモデルが出ても、それがWest Europeで適用されるまでに時間がかかる可能性があります。\n\n\n\n\n\n\nNote\n\n\n\nAzureの担当者にリージョン問題について問い合わせたところ、「どのリージョンでデプロイしても、Azure OpenAI ServiceはGDPRに準拠しています」との回答を得ました。 このページで同じ回答を確認することができます。したがって、リージョンをアメリカに設定することも可能かもしれません。ただし、アメリカを選ぶ場合には、さらなる調査が必要です。"
  },
  {
    "objectID": "posts/20230523_speech_foundation_models_part1/index.html",
    "href": "posts/20230523_speech_foundation_models_part1/index.html",
    "title": "音声基礎モデルの紹介 Part1",
    "section": "",
    "text": "本文の内容はYoutubeで音声認識に関する講義に基づいています。全体は3部分に分けられます。本文は音声表現学習に焦点を当て、対照学習モデルと予測モデルの両方について説明します。次のパートでは、音声大型言語モデルやその他の音声基礎モデルについて詳しく説明します。\n資料のリンクはこちらです。\n現在、音声処理の一般的な方法は2ステップです。まず、音声をSelf-supervised Learning Model(自己教師ありモデル)を通して、音声の特徴表現を抽出します。抽出した特徴表現を下流のモデルにインプットし、音声認識もしくは話者分離を行います。\nPart1の内容としては、音声に関する自己教師ありモデルの紹介と抽出した特徴表現の評価です。\nPart2では、SSLモデルの上に言語モデルを載せ、音声生成や、音声翻訳等の研究を紹介します。\nPart3だと、大量なラベル付けデータで学習したWhisperとUSMを紹介します。"
  },
  {
    "objectID": "posts/20230523_speech_foundation_models_part1/index.html#part-1-speech-representaion-learning",
    "href": "posts/20230523_speech_foundation_models_part1/index.html#part-1-speech-representaion-learning",
    "title": "音声基礎モデルの紹介 Part1",
    "section": "Part 1 Speech Representaion Learning",
    "text": "Part 1 Speech Representaion Learning\n\nなぜ音声表現学習が必要ですか。\nこれまで各タスクには別々のモデルを作成していました。例えば、感情認識の場合は、そのための専用なデータを使用して個別のモデルを作ります。話者分離の場合も、それに専用のデータを使用して個別のモデルを作ります。\n\nその欠点も明らかです。まず、ラベル付けのデータを作成するコストが高いです。そして、タスクごとにモデルを作成する必要があります。\n\n音声表現学習は自然言語処理のBERTのようなモデルと同じです。たくさんのラベルなしの音声で音声の表現を学習し、その上で全結合層をつけて、ラベル付きのデータで個別のタスクをFine-Tuningします。\n\n今回は、対照学習モデル（Contrastive models）と予測モデル（Predictive models）について紹介します。\n\n\n\n対照学習とは\n対照学習とは、同類のもの同士は近くに、異なるもの同士は遠くになるように特徴量を学習することです。具体的な例でいうと、図1と図2はどちらも猫の画像なので、図1から抽出するベクトルと図2から抽出するベクトルをなるべく近くにしたいです。一方、図3は犬の画像なので、図1と図3のベクトルをなるべく遠くにしたいです。そのような学習が対照学習です。\n\n\n\nContrastive Predictive Coding\nまずはContrastive Predictive Coding(CPC)を紹介します。\n16,000Hzでサンプリングされた音声に対して、まずCNNでDown samplingを行います。その結果をGRUを入力し、Context vectorをアウトプットします。\n\n\n\n\n\n\nGRUとは\n\n\n\nGRUは、ゲート付き回帰ユニットとも呼ばれるもので、LSTMの変化形のネットワークです。各ステップtに、前のステップt-1の結果と現在ステップtのインプットを受けて、アウトプットをします。\n\n\n\n対照学習では、Positive例とNegative例が必要です。CPCにおいて、Positiveの例は未来の音声で、Negativeの例はほかの音声から抽出したものです。\n得られたContext vectorを線形変換して各Pos/Negの例との類似度を計算して学習を行います。\n\nその結果は素晴らしいです。音素の分類は、話している内容で分類することです。一方、話者の分類は内容ではなくて、音の性質に基づいて行われます。同じモデル上にLinearの層を追加しただけで、音素の分類と話者の分類の両方で、教師あり学習の結果に近い精度が得られます。\n\n\n\n\n\n\nPhone classification(音素分類)とは\n\n\n\n音素(Phone)は、特定の言語の音の最小単位で、それ自体は意味を持たず、他の音素と組み合わせることで単語を形成します。例えば、英語の単語 “cat” は三つの音素 /k/ /æ/ /t/ に分けることができます。\nPhone classification（音素分類）とは音素に分割するプロセスを指します。\n\n\n\n\n\nWav2vec 2.0とXLS-R\nwav2vec 2.0はMetaが2020年6月に出した論文です。Wav2vecの場合は、音声をCNNでDown samplingした後、得た特徴量が2つの部分にインプットされます。\n\n下流のTransfomerのEncoder：ここでBERTと同じように一部のTokenがマスキングされます。また、各インプットのTokenに一つのアウトプットが出力されます。\nベクトル量子化層：ここでインプットを離散的な音声符号に変換します。具体的には、各入力ベクトルをコードブック内の最も近い原型ベクトルにマッピングし、原型のインデックスに対応するワンホットベクトルを出力します。\n\n順番がtのTransformerアウトプットのPositive例はtの特徴量を量子化したもの、Negative例は、ほかのTimestepの量子化したものです。\nつまり、TransformerのアウトプットがなるべくそのTimestepの特徴量に似ているように学習が行われます。詳細は以下の動画をご覧ください。\n\nXLS-RはWav2vec 2.0のアーキテクチャを使っています。Wav2vec 2.0との違いは対応する言語、データとモデルサイズです。\n\nXLS-Rは、Wav2vec 2.0をベースにしていますが、多言語に対応しています。XLS-Rは128言語の音声データで学習されており、Wav2vec 2.0よりも多くの言語をサポートしています。\nXLS-Rは、Wav2vec 2.0よりも大規模なモデルです。XLS-Rは最大20億パラメータを持ち、約50万時間の音声データで学習されています。Wav2vec 2.0は約3億パラメータで、約3万時間の音声データで学習されています。\n\n\n結果を見ると、モデルサイズが同じ場合は、データの種類が多いXLS-Rの精度が逆に下がりました。モデルが小さい場合は、データのバリエーションが多すぎることも必ずしも良いものではないことがわかります。\n\n多言語対応について、たったの10時間のラベリングデータで以前のモデルが全量のデータでFine-Tuning結果と同じレベルのエラー率になります。\n\n\n\nHuBERT\nつぎにPredictive modelの紹介をします。\nPredictive modelはPseudo Labelingで学習しています。まず、ラベルなしのデータに対してクラスリングでラベルを付けます。そのラベルを用いて学習を行います。\n\nHuBERTの名前の中にBERTがあります。その構造もBERTと似ています。音声がインプットされると、Down samplingされてTransformerに入れられます。入力する際に、一部の特徴量がマスキングされます。マスキングされるところのTokenを予測することでモデルが学習しています。\n自然言語の各単語が離散なものなので、それをTokenに変換することができます。音声の場合は、Codebookというものを作る必要があります。作り方はWav2vec 2.0のラベル付け方法と似ています。スライドの右のように、音声をMFCCで特徴量を抽出し、K-meansで複数のカテゴリをします。そのカテゴリの集合はCodebookで、一つずつのカテゴリはTokenになります。\n\n\n\n\n\n\nMFCC\n\n\n\nMFCC（Mel Frequency Cepstral Coefficients）は、音声信号の特徴を抽出するために使用される一連の特徴量です。音声認識や音楽情報検索、音声分析などの分野で広く使用されています。MFCCは、音声信号をメル尺度（人間の聴覚に近い周波数スケール）にマッピングし、さらにケプストラム分析を適用して、音声信号のスペクトル情報を圧縮した形で表現します。これにより、音声の特徴を効果的かつ簡潔に捉えることができます。\n\n\n\nまずこの手法で400,000ステップ学習し、HuBERT-1を学習します。次にHuBERT-1をMFCCの代わりに使用し、Codebookを作成して同様の学習を行い、HuBERT-2を得ます。HuBERT-2は最終モデルです。\n\n\n\nWavLM\nWavLMの構造はHuBERTとまったく同じですが、異なる点は2つあります。まず、WavLMのCodebookはMFCCではなく、直接HuBERTを使いました。また、音声をインプットする際に直接インプットするのではなく、2つの音声を重ねてインプットしました。つまり、モデルは予測の中でノイズ除去のこともしています。\n\nその結果、WavLMの精度がHuBERTより高くなっています。\n\n\n\nBEST-RQ\nBERT-RQとPredictive modelと異なる点はCodebookの生成方法です。BERT-RQはまず投影層をランダムに初期化し、それを固定します。インプットデータにその投影層を追加してCodebookのTokenを直接出力しています。\n\n面白いことに、この方法で学習したモデルはほかのモデルと同じような結果が得られます。つまり、Codebookが固定されたものであれば、完全にランダムでもモデルの学習ができるのます。\n\n\n\nSUPERB Benchmark\nSUPERB benchmarkとは、音声処理のための汎用的な共有表現の能力を評価するためのベンチマークリソースの集合です。\n各タスクについて、学習データと下流のモデルを固定し、上流のモデルの特徴抽出能力を評価することができます。\n\nまた、SUPERBを通じて、どのタスクにどのレイヤーを使ったほうが良いかもわかります。具体的には、学習時に各レイヤーを加重平均して下流のモデルに送ります。層のウェイトは固定ではなく、学習するパラメーターです。それで、重要な層のウェイトが最終的に高くなります。\n\n具体的なタスクは主に4つのカテゴリに分けることができます。\n\nベンチマークからわかることとしては、モデルが大きいほど性能が良いことです。\n\nまた、強いモデルはすべての領域において良い性能を持っています。(自然言語処理の傾向と似ていますね)\n\n右の図の縦はタスク、横はレイヤーのウェイトです。自然言語処理と同じように、話者分離など音に関するタスクは低いレイヤーのウェイトが重いです。意味を解析する音声認識などのタスクだと高いレイヤーのウェイトが重いです。"
  },
  {
    "objectID": "posts/20230505_LangChain_basic/index.html",
    "href": "posts/20230505_LangChain_basic/index.html",
    "title": "LangChainのベーシックを全面解説する",
    "section": "",
    "text": "OpenAIのGPTのAPIを利用してアプリを作成するには、今まで一番使いやすいパッケージはLangChain🦜️🔗 だと思います。本文では、LangChainの基本的な使い方を優しく説明します。\n\n\n\nLangChain"
  },
  {
    "objectID": "posts/20230505_LangChain_basic/index.html#前書き",
    "href": "posts/20230505_LangChain_basic/index.html#前書き",
    "title": "LangChainのベーシックを全面解説する",
    "section": "",
    "text": "OpenAIのGPTのAPIを利用してアプリを作成するには、今まで一番使いやすいパッケージはLangChain🦜️🔗 だと思います。本文では、LangChainの基本的な使い方を優しく説明します。\n\n\n\nLangChain"
  },
  {
    "objectID": "posts/20230505_LangChain_basic/index.html#環境設定",
    "href": "posts/20230505_LangChain_basic/index.html#環境設定",
    "title": "LangChainのベーシックを全面解説する",
    "section": "環境設定",
    "text": "環境設定\nまずは定番のpipからインストールすることです。\npip install langchain, openai\nそのつぎに、OpenAIのAPIキーを取得して、環境変数に設定します。 APIはここから取得できます。\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"...\"\n直接にAPIキーを書くのはセキュリティ上の問題があるので、スクリプトを共有する場合は(例えば本文)、APIキーを別ファイルに保存し、ファイルから読み込んだほうがよいです。\n\nimport os \nwith open(\"../../.env\", \"r\") as f: \n    os.environ.update(dict([line.strip().split(\"=\") for line in f.readlines()]))"
  },
  {
    "objectID": "posts/20230505_LangChain_basic/index.html#openaiのgptモデル",
    "href": "posts/20230505_LangChain_basic/index.html#openaiのgptモデル",
    "title": "LangChainのベーシックを全面解説する",
    "section": "OpenAIのGPTモデル",
    "text": "OpenAIのGPTモデル\nLangChainの中にOpenAIのGPTモデルを使うラッパーがあります。現在使えるモデルはテキスト補完モデルとChatモデルの2種類あります。生成モデルの場合は以下のように使います。\n\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\noutput = llm(\"日本の首都は?\")\nprint(output.strip())\n\n東京です。\n\n\nまた、Chatモデルを利用して対話を行うこともできます。\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\nchat = ChatOpenAI(temperature=0)\noutput = chat([HumanMessage(content=\"文法を修正してください:I loves programming.\")])\nprint(output.content)\n\nI love programming.\n\n\n\n各モデルの特性のまとめ\n各モデルの値段や、最大トークン数、モデルサイズは以下の表にまとめました。\nテキスト補完モデル\n\n\n\n\nモデル名\n\n\n値段(1k tokensごと)\n\n\n最大トークン数\n\n\nモデルサイズ(推測)\n\n\n\n\nDavinci\n\n\n$0.0200\n\n\n4,097\n\n\n175B\n\n\n\n\nCurie\n\n\n$0.0020\n\n\n4,097\n\n\n6.7B\n\n\n\n\nBabbage\n\n\n$0.0005\n\n\n4,097\n\n\n1.3B\n\n\n\n\nAda\n\n\n$0.0004\n\n\n4,097\n\n\n350M\n\n\n\n\nChatモデル\n\n\n\n\nモデル名\n\n\n値段(**Prompt)\n\n\n値段(**補完)\n\n\n最大トークン数\n\n\nモデルサイズ(推測)\n\n\n\n\ngpt-3.5-turbo\n\n\n$0.002\n\n\n$0.002\n\n\n4,096\n\n\n6.7B\n\n\n\n\ngpt-4\n\n\n$0.03\n\n\n$0.06\n\n\n8,192\n\n\n6.7B\n\n\n\n\ngpt-4-32k\n\n\n$0.06\n\n\n$0.12\n\n\n32,768\n\n\n1.3B\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nここで注意することとしてはGPT4の値段です。インプットするテキストがprompt、生成したテキストはcompletionに分かれていて、promptの値段とcompletionの値段を足したものがGPT4の値段になります。\n\n\n\n\nモデルの使い分け\nモデルの使い分けについては、最も使われているのはChatモデルのgpt-3.5-turboとgpt-4です。gpt-3.5-turboはモデルのサイズが小さいので、生成時間が短く、値段も安いです。一方、gpt-4は性能が良いので、性能を求める場合はgpt-4のほうが良いです。また、gpt-4の最大トークン数が8Kになっているので、生成するテキストの長さが長い場合もこちらを使うほうがいいです。\n他のモデルはほとんど使われないので、必要に応じて詳細を見れば良いです。\n\n\nTokenの計算方法\nTokenの計算方法については、こちらで紹介したので、本文では割愛します。要するに、日本語千文字のドキュメントはおおよそ1,000トークンになり、それを処理するにはgpt-3.5-turboの場合は概算で0.59円、gpt-4の場合は概算で$9.7円かかります。"
  },
  {
    "objectID": "posts/20230505_LangChain_basic/index.html#prompt-template",
    "href": "posts/20230505_LangChain_basic/index.html#prompt-template",
    "title": "LangChainのベーシックを全面解説する",
    "section": "Prompt Template",
    "text": "Prompt Template\nLangChainのPrompt TemplateはPromptを簡単に作成するためのモジュールです。Example selector付きのPromptを作るにはとても役に立ちます。でもそれはよりアドバンス的なやり方なので、入門の段階では単純にPythonのf-stringとして使えれば良いです。\nPromptのテンプレートを書いた後、それをPromptTemplateのインスタンスに渡して、PromptTemplateのformatメソッドを呼び出すと、Promptが生成されます。\n\nfrom langchain import PromptTemplate\n\ntemplate = \"私は{fruit}が好きです。\"\nprompt_template = PromptTemplate.from_template(template)\nprint(prompt_template.format(fruit=\"りんご\"))\nprint(prompt_template.format(fruit=\"みかん\"))\n\n私はりんごが好きです。\n私はみかんが好きです。"
  },
  {
    "objectID": "posts/20230505_LangChain_basic/index.html#vectorstore",
    "href": "posts/20230505_LangChain_basic/index.html#vectorstore",
    "title": "LangChainのベーシックを全面解説する",
    "section": "VectorStore",
    "text": "VectorStore\nドキュメントを検索するためには、VectorStoreを作成する必要があります。VectorStoreはドキュメントのリストを受け取って、それをベクトルに変換して保存します。検索する際に、検索クエリをベクトルに変換して、ベクトルの類似度を計算して、類似度が高いドキュメントを返します。\n\n\n\n\n\n\nFAISSについて\n\n\n\nFAISSはMetaが開発した高速な類似性検索ライブラリです。Faissは、大量のベクトルデータを格納し、高速な検索を行うことができます。\n\n\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.vectorstores import FAISS\n# create test data\nwith open(\"./test_data.txt\", \"w\") as f:\n    fruits = [\"りんご\", \"みかん\", \"バナナ\", \"パイナップル\", \"ぶどう\"]\n    for fruit in fruits:\n        f.write(f\"私は{fruit}が好きです。\\n\")\n        \n# load test data\nloader = TextLoader('./test_data.txt', encoding='utf8')\n\n# query test data\nindex = VectorstoreIndexCreator(vectorstore_cls=FAISS).from_loaders([loader])\nindex.query(\"りんご\")\n\n' 私はりんごが好きです。'"
  },
  {
    "objectID": "posts/20230505_LangChain_basic/index.html#chain",
    "href": "posts/20230505_LangChain_basic/index.html#chain",
    "title": "LangChainのベーシックを全面解説する",
    "section": "Chain",
    "text": "Chain\nChainはLangChainの中心的な概念です。今まで紹介した複数の部品を組み合わせでChainを作ることができます。インプットが入力された後、Chainの内部で処理し、アウトプットを出す。\n例えば、PromptTemplateとLLMをつなぐChainを作ることができます。PromptTemplateはPromptを生成するので、LLMのインプットになります。LLMはPromptを受け取って、それを補完して、アウトプットを生成します。こうしてPromptTemplateとLLMをつなぐChainを作ることができます。\n\n\n\n\nflowchart LR\n    Input([Input])--&gt;PromptTemplate\n    LLM--&gt;Output([Output])\n    subgraph Chain\n    PromptTemplate--&gt;formattedPrompt([Formatted Prompt])\n    formattedPrompt--&gt;LLM\n    end\n    style PromptTemplate stroke:#333,stroke-width:4px\n    style LLM stroke:#333,stroke-width:4px\n\n\nChainのダイアグラムの例\n\n\n\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMChain\nllm = OpenAI(temperature=0.9)\nprompt = PromptTemplate.from_template(\"{country}の首都は何ですか？\")\nchain = LLMChain(llm=llm, prompt=prompt)\n\nこれで各国の首都は簡単に検索できるようになりました。\n\nprint(chain.run({\"country\": \"日本\"}).strip())\nprint(chain.run({\"country\": \"アメリカ\"}).strip())\n\n東京です。\nワシントンD.C."
  },
  {
    "objectID": "posts/20230505_LangChain_basic/index.html#agent",
    "href": "posts/20230505_LangChain_basic/index.html#agent",
    "title": "LangChainのベーシックを全面解説する",
    "section": "Agent",
    "text": "Agent\nAgentはChainよりも高いレベルの概念です。Agentはツールを使うことができます。それにより、Agentは内部環境にとどまらず、外部環境ともやり取りできます。\n一番シンプルの例としてはBingChatがあげられます。ユーザーのクエリーを受けた後、BingChatはインタネットから情報を検索し、それをサマリーして、ユーザーのクエリに答えます。\nAgentの中身は複雑でドキュメントに書いていないので、今回は挙動だけ見せます。ここでBingChatに似ている機能を実現するAgentを作ります。このAgentはユーザーのクエリーを受け取って、それをインタネットで検索し、その答えを返すことができます。また、外部の電卓ツールを利用して計算もできます。\n\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n\n\nagent.run(\"今日の気温は何度ですか？その2乗は何ですか？\")\n\n\n\n&gt; Entering new AgentExecutor chain...\n I need to find out the temperature and then calculate its square.\nAction: Search\nAction Input: 今日の気温\nObservation: ニューヨーク, NY, アメリカ合衆国 の天気. 4. 今日 · 1時間ごと · 10日間 · レーダー. 1時間ごとの天気-ニューヨーク, NY, アメリカ合衆国. 13:48 EDT時点 ...\nThought: I need to find the temperature from the search results\nAction: Search\nAction Input: 今日の気温 ニューヨーク\nObservation: 16:00 · 体感温度16° · 風南東 8 km/h · 湿度47% · 紫外線指数2/10 · 雲量78% · 雨量0 cm ...\nThought: I now have the temperature, I need to calculate its square\nAction: Calculator\nAction Input: 16^2\nObservation: Answer: 256\nThought: I now know the final answer\nFinal Answer: 今日の気温は16度で、その2乗は256です。\n\n&gt; Finished chain.\n\n\n'今日の気温は16度で、その2乗は256です。'\n\n\n「今日の気温は何度ですか？その2乗は何ですか？」のクエリーを投げた後、Agentのほうはまずやるべきことを決めました。やるべきことをプランニングしながら、自分が持っているツールを駆使し、クエリーに答えました。"
  },
  {
    "objectID": "posts/20230505_LangChain_basic/index.html#まとめ",
    "href": "posts/20230505_LangChain_basic/index.html#まとめ",
    "title": "LangChainのベーシックを全面解説する",
    "section": "まとめ",
    "text": "まとめ\nこれでLangChainの中にあるMemory以外のものをひと通り浅く紹介しました。LangChainの開発はまだ初期の段階なので、APIの設計や、ドキュメントの充実さなどの問題があります。今後は各概念を解剖する記事を書いていきます。"
  },
  {
    "objectID": "posts/20230601_embedding_benchmark/index.html",
    "href": "posts/20230601_embedding_benchmark/index.html",
    "title": "日本語Embeddingモデルのベンチマーク比較: OpenAIが圧倒的な精度でリード",
    "section": "",
    "text": "質問応答システムを作成する際、最初のステップは関連するドキュメントをデータベースから取得することです。これは通常、クエリをOpenAIのAPIを使用して埋め込みに変換し、データベース内の埋め込みとの距離を計算して近い順に並べることで行われます。\nしかし、あるブログポストでは、「OpenAIの埋め込みサービスはコストが高く精度も低い」と主張されています。このポストでは、OpenAIの埋め込みモデルをGoogleの埋め込みモデルやSentence-Transformersモデルと比較し、精度が低くコストが高いことが示されています。ただし、この評価は英語に基づいており、2年前のものなので、現在の日本語の状況がどうなっているかが気になります。\nそれで今回は、日本語のEmbeddingモデルを比較するベンチマークを作りました。ベンチマークの結果は、下図のようになります。\nOpenAIの精度が最も高く、他のモデルより各指標で10%高いです。また、ほかのEmbeddingの中で一番精度が高いのはTensforFlowのUniversal Sentence Encoderです。\nそこで、今回は日本語の埋め込みモデルを比較するベンチマークを作成しました。結果は下図の通りで、OpenAIの精度が最も高く、他のモデルに比べて各指標で10%高いです。また、他の埋め込みモデルの中で最も精度が高いのは、TensorFlowのUniversal Sentence Encoderです。"
  },
  {
    "objectID": "posts/20230601_embedding_benchmark/index.html#使用したデータセット",
    "href": "posts/20230601_embedding_benchmark/index.html#使用したデータセット",
    "title": "日本語Embeddingモデルのベンチマーク比較: OpenAIが圧倒的な精度でリード",
    "section": "使用したデータセット",
    "text": "使用したデータセット\n今回使用するデータは東京都立大学のeラーニングシステムのQ&Aデータです。このデータは、東京都立大学で導入されたeラーニングシステムのユーザーから2015年4月から2018年7月までに報告された問題点としてのQ&Aデータを収集したものです。427の質問と79の回答が含まれています。質問にどの回答に紐づくかのラベルがあります。\nデータの様子は下記の通りです。\n\nimport pandas as pd\n# https://zenodo.org/record/2783642\nq_df = pd.read_csv(\"https://zenodo.org/record/2783642/files/Questions.csv\")\na_df = pd.read_csv(\"https://zenodo.org/record/2783642/files/Answers.csv\")\nprint(\"q_df.shape:\", q_df.shape)\nprint(\"a_df.shape:\", a_df.shape)\nq_df.columns = [c.strip() for c in q_df.columns]\na_df.columns = [c.strip() for c in a_df.columns]\ndf = q_df.merge(a_df, on=\"AID\")\ndf.columns = [\"query\",\"AID\",\"document\"]\n\nmetadata = a_df[[\"AID\"]].to_dict(orient=\"records\")\ndocuments = a_df[\"Text\"].tolist()\nquery_list = list(zip(q_df[\"Text\"], q_df[\"AID\"]))\ndisplay(q_df.head(3))\ndisplay(a_df.head(3))\n\nq_df.shape: (427, 2)\na_df.shape: (79, 2)\n\n\n\n\n\n\n\n\n\nText\nAID\n\n\n\n\n0\n履修している授業で先生が資料をアップロードしているはずだが、コース上に資料が見当たらない。\nA001\n\n\n1\n資料をマイページに置いたが、学生からは見えなかった。\nA001\n\n\n2\n前期の科目の「資料」を学生から見られないようにするにはどうしたら良いか？\nA001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAID\nText\n\n\n\n\n0\nA001\n資料が見つからない場合は、以下の点を確認してください。&lt;br&gt;&lt;br&gt;&lt;br&gt;【受講生編】&lt;...\n\n\n1\nA002\n資料のアップロードやお知らせ作成時の電子メールでの通知の有無は、各授業の担当教員が設定できま...\n\n\n2\nA003\nkibacoにはファイルへパスワードを設定する機能はありません。資料は受講生全員に開示されま..."
  },
  {
    "objectID": "posts/20230601_embedding_benchmark/index.html#評価対象と評価方法評価指標",
    "href": "posts/20230601_embedding_benchmark/index.html#評価対象と評価方法評価指標",
    "title": "日本語Embeddingモデルのベンチマーク比較: OpenAIが圧倒的な精度でリード",
    "section": "評価対象と評価方法、評価指標",
    "text": "評価対象と評価方法、評価指標\n今回評価対象は4つがあります。\n\nHuggingface hubの「Feature extraction」カテゴリで「Japanese」対応する全てのモデル\nSentence-Transformsersのモデル\nTensorFlowのUniversal Sentence Encoderモデル\nOpenAIのEmbeddingモデル\n\n評価方法は以下の3つのステップです。\n\n79のドキュメントをEmbeddingに変換し、FAISSのVectorstoreとして保存する。\n427の質問をEmbeddingに変換し、FAISSのVectorstoreを使用して、79のドキュメントを近い順に並べる。\n並んだ順番でEmbeddingの性能を評価する\n\n評価指標は以下の3つです。\n\nMean Reciprocal Rank（MRR）: 正解ドキュメントの順位の平均の逆数で、ランク全体を評価する指標。\nRecall@1: 正解ドキュメントが1番目に並んでいるかどうかを評価する指標。\nRecall@5: 正解ドキュメントが上位5位以内に入っているかどうかを評価する指標。\n\n評価に使用したコードは約100行です。興味があれば、展開してご覧ください。\n\n\nShow the code\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom tqdm.auto import tqdm\nfrom dataclasses import dataclass\nfrom huggingface_hub import HfApi, ModelFilter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.embeddings import TensorflowHubEmbeddings\n\nDOC_NUM = len(a_df)\n\n@dataclass\nclass EvaluationResult:\n    result_df: pd.DataFrame\n    mrr: float\n    recall_at_1: float\n    recall_at_5: float\n\nclass RankEvaluator:\n    def __init__(self, vectorstore):\n        self.vectorstore = vectorstore\n\n    def get_query_result_rank(self, txt):\n        search_result = self.vectorstore.similarity_search(txt, k=DOC_NUM)\n        rank_result = [r.metadata[\"AID\"] for r in search_result]\n        return rank_result\n\n    def evaluate(self, query_list, get_rank_func=None):\n        if not get_rank_func:\n            get_rank_func = self.get_query_result_rank\n\n        result_list = []\n        for query, aid in tqdm(query_list):\n            rank_result = get_rank_func(query)\n            rank = rank_result.index(aid) + 1\n            result_list.append((query, rank, rank_result))\n\n        result_df = pd.DataFrame(result_list, columns=[\"query\", \"rank\", \"rank_result\"])\n        return EvaluationResult(result_df, mrr(result_df[\"rank\"]), recall_at_k(result_df[\"rank\"], 1), recall_at_k(result_df[\"rank\"], 5))\n\ndef mrr(rank_array):\n    return (1 / rank_array).mean()\n\ndef recall_at_k(rank_array, k):\n    return (rank_array &lt;= k).mean()\n\ndef evaluate_embedding(embedding, get_rank_func=None, texts=documents, metadata=metadata):\n    faiss_vectorstore = FAISS.from_texts(\n        texts=texts,\n        embedding=embedding,\n        metadatas=metadata,\n    )\n    evaluator = RankEvaluator(faiss_vectorstore)\n    return evaluator.evaluate(query_list, get_rank_func=get_rank_func)\n\n# fetch feature extraction model ids that support Japanese form HuggingFace Hub\napi = HfApi()\nmodels = api.list_models(\n    filter=ModelFilter(\n        task=\"feature-extraction\",\n        library=\"pytorch\",\n        language=\"ja\"\n    )\n)\nmodel_id_list = [m.id for m in models]\n\n# add sentence-transformers models\nmodel_id_list += [\n    \"distiluse-base-multilingual-cased-v2\",\n    \"paraphrase-multilingual-MiniLM-L12-v2\",\n    \"paraphrase-multilingual-mpnet-base-v2\",\n    \"sentence-transformers/stsb-xlm-r-multilingual\"\n]\n\n# evaluate models\n# if evaluation fails, set result to None\nresult_dict = {}\nfor model_id in model_id_list:\n    try:\n        embedding = HuggingFaceEmbeddings(model_name=model_id)\n        result = evaluate_embedding(embedding)\n        result_dict[model_id] = result\n    except:\n        result_dict[model_id] = None\n\n# evaluate Tensorflow Hub models\nembeddings_tfhub = TensorflowHubEmbeddings()\nresult = evaluate_embedding(embeddings_tfhub)\nresult_dict[\"TensorflowHubEmbeddings\"] = result\n\n# evaluate OpenAIEmbeddings\nembedding_openai = OpenAIEmbeddings()\nreuslt_openai = evaluate_embedding(embedding_openai)\nresult_dict[\"OpenAIEmbeddings\"] = reuslt_openai\n\n# summary the result to DataFrame\nresult_list = []\nfor model in result_dict.keys():\n    result = result_dict[model]\n    if result:\n        result_list.append([model,result.mrr, result.recall_at_1, result.recall_at_5] )\n    else:\n        result_list.append([model,0, 0,0] )\n        \nresult_df = pd.DataFrame(\n    result_list, \n    columns = [\"model_id\",\"mrr\",\"recall_at_1\",\"recall_at_5\"]\n    ).sort_values(\"mrr\", ascending=False)\n\nresult_df.reset_index(drop=True, inplace=True)"
  },
  {
    "objectID": "posts/20230601_embedding_benchmark/index.html#テスト結果",
    "href": "posts/20230601_embedding_benchmark/index.html#テスト結果",
    "title": "日本語Embeddingモデルのベンチマーク比較: OpenAIが圧倒的な精度でリード",
    "section": "テスト結果",
    "text": "テスト結果\n全体の結果を見ると、OpenAIEmbeddingが圧倒的に優れており、54%のQueryで正解のドキュメントが1番目に、そして86%のQueryで正解のドキュメントが5番目にランクインしています。\nこれに対して2位のTensorFlowのUniversal Sentence Encoderは、各指標で10%以上低いパフォーマンスを示しています。そのため、精度にこのような大きな差があることから、OpenAIEmbedding以外のモデルを使用する意義はほとんどなくなります。\n(結果が0になったモデルは実験中にエラーが発生したものです。)\n\nresult_df\n\n\n\n\n\n\n\n\nmodel_id\nmrr\nrecall_at_1\nrecall_at_5\n\n\n\n\n0\nOpenAIEmbeddings\n0.684147\n0.548009\n0.868852\n\n\n1\nTensorflowHubEmbeddings\n0.560619\n0.407494\n0.761124\n\n\n2\nparaphrase-multilingual-mpnet-base-v2\n0.525899\n0.398126\n0.676815\n\n\n3\noshizo/sbert-jsnli-luke-japanese-base-lite\n0.520106\n0.405152\n0.655738\n\n\n4\nparaphrase-multilingual-MiniLM-L12-v2\n0.497027\n0.370023\n0.639344\n\n\n5\nintfloat/multilingual-e5-base\n0.481144\n0.337237\n0.632319\n\n\n6\nsonoisa/sentence-bert-base-ja-mean-tokens-v2\n0.465294\n0.327869\n0.622951\n\n\n7\nsetu4993/smaller-LaBSE\n0.450434\n0.290398\n0.632319\n\n\n8\nsonoisa/sentence-bert-base-ja-en-mean-tokens\n0.438923\n0.304450\n0.599532\n\n\n9\nsetu4993/LaBSE\n0.434725\n0.274005\n0.625293\n\n\n10\nBlaxzter/LaBSE-sentence-embeddings\n0.434725\n0.274005\n0.625293\n\n\n11\ndistiluse-base-multilingual-cased-v2\n0.428484\n0.264637\n0.620609\n\n\n12\nZurichNLP/unsup-simcse-xlm-roberta-base\n0.419397\n0.299766\n0.526932\n\n\n13\nsentence-transformers/stsb-xlm-r-multilingual\n0.361811\n0.231850\n0.484778\n\n\n14\nsonoisa/clip-vit-b-32-japanese-v1\n0.320160\n0.203747\n0.437939\n\n\n15\nsonoisa/sentence-bert-base-ja-mean-tokens\n0.293779\n0.177986\n0.402810\n\n\n16\ngoogle/canine-s\n0.270446\n0.159251\n0.358314\n\n\n17\ngoogle/canine-c\n0.258978\n0.159251\n0.341920\n\n\n18\ncolorfulscoop/sbert-base-ja\n0.227531\n0.133489\n0.295082\n\n\n19\nsonoisa/t5-base-japanese\n0.213053\n0.135831\n0.278689\n\n\n20\nM-CLIP/M-BERT-Distil-40\n0.170714\n0.084309\n0.236534\n\n\n21\nmicrosoft/unihanlm-base\n0.162957\n0.098361\n0.187354\n\n\n22\nnielsr/lilt-xlm-roberta-base\n0.143722\n0.074941\n0.173302\n\n\n23\nseverinsimmler/xlm-roberta-longformer-base-16384\n0.129116\n0.072600\n0.145199\n\n\n24\nfacebook/nllb-moe-54b\n0.000000\n0.000000\n0.000000\n\n\n25\nsonoisa/sentence-luke-japanese-base-lite\n0.000000\n0.000000\n0.000000\n\n\n26\nTylorShine/distilhubert-ft-japanese-50k\n0.000000\n0.000000\n0.000000\n\n\n27\nrinna/japanese-hubert-base\n0.000000\n0.000000\n0.000000\n\n\n28\nArthurZ/nllb-moe-128\n0.000000\n0.000000\n0.000000\n\n\n29\npkshatech/simcse-ja-bert-base-clcmlp\n0.000000\n0.000000\n0.000000\n\n\n30\npaulhindemith/fasttext-jp-embedding\n0.000000\n0.000000\n0.000000\n\n\n31\nrinna/japanese-cloob-vit-b-16\n0.000000\n0.000000\n0.000000\n\n\n32\nrinna/japanese-clip-vit-b-16\n0.000000\n0.000000\n0.000000\n\n\n33\nsonoisa/sentence-t5-base-ja-mean-tokens\n0.000000\n0.000000\n0.000000\n\n\n34\nmegagonlabs/transformers-ud-japanese-electra-b...\n0.000000\n0.000000\n0.000000"
  },
  {
    "objectID": "posts/20230510_hyde_detailed/index.html",
    "href": "posts/20230510_hyde_detailed/index.html",
    "title": "LangChain Hypothetical Document Embeddings (HyDE) 全面解説",
    "section": "",
    "text": "Hypothetical Document Embeddings (HyDE)は去年提出した情報検索の精度を向上させるための手法です。\n本文はHyDEの概念とLangchainでのその使い方を紹介しました。また、普通のEmbedding手法、HyDE、そして本文で提案したHyDE改善案、この三者の性能を比較しました。以下はテスト結果です。\n\n\n\n手法\n正解数(50件の中)\nMRR\nスピード\n\n\n\n\n普通のEmbedding\n37\n0.855\n17秒\n\n\nHyDE\n37\n0.813\n4分15秒\n\n\nHyDE with title\n40\n0.897\n5分2秒\n\n\n\n結論としてはHyDEはそれほど有効ではないことです。少し改善すれば性能は良くなりますが、検索スピードは非常に遅いですし、コストも大幅増加するので、ほぼ実用ではない手法といえます。"
  },
  {
    "objectID": "posts/20230510_hyde_detailed/index.html#本文の概要",
    "href": "posts/20230510_hyde_detailed/index.html#本文の概要",
    "title": "LangChain Hypothetical Document Embeddings (HyDE) 全面解説",
    "section": "",
    "text": "Hypothetical Document Embeddings (HyDE)は去年提出した情報検索の精度を向上させるための手法です。\n本文はHyDEの概念とLangchainでのその使い方を紹介しました。また、普通のEmbedding手法、HyDE、そして本文で提案したHyDE改善案、この三者の性能を比較しました。以下はテスト結果です。\n\n\n\n手法\n正解数(50件の中)\nMRR\nスピード\n\n\n\n\n普通のEmbedding\n37\n0.855\n17秒\n\n\nHyDE\n37\n0.813\n4分15秒\n\n\nHyDE with title\n40\n0.897\n5分2秒\n\n\n\n結論としてはHyDEはそれほど有効ではないことです。少し改善すれば性能は良くなりますが、検索スピードは非常に遅いですし、コストも大幅増加するので、ほぼ実用ではない手法といえます。"
  },
  {
    "objectID": "posts/20230510_hyde_detailed/index.html#hypothetical-document-embeddings-hydeの詳細",
    "href": "posts/20230510_hyde_detailed/index.html#hypothetical-document-embeddings-hydeの詳細",
    "title": "LangChain Hypothetical Document Embeddings (HyDE) 全面解説",
    "section": "Hypothetical Document Embeddings (HyDE)の詳細",
    "text": "Hypothetical Document Embeddings (HyDE)の詳細\n一般的なDense Information Retrievalの手順は以下のステップで行われます。\n\nQueryとDocument両方ともEmbedding(ベクトル)に変換する\nQueryとDocumentのコサイン類似度を計算する\nコサイン類似度が一番高いDocumentを返す\n\n\n\n\n\nflowchart LR\n    Input[query]--&gt;Embedding[query embedding]\n    Embedding--&gt;DocumentEmbedding1[document embedding 1]\n    Embedding--&gt;DocumentEmbedding2[document embedding 2]\n    Embedding--&gt;DocumentEmbedding3[document embedding 3]\n\n    subgraph CosineSimilarity[cosine similarity]\n    DocumentEmbedding1--&gt;CosineSimilarity1[0.1]\n    DocumentEmbedding2--&gt;CosineSimilarity2[0.8]\n    DocumentEmbedding3--&gt;CosineSimilarity3[0.3]\n    end\n    CosineSimilarity2--&gt;FinalResult1[final result]\n\n\n\n\n\nHyDEだと、query embeddingのところに工夫しました。直接QueryをEmbeddingに変換するのではなく、まずQueryに答えるドキュメントをLLMに生成させて、生成した仮想な答案をEmbeddingに変換します。\n\n\n\n\nflowchart LR\n    Input[query]--&gt;LLM\n    subgraph HyDE\n    LLM--&gt;FakeAnser[fake answer]\n    end\n        FakeAnser--&gt;QueryEmbedding[query embedding]\n    QueryEmbedding--&gt;DocumentEmbedding1[document embedding 1]\n    QueryEmbedding--&gt;DocumentEmbedding2[document embedding 2]\n    QueryEmbedding--&gt;DocumentEmbedding3[document embedding 3]\n    \n    subgraph CosineSimilarity[cosine similarity]\n    DocumentEmbedding1--&gt;CosineSimilarity1[0.1]\n    DocumentEmbedding2--&gt;CosineSimilarity2[0.8]\n    DocumentEmbedding3--&gt;CosineSimilarity3[0.3]\n    end\n    CosineSimilarity2--&gt;FinalResult1[final result]\n    style HyDE  stroke:#333,stroke-width:4px\n\n\n\n\n\n実際にLangChainで使いましょう。\n\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.chains import LLMChain, HypotheticalDocumentEmbedder\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chat_models import ChatOpenAI\nfrom dotenv import load_dotenv\n# set the environment variables\nload_dotenv()\n\n# prepare the prompt template for document generation\nprompt_template = \"\"\"質問を回答しなさい。\n質問：{question}\n回答：\"\"\"\nllm = ChatOpenAI()\n# multi_llm = ChatOpenAI(n=4)\nprompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\nllm_chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n\n# initialize the hypothetical document embedder\nbase_embeddings = OpenAIEmbeddings()\nembeddings = HypotheticalDocumentEmbedder(llm_chain=llm_chain, base_embeddings=base_embeddings)\n\nresult = embeddings.embed_query(\"ゼルダの伝説の主人公は誰ですか？\")\nlen(result)\n\n1536\n\n\nLangchainでHyDEを使うには、まずはHypotheticalDocumentEmbedderを初期化する必要があります。初期化する際に必要なのは、仮想な答案を生成するllm_chainと生成したテキストをEmbeddingに変換するbase_embeddingsです。\n\n\n\n\n\n\nTip\n\n\n\nllmを定義する時、一度生成するドキュメントの数を指定できます。例えばnを4に指定すると、一度4つのドキュメントを生成します。\n\n\n使用する際には、embedding.embed_queryを使ってQueryをEmbeddingに変換します。これで最終的に1536次元のベクトルが得られます。"
  },
  {
    "objectID": "posts/20230510_hyde_detailed/index.html#hypotheticaldocumentembedderの内部処理",
    "href": "posts/20230510_hyde_detailed/index.html#hypotheticaldocumentembedderの内部処理",
    "title": "LangChain Hypothetical Document Embeddings (HyDE) 全面解説",
    "section": "HypotheticalDocumentEmbedderの内部処理",
    "text": "HypotheticalDocumentEmbedderの内部処理\n次にHypotheticalDocumentEmbedderの内部は同様な処理になっているかを見ましょう。コアの関数は以下の2つです。\n    def combine_embeddings(self, embeddings: List[List[float]]) -&gt; List[float]:\n        \"\"\"Combine embeddings into final embeddings.\"\"\"\n        return list(np.array(embeddings).mean(axis=0))\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        \"\"\"Generate a hypothetical document and embedded it.\"\"\"\n        # generate n hypothetical documents\n        var_name = self.llm_chain.input_keys[0]\n        result = self.llm_chain.generate([{var_name: text}])\n        # get all the hypothetical documents from result\n        documents = [generation.text for generation in result.generations[0]]\n        # embed the hypothetical documents\n        embeddings = self.embed_documents(documents)\n        # combine the embeddings by averaging\n        return self.combine_embeddings(embeddings)\n毎回2つ仮想な答案を生成する場合のフロー図にすると以下のようになります。\n\n\n\n\nflowchart LR\n    Input([query])--&gt;llm_chain\n    subgraph HypotheticalDocumentEmbedder\n    llm_chain--&gt;ga1([generated answer 1])\n    llm_chain--&gt;ga2([generated answer 2])\n    ga1--&gt;OpenAIEmbeddings\n    ga2--&gt;OpenAIEmbeddings\n    OpenAIEmbeddings --&gt;embed1([embedding 1])\n    OpenAIEmbeddings --&gt;embed2([embedding 2])\n    end\n    embed1--&gt;combine([averaged embedding])\n    embed2--&gt;combine\n    style llm_chain  stroke:#333,stroke-width:4px\n    style OpenAIEmbeddings stroke:#333,stroke-width:4px\n\n\n\nHypotheticalDocumentEmbedderの処理の流れ"
  },
  {
    "objectID": "posts/20230510_hyde_detailed/index.html#実際のパフォーマンステスト",
    "href": "posts/20230510_hyde_detailed/index.html#実際のパフォーマンステスト",
    "title": "LangChain Hypothetical Document Embeddings (HyDE) 全面解説",
    "section": "実際のパフォーマンステスト",
    "text": "実際のパフォーマンステスト\nHyDEは普通のEmbedding手法と比べてどのぐらい優れているかを実際に確認しましょう。\n使うデータは多言語質問応答データセットである「Mr.TyDi」にある日本語データです。各Queryに対して、Positive DocumentとNegative Documentが与えられています。また、Queryの内容は基本的にWikiで検索できる一般的な知識です。なので、今回のHyDEには非常に適していると思います。\nデータはHuggingFaceのdatasetsからダウンロードします。データセットは7千件ありますが、コストを考慮して今回は100件のデータのみを使用します。また、テストする使うQueryの数は50件のみにします。つまり、50件のQueryに対して、合計200件(Pos + Neg)のドキュメントのランキングを行います。\n\nfrom datasets import load_dataset\nimport pandas as pd\n# to load all train, dev and test sets\ndataset = load_dataset('castorini/mr-tydi', \"japanese\", split=\"train\")\ntydi_df = pd.DataFrame(dataset).sample(100, random_state=42)\nfor col in [\"positive_passages\", \"negative_passages\"]:\n    tydi_df[col] = tydi_df[col].apply(lambda x: x[0][\"text\"])\ntydi_df_sample = tydi_df.iloc[:50,:].copy()\n\n\ntydi_df_sample.head(2)\n\n\n\n\n\n\n\n\nquery_id\nquery\npositive_passages\nnegative_passages\n\n\n\n\n1041\n1320\n有価証券とはなんですか？\n有価証券（ゆうかしょうけん）とは、伝統的には財産的価値のある私権を表章する証券で、その権利の...\n有価証券届出書の提出日以降、当該有価証券届出書の効力が発生する以前において、有価証券届出書に...\n\n\n670\n862\n浅草寺はいつ建設された\n推古天皇36年（628年）、宮戸川（現・隅田川）で漁をしていた檜前浜成・竹成（ひのくまのはま...\n1907年（明治40年）、昆虫学者名和靖は日露戦争の勝利記念に昆虫館を建設したいと考え、東京...\n\n\n\n\n\n\n\n確認する内容としては以下の3つとします。\n\nMRR: Mean Reciprocal Rank、平均逆順位です。総合的にパフォーマンスを確認することができます。\n正解数：上位1位は正解の数です。直感的にわかりやすいです。\n検索にかかる時間：HyDEはLLMでテキスト生成を行なうため、検索時間が大幅に増える予想です。\n\nまずは、普通のEmbedding手法を使って場合のパフォーマンスをテストしてみましょう。\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom tqdm.auto import tqdm\n\ndef get_rank(query, docs):\n    for i, doc in enumerate(docs, start=1):\n        if query == doc.metadata[\"query\"]:\n            return i\n\ndef test(test_query_list, vectorstore):\n    # fetch the documents\n    rank_list = []\n    for title in tqdm(test_query_list):\n        docs = vectorstore.similarity_search(title, k=200)\n        rank_list.append(get_rank(title, docs))\n\n    # summarize the results\n    return rank_list\n\ndef get_mrr(rank_list):\n    return sum([1/rank for rank in rank_list])/len(rank_list)\ndef get_correct_num(rank_list):\n    return len([rank for rank in rank_list if rank == 1])\n\n# prepare the vectorstore\ndocs = tydi_df[\"positive_passages\"].tolist() + tydi_df[\"negative_passages\"].tolist()\nmeta_datas = [{\"query\": q} for q in tydi_df[\"query\"].tolist()] + [{\"query\": \"\"} for q in tydi_df[\"query\"].tolist()]\nbase_embeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_texts(\n    texts=docs,\n    embedding=base_embeddings,\n    metadatas=meta_datas,\n)\n\n\nrank_list = test(tydi_df_sample[\"query\"].tolist(), vectorstore)\n\n\n\n\n\nprint(f\"mrr: {get_mrr(rank_list):.3f}\")\nprint(f\"correct num: {get_correct_num(rank_list)}\")\n\nmrr: 0.855\ncorrect num: 37\n\n\n普通のEmbedding手法だと、50件のQueryの中、37件のドキュメントを正しく返せました。MRRは0.855、また、処理時間は17秒でした。\n次にHyDEを使ってテストします。\n\nfrom langchain.chat_models import ChatOpenAI\nprompt_template = \"\"\"質問に答えてください。\n質問：{question}\n答案：\"\"\"\nllm = ChatOpenAI(verbose=True)\nprompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\nllm_chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\nembeddings = HypotheticalDocumentEmbedder(llm_chain=llm_chain, base_embeddings=base_embeddings)\nvectorstore.embedding_function = embeddings.embed_query\n\n\nhyde_rank_list = test(tydi_df_sample[\"query\"].tolist(), vectorstore)\n\n\n\n\n\nprint(f\"mrr: {get_mrr(hyde_rank_list):.3f}\")\nprint(f\"correct num: {get_correct_num(hyde_rank_list)}\")\n\nmrr: 0.813\ncorrect num: 35\n\n\n意外ですが、HyDEを使うと逆に正解数が減りました。正解数は35件、MRRは0.813、処理時間は4分15秒でした。"
  },
  {
    "objectID": "posts/20230510_hyde_detailed/index.html#hydeの改善",
    "href": "posts/20230510_hyde_detailed/index.html#hydeの改善",
    "title": "LangChain Hypothetical Document Embeddings (HyDE) 全面解説",
    "section": "HyDEの改善",
    "text": "HyDEの改善\nHyDEは生成した仮想な答案をEmbeddingにしていますが、逆に重要なQueryの情報を捨てています。なので、仮想な答案をEmbeddingする前にQueryの情報を仮想な答案に加えることができれば、もっとパフォーマンスを改善できると考えられます。\nその改善をしてみましょう。そのためには、まずHypotheticalDocumentEmbedderを継承したクラスを作り、embed_queryを再定義する必要があります。\n\nclass HyDEWithTitle(HypotheticalDocumentEmbedder):\n\n    def embed_query(self, text: str):\n        \"\"\"Generate a hypothetical document and embedded it.\"\"\"\n        var_name = self.llm_chain.input_keys[0]\n        result = self.llm_chain.generate([{var_name: text}])\n        documents = [generation.text for generation in result.generations[0]]\n        # add query to the beginning of the document\n        documents = [f\"{text}\\n{document}\" for document in documents]\n        embeddings = self.embed_documents(documents)\n        return self.combine_embeddings(embeddings)\n\nembeddings = HyDEWithTitle(llm_chain=llm_chain, base_embeddings=base_embeddings)\nvectorstore.embedding_function = embeddings.embed_query\n\n\nhyde_with_title_rank_list = test(tydi_df_sample[\"query\"].tolist(), vectorstore)\n\n\n\n\nRetrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.&lt;locals&gt;._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0d85be599b6abc67a7a59f467a5101cd in your message.).\n\n\n\nprint(f\"mrr: {get_mrr(hyde_with_title_rank_list):.3f}\")\nprint(f\"correct num: {get_correct_num(hyde_with_title_rank_list)}\")\n\nmrr: 0.897\ncorrect num: 40\n\n\n検索するQueryを仮想な答案に追加することにより、正解数が多くなりましたし、全体のランクも上がりました。"
  },
  {
    "objectID": "posts/20230510_hyde_detailed/index.html#まとめ",
    "href": "posts/20230510_hyde_detailed/index.html#まとめ",
    "title": "LangChain Hypothetical Document Embeddings (HyDE) 全面解説",
    "section": "まとめ",
    "text": "まとめ\nHyDEは予想より精度の改善が得られなかったです。Queryを仮想な答案に追加することにより、精度は普通のEmbedding手法より上がりました。しかし、処理時間が大幅に増えてしまいました。また、今回は測っていないですが、1件あたりのコストも何倍になると思うので、実際に使う場合は、精度と処理時間、コストを総合的に考えて使う必要があります。\n\n\n\n手法\n正解数(50件の中)\nMRR\nスピード\n\n\n\n\n普通のEmbedding\n37\n0.855\n17秒\n\n\nHyDE\n37\n0.813\n4分15秒\n\n\nHyDE with title\n40\n0.897\n5分2秒"
  },
  {
    "objectID": "posts/20230430_HyDE/index.html",
    "href": "posts/20230430_HyDE/index.html",
    "title": "HyDE 論文解読",
    "section": "",
    "text": "HyDE論文では、教師なしのZero-shot dense retrievalシステムを提案。従来のDense retrieverとは異なり、HyDEはQueryから仮想的なDocumentを生成し、その類似度でランキング。InstructGPTで仮想ドキュメントを生成し、Contrieverを使ってEmbeddingに変換。様々なデータセットでテストした結果、HyDEは教師なし領域で従来のContrieverを凌駕し、教師ありモデルとも遜色ない精度を示した。実装はLangChainで利用可能。\n論文URL：https://arxiv.org/abs/2204.07496"
  },
  {
    "objectID": "posts/20230430_HyDE/index.html#related-works",
    "href": "posts/20230430_HyDE/index.html#related-works",
    "title": "HyDE 論文解読",
    "section": "2 Related works",
    "text": "2 Related works"
  },
  {
    "objectID": "posts/20230430_HyDE/index.html#methology",
    "href": "posts/20230430_HyDE/index.html#methology",
    "title": "HyDE 論文解読",
    "section": "3 Methology",
    "text": "3 Methology"
  },
  {
    "objectID": "posts/20230430_HyDE/index.html#experiments",
    "href": "posts/20230430_HyDE/index.html#experiments",
    "title": "HyDE 論文解読",
    "section": "4 Experiments",
    "text": "4 Experiments\n仮想なDocumentはInstructGPTで生成した。生成したDocumentをContrieverを用いてEmbeddingに変換した。\nテストのデータとしては、MS-MARCOをベースとしたTREC DL19 DL20があり、BEIRからもLow-resourceのデータセットをいくつ利用した。また、英語以外、韓国語、日本語等データセットも使った。\n\n\n\nweb search query sets\n\n\n\n\n\nlow-resource datasets\n\n\n\n\n\nnon-English retrieval\n\n\n結果を見ると、教師なしの領域でHyDEは全面的に以前のContrieverを超えた。また、教師あるのモデルから比較しても遜色しない精度を出した。"
  },
  {
    "objectID": "posts/20230430_HyDE/index.html#analysis",
    "href": "posts/20230430_HyDE/index.html#analysis",
    "title": "HyDE 論文解読",
    "section": "5 Analysis",
    "text": "5 Analysis\n\n\n\nLLM difference\n\n\n当たり前だが、仮想なドキュメントを生成するLLMによって最終の精度が違う。また、HyDEは教師なしの手法だが、教師ありのRetrieverの精度も向上できる。"
  },
  {
    "objectID": "posts/20230430_HyDE/index.html#実装",
    "href": "posts/20230430_HyDE/index.html#実装",
    "title": "HyDE 論文解読",
    "section": "実装",
    "text": "実装\nHyDEはすでにLangChainで実装されている。\nfrom langchain.llms import OpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.chains import HypotheticalDocumentEmbedder\n\nbase_embeddings = OpenAIEmbeddings()\nllm = OpenAI()\n\n# Load with `web_search` prompt\nembeddings = HypotheticalDocumentEmbedder.from_llm(llm, base_embeddings, \"web_search\")\n\n# Now we can use it as any embedding class!\nresult = embeddings.embed_query(\"Where is the Taj Mahal?\")"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html",
    "href": "posts/20230429_promptagator/index.html",
    "title": "Promptagator 論文解読",
    "section": "",
    "text": "Promptagator論文では、Few-shot Retrieval settingを提案し、異なる検索タスクに対応するために専用のPromptを作成してLLMでDocumentに関連するQueryを生成。生成されたQueryをフィルタリングし、Retrieverを学習させる。実験結果から、Promptagatorは既存の50万件以上の学習データを使ったモデルより高い精度を達成し、教師ありのRetrieverよりも優れていることが分かった。\n論文URL：https://arxiv.org/abs/2209.11755"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#prompt-base-query-generation",
    "href": "posts/20230429_promptagator/index.html#prompt-base-query-generation",
    "title": "Promptagator 論文解読",
    "section": "3.1 Prompt-base query generation",
    "text": "3.1 Prompt-base query generation\nLLMはFLANを利用した。Promptの形式としては、HotpotQAを例として説明すると以下になる。\nEvidence: passage 1 \nVexed question: query 1\n...\nEvidence: passage k\nVexed question: query k\nEvidence: target passage\n下表のようにタスクごとに違うPromptを設定した。（Promptの中の0と 1の意味が不明）\n\nPromptで使用した例は最大8個にし、例の長さによって調整している。文書が長い場合は、必要に応じて切断している。\n各コーパスから最大100万のドキュメントを抽出し、各ドキュメントで8個のQueryを生成している。LLMはFLAN137Bを使った。生成する際に0.7の温度を使った。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#consistency-filtering-using-only-generated-data",
    "href": "posts/20230429_promptagator/index.html#consistency-filtering-using-only-generated-data",
    "title": "Promptagator 論文解読",
    "section": "3.2 Consistency filtering using only generated data",
    "text": "3.2 Consistency filtering using only generated data\n生成したQueryに対して、生成元がその答案を含む必要がある。今までの研究で、その原則で生成したQueryをフィルタリングすることは重要であることがしめされている。\n過去の研究の中で外部の質問応答モデルを用いて実現していたが、この研究では生成したデータで初期のRetrieverを学習されている。各生成したQueryに対して、Retrieverが検索したTopKの中に生成元のドキュメントが含まれていない場合は、そのQueryを除外する。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#few-shot-promptagator-retriever",
    "href": "posts/20230429_promptagator/index.html#few-shot-promptagator-retriever",
    "title": "Promptagator 論文解読",
    "section": "3.3 Few-shot promptagator retriever",
    "text": "3.3 Few-shot promptagator retriever\nDural EncoderのRetrieverを利用している。ベースモデルはT5で、それをC4（Common Crawlのweb crawlコーパス）データセットを使って、Contriverが使用したindependent cropping taskでさらに学習させた。（independent cropping taskとは、同じ文書の異なる部分のペアをPositive example、異なる文書のテキストのペアをNegative exampleとして、教師なしでRetrieverを学習する手法）\nその後、生成されたQueryとDocumentのペアを使って継続的に学習させる。学習時にBatch内のQueryとDocumentのペアをシャッフルしてNegative exampleとする。また、一定のStep数を学習した後、それを初期のRetrieverとして生成されたQueryのフィルタリングを行う。フィルタリングした後、継続的に学習させる。\nまた、Promptagator++というRerankerも提案した。学習データがRetrieverと同じだが、モデルはもっと精度が高く、推論時間が長いCross-attention modelを使った。Retrieverから取得した上位の200件のDocumentから31個のDocumentをサンプリングして、Negative exampleとして使っている。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#zero-shot-promptagator-retriever",
    "href": "posts/20230429_promptagator/index.html#zero-shot-promptagator-retriever",
    "title": "Promptagator 論文解読",
    "section": "3.4 Zero-shot promptagator retriever",
    "text": "3.4 Zero-shot promptagator retriever\nZero-shotでQueryを生成する場合は以下の形式でPromptを書いた：\nf'{d} Read the passage and generate a query.'"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#implementation",
    "href": "posts/20230429_promptagator/index.html#implementation",
    "title": "Promptagator 論文解読",
    "section": "4.1 Implementation",
    "text": "4.1 Implementation\nQueryを生成する際に温度を0.7にした。\n生成したQueryをフィルタリングする際にKを1にした。\nDual Encodersは同じT5-base v1.1 のEncoder(110M)を使っている。Encoderのトップの層を平均し、768次元のEmbeddingへ投影した。\nPromptagator++のRerankerもT5-base v1.1 のEncoder(110M)を使っているが、Cross AttentionのEncoderにしている。\nFine-tuningする際にの具体的なBatch sizeとStepsが下表の通り：\n\n\n\nModel type\nDataset size\nBatch size\nFine tune steps\n\n\n\n\nDual encoder\nBig(&gt;500k)\n6k\n5k\n\n\nDual encoder\nSmall(&lt;=500k)\n128\n1k\n\n\nReranker\nBig(&gt;500k)\n64\n20k\n\n\nReranker\nSmall(&lt;=500k)\n64\n5k"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#main-results",
    "href": "posts/20230429_promptagator/index.html#main-results",
    "title": "Promptagator 論文解読",
    "section": "4.2 Main Results",
    "text": "4.2 Main Results\n\n\n\nMain result\n\n\n表の前半ではRetrieverの比較が行われている。Zero-shotのPromptagatorはすでに大多数のMS MARCOでFine-tuningした教師ありのRetrieverと同等な精度を出している。Few-shotのPromptagatorはさらに教師ありのRetrieverより高い精度を出している。\n後半ではRetriever+Rerankerの組み合わせの比較になる。Retrieverと同じ傾向で、Zero-shotでかなり良い精度を出している。Few-shotになるとさらに更に精度が3%向上し、Sotaになっている。\nまた、Promptagatorのもう一つ優れている点はモデルのサイズである。他のモデルは大体3Bの大きさだが、Promptagatorはわずか110Mのみである。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#abalation-study",
    "href": "posts/20230429_promptagator/index.html#abalation-study",
    "title": "Promptagator 論文解読",
    "section": "4.3 Abalation Study",
    "text": "4.3 Abalation Study\n\n\n\nAbalation study\n\n\nQueryフィルタリングの効果 Figure2の左図は、Queryを一回フィルタリングした効果をしめしている。大多数のデータセットにとって、Queryフィルタリングが有効だが、逆効果のものも存在する。NFCorpus and SciFactは小さいデータセットなので、フィルタリングで過学習している可能性がある。\nまた、詳細にフィルタリングされた例をみると、多くケースはQueryは一般化過ぎて多数のドキュメントにマッチングされていること、もしくは単純にQueryが間違っていることがわかる。\n生成したQueryで人間のデータを代替することができるか？ Figure2の真ん中の図は、8-shotのPromptagatorは5万件の人間がラベリングしたデータと同じ効果であることを示している。\nPromptagatorのQuery生成が効いているか？ Figure2の右図のGenQはBEIR論文の中で提案されたモデル、NQ-QGenはこの論文提案した方法でNQデータセット学習したモデル、NQ-QGenとGenQの違いはQuery生成の部分のみ。NQ-QGenの精度は2.7%高いため、提案したQuery生成の方法が有効だと言える。\n\n\n\nImpact of FLAN\n\n\nFLANの影響 PromptagatorのLLMはFLANを利用している。FLANの学習データの中にNQとQuaroデータが含まれいている。その影響を検証するため､それらを除いたデータセットでFLANを学習し、その結果を比較した(さすがGoogle Research、金ならあるの感じ)。その結果、精度は若干低下したが、以前の研究よりは高い精度を達成している。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#qualitative-analysis",
    "href": "posts/20230429_promptagator/index.html#qualitative-analysis",
    "title": "Promptagator 論文解読",
    "section": "4.4 Qualitative Analysis",
    "text": "4.4 Qualitative Analysis\n\n\n\nTop word distribution\n\n\nQueryの最初のWordの分布を調査した。Few-shotが生成したQueryの分布は実際のQueryの分布と同じであることがわかる。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#neural-retrieval-models",
    "href": "posts/20230429_promptagator/index.html#neural-retrieval-models",
    "title": "Promptagator 論文解読",
    "section": "Neural retrieval models",
    "text": "Neural retrieval models\nNeural retrieval modelはrepresentation based modelとinteraction based modelの2つに分類できる。\nRepresentation based modelはQueryとDocument両方とも分散表現に変換し、分散表現の類似度で相関性を測っている。最近の研究は以下のことにフォーカスしている：\n\nより良い前学習するタスクやモデルのアーキテクチャを開発する\nMulti-Vectorでより分散表現の表現性能を向上させる\nよりよいNegative sample手法を開発する\nドメイン横断での汎化性能を向上させる\n\nInteraction based modelだと、QueryとDocumentを一緒に処理するため、精度が高い。一方、計算コストが高いため、Rerankerとして使われることが多い。それに関する研究は以下のものがある：\n\nInteraction based modelを蒸留し、Representation based modelとして使う。\nInteractionを最後にさせること。"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#prompt-based-query-generation",
    "href": "posts/20230429_promptagator/index.html#prompt-based-query-generation",
    "title": "Promptagator 論文解読",
    "section": "Prompt-based query generation",
    "text": "Prompt-based query generation\nUPR：直接LLMSを使ってRerankを行う\nInPars：GPT3でQueryを生成し、T5のRerankerを学習する"
  },
  {
    "objectID": "posts/20230429_promptagator/index.html#retrievers-with-late-interactions",
    "href": "posts/20230429_promptagator/index.html#retrievers-with-late-interactions",
    "title": "Promptagator 論文解読",
    "section": "Retrievers with late interactions",
    "text": "Retrievers with late interactions\nDual encoder modelの効率が良いが、QueryとDocumentの相互作用は最後の内積のみであるため、性能が弱い。ColBERTとSPLADEは最後のToken-level interactionを使ったため、計算コストを少し犠牲して性能を向上させた。"
  },
  {
    "objectID": "posts/20230523_speech_foundation_models_part2/index.html",
    "href": "posts/20230523_speech_foundation_models_part2/index.html",
    "title": "音声基礎モデルの紹介 Part2",
    "section": "",
    "text": "音声基礎モデルに関する内容を3部分に分けて紹介しています。このポストはPart2と3について紹介します。\n前のパートはこちらです。\n内容はYoutubeで音声認識に関する講義に基づいています。\n本文の内容はYoutubeで音声認識に関する講義に基づいています。全体は3部分に分けられます。前のパートでは、音声表現学習に焦点を当て、対照学習モデルと予測モデルの両方について説明しました。本文はPart2からの音声大型言語モデルとその他の音声基礎モデルについて説明します。\n資料のリンクはこちらです。"
  },
  {
    "objectID": "posts/20230523_speech_foundation_models_part2/index.html#part2-speech-large-language-models",
    "href": "posts/20230523_speech_foundation_models_part2/index.html#part2-speech-large-language-models",
    "title": "音声基礎モデルの紹介 Part2",
    "section": "Part2 Speech Large Language Models",
    "text": "Part2 Speech Large Language Models"
  },
  {
    "objectID": "posts/20230523_speech_foundation_models_part2/index.html#textless-nlp-project",
    "href": "posts/20230523_speech_foundation_models_part2/index.html#textless-nlp-project",
    "title": "音声基礎モデルの紹介 Part2",
    "section": "Textless NLP Project",
    "text": "Textless NLP Project\n音声翻訳を行なう場合、通常はCascaded Systemという複数のモデルを繋ぐ方法が使われます。まず音声をテキストに変換し、変換されたテキストを機械翻訳のモデルに通して翻訳します。翻訳したテキストを再びTTSモデルで音声に変換します。\nしかし、この方法だと、モデルを学習するためにテキストデータが必要になります。また、複数の中間ステップがあるため、中間ステップで一つ間違える最終結果が間違ってしまいます。\n\nTextless NLPでは、中間ステップで生成されるテキストを「疑似テキスト（Psuedo text）」に置き換えることで、テキストを必要とせず、音声から音声へのエンドツーエンド学習が可能になりました。また、疑似テキストを用いて、現在のNLP技術を活用して中間ステップの改善ができます。\n\nさらに、音声翻訳のみではなく、中間のモデルを差し替えることで別のタスクに対応することができます。例えば、Speech continuationの場合はGPTを使います。音声合成の場合は中間のモデルを使わないです。\n\n\n\n\n\n\nSpeech continuation\n\n\n\nSpeech continuation（スピーチ・コンティニュエーション）は、音声認識や自然言語処理技術を用いて、話者の発言を他の話者が続けることができるようにする研究です。この研究は、人間と人工知能（AI）がスムーズにコミュニケーションできるようにすることを目的としています。具体的には、話者の言語パターンやスタイルを学習し、それに基づいて自然で適切なレスポンスを生成する技術を開発しています\n\n\n\nPsuedo textの作り方はPart 1で紹介した音声の量子化と同じです。HuBERTを使って音声の特徴を抽出し、K-meansで得たカテゴリをTokenとして扱います。\n\n\nGenerative Spoken Language Modeling\nTextless NLPでSpeech continuationの例を説明します。まず、HuBERTを使用して音声をトークンに変換します。ここでTokenを得たことで、次のステップは完全にNLPの領域になります。GPTと同じようなモデルを使って次のTokenを予測するモデルを作ることができます。\n\n最後に予測されたTokenをDecoderに渡し、音声を合成してもらいます。音声合成モデルも学習する必要があります。学習の方法としては、音声とTokenのペアを用意して、Tokenをインプットし、音声をアウトプットするモデルを学習します。\n\n中間のモデルは言語モデルなので、条件なしの音声生成もできます。一方で、条件なしでは、一見意味があるように見えるものが出力されますが、よく観察すると、意味が通じないものが生成されることがあります。\n\n\n\nSpeech Resynthesis\nTextless NLPは音声合成もできます。一方、この場合はインプットから内容、音調、そして話者の3つの特徴量を抽出しています。\n\nなぜここで別の特徴量が必要かというと、量子化することにより音声内容が抽出され、逆に話者の情報が落ちてしましました。下図の右の表でその実験結果があります。話者識別の精度について、量子化しないHuBERTだと99%の精度を得られますが、量子化の粒度が粗いほど話者識別の精度が悪くなります。CPCモデルも同じ傾向です。\n\n同じモデルでコナンの蝶ネクタイ型変声機のようなVoice Conversionもできます。Vioce Conversionとは、音声の内容は同じですが、話者がほかの人にすることです。Speaker Embeddingを差し替えるだけでできます。\nまた、この研究でHuBERTがいいモデルということもわかりましが。\n\n同じフレームワークでSpeech codecもできます。speech codecは、音声信号をデジタルデータに変換し、デジタルデータを音声信号に戻す技術の研究分野です。この分野の目的は、音声データを効率的に圧縮し、伝送やストレージに適した形式にすることです。\nこのモデルはまず音声を特徴量化(下図の赤枠)にして、転送先にDecoderがあれば似た音声を生成することができます。右の図からその効果がわかります。横軸は圧縮した後の情報量、縦は復元した音声の品質、HuBERTはを使ったモデルはこの図の左上にあります。つまり、少ない情報量で高品質な音声を復元できることが示されています。\n\n\n\nSpeech Translation: Unit BART\n音声翻訳もできます。英語とスペイン語の音声翻訳を例に挙げます。この時真ん中にあるものはBARTというEncoderとDecoderを兼ね備えたTransfomerです。\n英語とスペイン語のデータは、ペアではなくても大丈夫です。最初に、英語またはスペイン語の音声をマルチ言語のHuBERTに通してトークンを取得します。続いて、BARTにトークンを入力し、Auto-encoderによる学習を行います。つまり、入力トークンの一部をマスキングして復元することで学習が進みます。\n最終的に、英語とスペイン語の両方を適切に復元できるBARTモデルが完成します。\n\nただし、学習されたBARTモデルは入力言語のみを復元できるため、翻訳を行うにはペアデータを使って学習する必要があります。この論文では、エンコーダーをWav2vec 2.0に置き換えて実験が行われています。元のBARTを使用しても問題ないですが、Wav2vec 2.0の方が性能が良かったため採用されました。\n\nこのアプローチにより、Textless NLPはテキストデータがない状況でも、従来のCascaded systemと同等の精度を達成しています。\n\n\n\nUnitY\nこれまで紹介したモデルはテキストデータが必要ではないですが、逆に言えば、テキストデータがあっても使えないです。その問題を解決できるのがUnitYモデルです。\nこのモデルは4つの部分によって構成されています。音声が入力されると、まずSpeech encoderを通過し、得られた隠れ層をText decoderに入力します。その後、処理が分岐します。\n音声とテキストのペアデータが存在する場合、補助タスク(Auxiliary task)としてテキストの予測が可能です。それと同時に、Text decoderから出力された別のベクトルが次のEncoderに送られ、最終的に音声が出力されます。\n\nこのモデルは、音声とテキストのペアだけでなく、すべての形式のデータを学習に活用できます（ラベルなしテキスト、ラベルなし音声、ラベル付き音声、ペア音声のすべてのデータ形式）。\n\nUnitYはCascaded systemより精度が良いです。\n\n以前Metaが出した英語と福建語の音声翻訳デモはこのUnitYを使いました。下のビデオをご覧ください。\n\n\nビデオでは、音声翻訳時に出力される声が本人の声ではないことがわかります。これは、音声を量子化する際に話者の情報が失われるためです。"
  },
  {
    "objectID": "posts/20230523_speech_foundation_models_part2/index.html#audiolm",
    "href": "posts/20230523_speech_foundation_models_part2/index.html#audiolm",
    "title": "音声基礎モデルの紹介 Part2",
    "section": "AudioLM",
    "text": "AudioLM\nAudioLMはGoogleが2022年10月出した研究です。ここで実際に生成したサンプルを見ることができます。\nTextless NLPとは異なり、AudioLMは話者の情報を維持しています。音声の意味(Sementic)以外に、音響(Acoustic)も同じような方法で量子化し、モデルに入力しています。\nAcoustic Tokenは、Codec Encoderによって抽出され、Codec Decoderから出力されます。 \nCodecモデルは、オートエンコーダーの方法で入力された音声を復元するタスクを学習します。\n通常の自然言語処理のTokenとは異なり、Acoustic Tokenには複数の層があります。抽出方法は、まず音声をエンコーダーに通し、出力された複数のベクトルに対して、最も近いTokenを探すところから始まります。ここではA3, A2, A6がその結果でした。次に、一層目で得たベクトルとA3, A2, A6の差分を取って、次の層にインプットして同じようなことをします。これによって、最終的にN層のTokenを得られるます。得たTokenをDecoderに渡してDecodeします。\n各層の入力は、前の層の出力とTokenのベクトルの差分であり、値が徐々に小さくなります。これによって、必要に応じ層を少なくして、音声の質をあまり落とさないまま、音声の圧縮することもできます。\n\n音声生成において、中間のunit LMは3つのTransformerを連結して構成されます。\n\n音声出力のプロセスは以下の通りです。\n\n最初のモデルは過去のSemantic Tokenを入力とし、次に話す内容、つまり未来のSemantic Tokenを予測します。\n生成された未来のSemantic Tokenと過去の荒いAcoustic Tokenを組み合わせて、2番目のモデルに入力し、未来の荒いAcoustic Tokenを予測します。\n2で生成された荒いAcoustic Tokenを3番目のモデルに入力し、良いAcoustic Tokenを生成します。\n2と3の結果を組み合わせてデコーダーに渡し、音声が生成されます。\n\n\nこの方法では、音声だけでなく音楽の生成も可能で、以下のビデオでサンプルを聞くことができます。"
  },
  {
    "objectID": "posts/20230523_speech_foundation_models_part2/index.html#vall-e",
    "href": "posts/20230523_speech_foundation_models_part2/index.html#vall-e",
    "title": "音声基礎モデルの紹介 Part2",
    "section": "VALL-E",
    "text": "VALL-E\nVALL-Eは、2023年1月にMicrosoftが公開したTTS（Text to Speech）モデルです。その特徴は、テキストだけでなく、最大3秒の音声も入力できる点です。この機能により、出力される音声が入力音声と同じ人が話しているかのように聞こえます。実際の例はこのブログで見れます。\n\n前も言及したが、Acoustic Tokenの中で一番重要なのは最初の層です。VALL-Eは生成音声の質と生成速度をトレードオフのため、重要な一層目は質が高いがスピードが遅い自己回帰モデル(Autoregressive)を使いました。残りの層だとスピードが早いが、質が良くない非自己回帰モデル(Non-Autoregressive)を使いました。 以前も触れましたが、Acoustic Tokenの中で最も重要なのは最初の層です。VALL-Eは、生成音声の品質と生成速度のトレードオフを考慮し、最初の層には品質が高いものの速度が遅い自己回帰モデル(Autoregressive)を採用しています。その後の層には、速度が速いが品質は劣る非自己回帰モデル(Non-Autoregressive)を使用しています。\n\n\n\n\n\n\nAutoregressiveとNon-Autoregressiveの違いは？\n\n\n\n\n\nAutoregressive（自己回帰）とNon-Autoregressive（非自己回帰）は、機械学習や自然言語処理の文脈で使われる用語です。これらは、特にシーケンスデータ（テキスト、音声など）の生成や予測に関連しています。\nAutoregressiveモデルは、シーケンスの各要素を直前の要素に依存して生成するモデルです。つまり、出力の予測には、すでに生成された過去の要素が必要です。例えば、自然言語処理の場合、次の単語を予測するために、前の単語の情報を使用します。具体的な例としては、言語モデルの一種である従来のリカレントニューラルネットワーク（RNN）があります。このモデルでは、過去の出力を入力として使用して次の出力を生成します。\n一方、Non-Autoregressiveモデルは、各要素の生成が直前の要素に依存せず、同時に独立して生成されるモデルです。これは、逐次的な生成を必要とせず、並列的に処理できる利点があります。具体的な例としては、Transformerモデルがあります。Transformerモデルでは、異なる位置の要素を同時に処理し、出力を生成します。これにより、高速な並列処理が可能となります。\nAutoregressiveモデルは、文脈の長さに依存するため、生成に時間がかかる場合があります。一方、Non-Autoregressiveモデルは、並列処理が可能なため、生成速度が速くなる傾向がありますが、生成の品質や文脈の考慮が制限される可能性もあります。\n\n\n\n\nVALL-Eは、最先端のTTSシステムであり、生成された音声と元の音声の話者の類似度が高いことが特徴です。さらに、声の感情や音の環境も入力音声に近いものを出力することができます。"
  },
  {
    "objectID": "posts/20230523_speech_foundation_models_part2/index.html#part-3-other-speech-foundation-models",
    "href": "posts/20230523_speech_foundation_models_part2/index.html#part-3-other-speech-foundation-models",
    "title": "音声基礎モデルの紹介 Part2",
    "section": "Part 3 Other Speech Foundation Models",
    "text": "Part 3 Other Speech Foundation Models\nPart1とPart2で説明したモデルは主にラベルなしのデータを活用する目的でした。WhisperとUSMでは、そもそも学習データの量がものすごく多いため、モデルの構造上新規性がないですが、良い効果が出ています。\n\n\nWhisper\nWhisperは、OpenAIが2022年9月にリリースしたSTT（Speech to Text）モデルです。このモデルは、680,000時間の膨大なラベル付きデータを使用して学習されています。モデルの構造は普通のTransfomerのEncoder-Decoder構造です。\n\nWhisperは、マルチタスク学習を行っています。最初に音声が存在するかどうかを判断し、音声がない場合は&lt;EOS&gt;トークンを出力して終了します。音声が存在する場合は、まず音声の言語を判断し、続いてTranscribeタスクを実行してテキストに変換します。翻訳タスクの場合は、目標言語のテキストを出力します。\n\nWhisperの学習データは非常に多く、例えば多言語音声データが12万時間、英語音声データが44万時間、他言語と英語のテキストを含む音声データが12万時間利用されています。\n\nWhisperはオープンソースのモデルであるため、自分で実行することが可能です。また、OpenAIはAPIも提供しています。\n\n\n\nUSM\nUSMは、2023年3月にGoogleが開発した音声認識モデルです。1200万の音声データ(さすがGoogle)を用いて事前学習が行われ、その後、Whisperの1/7のラベル付きデータを使ってFine-Tuningが実施されました。\n\nUSMモデルは、音声データだけでなく、テキストデータも活用しています。\nモデルの構造としては、上流にSpeech EncoderとText Embeddingが設置され、下流にEncoderが存在します。\n学習する際に、3つのタスクを学習しました。\n\n音声データのみを用いる場合、量子化手法を使って音声を学習する。\nテキストのみの場合は、RNN-T decoderを使ってテキストを復元するタスクで学習する\n音声とテキストのペアがある場合は、普通のFine-Tuinigをする\n\n\nこの方法により、Whisperよりも優れたモデルが構築されました。ただし、USMはオープンソースではなく、GoogleのAPIを通じてのみ利用可能です。"
  },
  {
    "objectID": "posts/20230523_speech_foundation_models_part2/index.html#まとめ",
    "href": "posts/20230523_speech_foundation_models_part2/index.html#まとめ",
    "title": "音声基礎モデルの紹介 Part2",
    "section": "まとめ",
    "text": "まとめ\n近年の進歩により、疑似テキストを活用したエンドツーエンドの音声学習が実現し、Textless NLPやAudioLMなどのモデルが音声翻訳や音声合成などのタスクで活躍しています。また、VALL-E、Whisper、USMなどのモデルは、音声認識やテキストから音声への変換などの分野で効果を発揮しています。これらのモデルは、大量のデータを利用して学習され、音声関連タスクの精度向上に寄与しています。"
  },
  {
    "objectID": "posts/20230518_grammar_checker/index.html",
    "href": "posts/20230518_grammar_checker/index.html",
    "title": "GPT文書校正アシスタント",
    "section": "",
    "text": "ChatGPTが登場する前、書いたものが正しいかをチェックするために、いつも「Back Translation」の手法を使っていました。つまり、DeepLでまず書いた日本語を中国語に訳し、正しい意味を伝えたかを確認します。修正したものを日本語に訳し、それに基づいて元の文書を修正します。\nChatGPTができた後は、全部ChatGPTに任せています。でも、ChatGPTで校正を行う時に、使いにくい点は2つあります。\nまず、毎回校正のプロンプトを書かなければならないため、面倒です。\nまた、校正してくれた文書をそのままコピー＆ペーストではなく、元の文書と比較して、どこが変わったかを確認する必要があります。しかし、ChatGPTにはGitのDiffの機能がないため、一文ずつ自分で確認しなければなりません。\nその問題を解決するために、自分でOpenAIのAPIを利用して、日本語文書校正アシスタントを作りました。"
  },
  {
    "objectID": "posts/20230518_grammar_checker/index.html#特徴",
    "href": "posts/20230518_grammar_checker/index.html#特徴",
    "title": "GPT文書校正アシスタント",
    "section": "特徴",
    "text": "特徴\n\n日本語と英語の文法や誤字脱字を自動的に修正します。\n入力されたテキストの言語を自動的に判定します。\n校正結果を元のテキストと比較して表示します\nユーザーが毎回校正のプロンプトを書く手間を省きます。\n\nプロジェクトのメインの流れは以下です。\n\n\n\n\ngraph TB\nA[ユーザーが文書を入力] --&gt; B[言語を検出]\nB --&gt;|日本語| C[日本語のプロンプトを選択]\nB --&gt;|英語| D[英語のプロンプトを選択]\nC --&gt; E[OpenAI APIによる校正]\nD --&gt; E\nE --&gt; F[校正後のテキストを表示]\nE --&gt; G[原文と校正後のテキストの差分を計算]\nG --&gt; H[差分を表示]\n\n\n\n\n\n実際のスクリーンショットは以下の通りです。\n\nツールをデプロイし、以下に埋め込みました。試してみてください。また、こちらで直接デプロイしたページをご覧いただくこともできます。\n\n\nコードはこちらです。 https://github.com/DayuanJiang/gpt_grammar_checker"
  },
  {
    "objectID": "posts/20230428_inpars/index.html",
    "href": "posts/20230428_inpars/index.html",
    "title": "InPars 論文解読",
    "section": "",
    "text": "InPars論文では、擬似データ（Pseudo data）生成によるランキングモデル学習手法を提案。LLMを用いて擬似データを生成し、それを使ってモデルを学習させることで、情報検索（IR）の精度を向上させる。実験では、生成された擬似データでMonoT5をFine-tuningし、結果として従来のUnsupervisedモデルより優れた性能を示した。\n論文URL：https://arxiv.org/abs/2202.05144"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#introduction",
    "href": "posts/20230428_inpars/index.html#introduction",
    "title": "InPars 論文解読",
    "section": "1. Introduction",
    "text": "1. Introduction\nLLMは高性能を誇るものの、情報検索（IR）への応用が制限されている理由として、大規模な計算量が必要であることと、コストが高いことが挙げられる。GPT-3の埋め込みサービスを利用する場合、すべてのテキストを少なくとも1回は処理する必要があり、件数が多い場合、コストが膨大になることが問題となる。\nまた、学習データにも課題が存在し、現存するデータが商用利用に適さないものが多く、また、既存のデータを用いて学習したモデルが他の領域に汎用性を持たないという問題がある。\nこの論文では、検索推論にLLMを直接使用するのではなく、LLMを用いて擬似データ（Pseudo data）を生成し、そのデータを使ってランキングモデルを学習する手法を提案している。"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#related-work",
    "href": "posts/20230428_inpars/index.html#related-work",
    "title": "InPars 論文解読",
    "section": "2. Related work",
    "text": "2. Related work\nこれまで情報検索（IR）領域におけるデータ生成の研究では、BM25を用いて類似度が高いドキュメントをペアとしてモデルを学習する研究が存在する。また、教師ありのモデルでテキストからQueryを生成し、それとテキストのペアを学習データとする研究もあった。\nこの研究の特徴として、モデルを学習させなく、LLMからFew-shotでデータを生成したこと（ちょっと新奇性が足りていない気がする）。"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#our-method-inpars",
    "href": "posts/20230428_inpars/index.html#our-method-inpars",
    "title": "InPars 論文解読",
    "section": "3. Our Method: InPars",
    "text": "3. Our Method: InPars\n以下はInParsのステップ：\n\n複数のドキュメントとクエリーのペアが存在すると仮定する。それを例として、Few-shotでLLMでドキュメントに関連するクエリーを生成してもらう。(\n詳細は4.2で紹介する）\n生成したドキュメントとクエリーのペアについて、LLMが出力する際に出している結果のLog probabilityが上位のものを選択する。ちなみに、このステップが大きく精度を改善した。\n生成したqとdのペアを学習データとしてRerankのモデルをFine-tuningする。（詳細は4.3で紹介する）\n\n\n\n\nInPars"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#experimental-setup",
    "href": "posts/20230428_inpars/index.html#experimental-setup",
    "title": "InPars 論文解読",
    "section": "4. Experimental Setup",
    "text": "4. Experimental Setup\n\n4.1 Datasets\n今回使用したデータセットは以下：\n\nMS MARCO：Microsoftが出したBingの実際のユーザログをベースとした大型データセット。880万のドキュメントと50万のドキュメントとクエリーのペアがある。各クエリーが平均的に1ドキュメントに対応する。\nTREC-DL：MS MARCOと同じドキュメントを持っているが、クエリーは54件のみである。また、各クエリーについてアノテーションしたドキュメントが多い。\nRobust04：新聞領域のデータセット。52万のドキュメントがあり、249クエリーがある。各クエリーに対して、平均的に1250件のドキュメントをアノテーションした。\nNatural Questions：260万件のWikipediaテキストをベースとしたQuestion Answer データセット。QuestionはGoogleの検索エンジンのログから作ったもの。\nTREC-COVID：コロナの情報に関するデータセット\n\n\n\n4.2 Training Data Generation\n各学習データは（Query, Positive document, Negative document）のTripleによって構成させる。その生成のステップは以下：\n\n10万のドキュメントをサンプリングし、GPT3のCurieでQueryを生成させる。\n最終的にLog probabilityが上位の1万件のペアのみ学習データとして使う。\nBM25で検索した1000件の中でランダムに1件を抽出し、それをNegative Documentとする。（このやり方で多くのノイズを入れてしまうのでは？）\n\n以下は２点の補足：\n\n生成する際に温度とTopーPのパラメータ設定は結果に有意の影響しない。\n長さが300文字のドキュメントは捨てられる。\n\nQuery生成する際にPromptの書き方は2つを利用した（Figure２)：\n\n一般方法（Vanilla)：MS MARCOからランダムに3つのデータを抽出し、それを例として、FewーshotでQueryを生成させる。\nGBQ（Guided by Bad Questions）：一般方法と同じように、MS MARCOからランダムに3つのデータを抽出する。しかし、MS MARCOのQueryが漠然すぎるため、それをBad questionとして提示する。Documentを読んで、より関連するGood Questionを手動で作った。（Document, Good Question, Bad Question)で例を提示し、生成したGood QuestionをQueryとする。\n\n\n\n\nPrompt example\n\n\n\n\n4.3 Retrieval Methods\n２段階の検索を採用している。まずBM25で上位1000のドキュメントを取り出す。その次、MonoT5を使ってRerankingをする。\nMonoT5はTransformerのEncoderとDecoder両方とも使っているモデルで、Cross-Encoderモデルである。今回の実験では、サイズは220Mと3Bのモデルでテストした。\n各データセットにおいて作成された擬似データでMonoT5をFine-tuningした。"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#results",
    "href": "posts/20230428_inpars/index.html#results",
    "title": "InPars 論文解読",
    "section": "5 Results",
    "text": "5 Results\n\n7,8行目を見ると、BM25やContriever等の以前のUnsupervised結果より優れていることがわかる。また、16行目はMS MARCOでFine-tuningした後さらに擬似データでFine-tuningした結果。幾つかのデータセットで単純にMS MARCOでFine-tuningするより良い結果が出ている。"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#ablation-study-and-analysis",
    "href": "posts/20230428_inpars/index.html#ablation-study-and-analysis",
    "title": "InPars 論文解読",
    "section": "6 Ablation Study and Analysis",
    "text": "6 Ablation Study and Analysis\n6.1 Prompt Selection and Source Corpus\n比較対象が混乱のため、何が言いたいかがわからなかった。\n6.2 Model Size Impact on IR Metrics\n当たり前だけど、Questionを生成するモデルのサイズが大きいほど結果がよくなる。\n6.3 Filtering by the Most Likely Questions\nTop1万件のデータを利用することにより精度が向上した。\n6.4 Was GPT-3 Trained on Supervised IR Data?\n生成したQuestionがどのぐらい実際のMS MARCOデータにあるかで、GPT3の学習データにMS MARCOのデータを含まれているかを検証した。結果としては多くても10％前後のため影響がない。（検証のやり方が正しいかが疑問）"
  },
  {
    "objectID": "posts/20230428_inpars/index.html#conclusion-and-future-work",
    "href": "posts/20230428_inpars/index.html#conclusion-and-future-work",
    "title": "InPars 論文解読",
    "section": "7 Conclusion and Future Work",
    "text": "7 Conclusion and Future Work\n本文は、LLMを利用してQueryを生成して、生成したQueryとドキュメントのペアを学習データでモデルを学習する手法を提案した。\n今後の改善点としては、\n\n擬似データでDense RetrieverをFine-tuningする（今回はRerankerのみ）\nデータを生成する際に作った”BAD question”をNegative exampleとして使用する\n擬似データの数を増やす\n（Query, Document)のペアを探すもっと良い手法を開発する"
  },
  {
    "objectID": "posts/20230502_inpars_light/index.html",
    "href": "posts/20230502_inpars_light/index.html",
    "title": "InPars light 論文解読",
    "section": "",
    "text": "InPars-lightは、無料で利用可能な言語モデルBLOOMをランキングモデルを使用し、1000個ではなく100個の候補レコードを再ランクしした。 先行研究の主要な知見を再現するだけでなく、Consistency checkingとAll-domain pre-trainingを組み合わせることで、非常に効率的で小型なモデルMiniLM-L6-30Mを訓練し、すべてのデータセットでBM25を上回る性能を達成した。最後に、大きなDeBERTA-v3-435Mモデルを使用して、7倍大きなMonoT5-3Bの性能をほぼマッチさせることができた。\n論文URL：&lt;https://arxiv.org/abs/2301.02998&gt;"
  },
  {
    "objectID": "posts/20230502_inpars_light/index.html#related-work",
    "href": "posts/20230502_inpars_light/index.html#related-work",
    "title": "InPars light 論文解読",
    "section": "2 Related Work",
    "text": "2 Related Work\nUPR: 3BのLLMをRerankとして使った。第一段階で取り出したドキュメントに対して”please write a question for this document”でQueyrを生成するLog probabiltyを計算し、それでRerankをしている。(LLMを学習させる際にLossの計算と同じやり方)\nその他、InPars-v1、InPars-v2、Promptagator、HyDEが紹介された。以前の論文紹介で詳細を書いたため、今回は割愛する。"
  },
  {
    "objectID": "posts/20230502_inpars_light/index.html#methods",
    "href": "posts/20230502_inpars_light/index.html#methods",
    "title": "InPars light 論文解読",
    "section": "3 Methods",
    "text": "3 Methods\nこの論文も2段階の検索を使った。まずBM25で大量なDocumentから関連するDocumentをフィルタリングする。そのつぎにニューラルモデルでRerankする。\nRerankはCross-encoderを利用した。具体的に以下の3種類なものがある。\n\nMiniLM-L6(30M)\nERNIE-v2(335M)\nDeBERTA-v3(435M)\n\nERNIEとDeBERTAを利用した理由としては、今2つのモデルはMS MARCOで強い結果を出したことがある。\nInparsと同じように各データセットに対して100kのQueryを生成した。生成したQueryとDocumentのペアでRerankerを学習させ、それを使ってConsistency checkingをした。Consistency checkingをする時に、生成したQueryで検索をかけて、生成元のDocumentがTop-Kにないとそれを捨てる。Kについては、1でも良いが、3のほうが精度が高かった。\nまた、面白いのは、Consistency checkingでフィルタリングしたデータとLog Probabilityでフィルタリングしたデータは20〜30%のみ共通している。\nRerankerを学習させる際に、まず生成した全データで学習させ、その上で、フィルタリングしたデータでFine-Tuningを行った。\nこの研究でMiniMLに対して、まずすべてのデータセットで生成したすべてのデータで学習し、さらにすべてのデータセットのフィルタリングしたデータでFine-Tuningしたが、過学習した。\n実装する際に、FlexNeuARTのフレームワークを使った。モデルを学習させる際にInfoNCE Lossを使った。各Queryに対してNegative sampleを、BM25で検索できた上位1000件の中から3つサンプリングした。\n各モデルについて、3つのSeedで3回学習し、結果の平均値をとった。結果の有意性のチェックはpaired two-sided t-testを使った。大きいデータセットだと0.01の閾値を使った。小さいデータセットだと、0.05の閾値を使った。\nPromptの作り方はInParsが使った一般的なやり方と同じ。Queryを生成する際に、最大Token数を32に設定した。"
  },
  {
    "objectID": "posts/20230502_inpars_light/index.html#データセット",
    "href": "posts/20230502_inpars_light/index.html#データセット",
    "title": "InPars light 論文解読",
    "section": "4 データセット",
    "text": "4 データセット\nInPars[4]の主要結果を再現するために、同じクエリとデータセットを使用した。MS MARCO以外のデータセットは「ir_datasets」というツールを利用して処理した。\nInParsの論文で提供したGithubにGPT-3 Curieモデルで生成されたクエリと、それを生成するための文書が提供されている。これにより、GPT-3 CurieとオープンソースモデルGPT-J、BLOOMで生成されたQueryの品質を比較できる。クエリの生成コストがまだ高いため、他のオープンソースモデルの検討は将来の課題である。"
  },
  {
    "objectID": "posts/20230502_inpars_light/index.html#results",
    "href": "posts/20230502_inpars_light/index.html#results",
    "title": "InPars light 論文解読",
    "section": "5 Results",
    "text": "5 Results\n\n5.1 Main Results\n\n\n\nmain results\n\n\nBM25 この論文は使うフィールドについて少し調整したが、InParsの結果と大きく変わらない。\n教師なし学習 今回使ったDeBERTA-v3-435Mは以前のMonoT5-3Bの性能と同じ。また今回提案したMiniLM-L6-30MはInParsのものT5-220M相当な性能を出している。\nConsistency checkingとall-domain pre-training 両方とも良い影響を与えることがわかる。Deberta-v3-435Mに対してAll-domain pre-trainingが逆効果があるが、理由が不明。\n教師あり学習 今回提案した2つのモデルの性能がいまいち。\n\n\n\nmodel performance\n\n\nQueryを生成するLLMモデルの比較について、オープンソースのGPT-JとBLOOMはOpen AI Curieよりよい性能を出している。\nまた、Rerankerについては、Deberta-v3-435MはERNIE-v2-335Mよりよいことがわかる。\n\n\n5.2 Cost and Efficiency\nRTX3039を使う場合は：\n\nMiniLM-L6-30Mの推論のThroughputは1秒500ドキュメント(LLM各ドキュメントの長さは477キャラクター以下)、そのため、100ドキュメントをRerankする場合は1秒かからない。\nMiniLM-L6-30Mを全データセットで前学習しても2時間しかかからない。一方、Deberta-v3-435Mは28時間かかる。\nall-domain pre-trainingをする際に、一番時間がかかる操作はMS MARCOのような大きいなデータセットのバリデーションとConsistency checking。Deberta-v3-435MでMS MARCOでのバリデーション時間は6時間、Consistency checkingだと48時間かかった。\nQuery生成の時間：100kのQueryを生成するためには15時間がかかる。"
  },
  {
    "objectID": "posts/20230505_tiktoken/index.html",
    "href": "posts/20230505_tiktoken/index.html",
    "title": "OpenAIのGPTのAPIのToken数に関する調査",
    "section": "",
    "text": "OpenAIのGPTモデルでドキュメントを処理する際に、日本語の1文字は大よそ1Tokenに等しいです。千文字のドキュメントを処理するためには、概算で、スピード重視のgpt-3.5-turboを使う場合は0.59円かかります。性能重視のgpt-4-32kを利用する場合は、9.7円かかります。"
  },
  {
    "objectID": "posts/20230505_tiktoken/index.html#結論",
    "href": "posts/20230505_tiktoken/index.html#結論",
    "title": "OpenAIのGPTのAPIのToken数に関する調査",
    "section": "",
    "text": "OpenAIのGPTモデルでドキュメントを処理する際に、日本語の1文字は大よそ1Tokenに等しいです。千文字のドキュメントを処理するためには、概算で、スピード重視のgpt-3.5-turboを使う場合は0.59円かかります。性能重視のgpt-4-32kを利用する場合は、9.7円かかります。"
  },
  {
    "objectID": "posts/20230505_tiktoken/index.html#目的",
    "href": "posts/20230505_tiktoken/index.html#目的",
    "title": "OpenAIのGPTのAPIのToken数に関する調査",
    "section": "目的",
    "text": "目的\nGPT3を用いた提案をする際によく聞かれることとしては、コストいくらかのことです。GPT3のAPIの課金は下記のように文字数ではなく、tokenを単位としているため、説明するのは簡単ではないです。\n本文は値段の説明をしやすいように、実際のデータで実験してみます。ついてにTicTokenの挙動についても掘り下げてみます。 実験のステップは下記の通りです。\n\nlivedoor ニュースコーパスをダウンロードする\n\nニュースコーパスをtiktokenでトークナイズする\nToken数/文字数で、千文字あたりの値段を計算する\n\nテキスト補完モデル\n\n\n\n\nモデル名\n\n\n値段(Prompt)\n\n\n値段(補完)\n\n\n最大トークン数\n\n\nモデルサイズ(推測)\n\n\n\n\ngpt-3.5-turbo\n\n\n$0.002\n\n\n$0.002\n\n\n4,096\n\n\n6.7B\n\n\n\n\ngpt-4\n\n\n$0.03\n\n\n$0.06\n\n\n8,192\n\n\n不明\n\n\n\n\ngpt-4-32k\n\n\n$0.06\n\n\n$0.12\n\n\n32,768\n\n\n不明"
  },
  {
    "objectID": "posts/20230505_tiktoken/index.html#前準備",
    "href": "posts/20230505_tiktoken/index.html#前準備",
    "title": "OpenAIのGPTのAPIのToken数に関する調査",
    "section": "前準備",
    "text": "前準備\nGPT3のTokenizerはtiktokenというライブラリを利用しているので、検証するためにはtiktokenをインストールする必要があります。\n今回利用するデータは、livedoor ニュースコーパスです。livedoor ニュースコーパスは、9つのカテゴリに分類された、記事のデータセットです。\n\n!pip install tiktoken\n!curl -O https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n!tar -zxvf ldcc-20140209.tar.gz\n\n次に文字数とトークン数の関係を計算します。\n\nimport glob\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport tiktoken\n\n# load data\npath_list = glob.glob('./text/*/*.txt')\ntxt_list =[]\nfor path in path_list:\n    category = path.split('/')[2]\n    with open(path) as f:\n        # skip first 2 lines\n        for i in range(2):\n            f.readline()\n        txt_list.append(( category, f.read()))\ndf =  pd.DataFrame( txt_list, columns=['category', 'text'])\ndf[\"word_count\"] = df[\"text\"].apply(lambda x: len(x))\n\n# cl100k_base is for gpt-4, gpt-3.5-turbo, text-embedding-ada-002\n# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\nencoder = tiktoken.get_encoding(\"cl100k_base\")\ndf[\"token_ids\"] = df[\"text\"].apply(lambda x: encoder.encode(x))\ndf[\"token_count\"] = df[\"token_ids\"].apply(lambda x: len(x))\ndf[\"tokens\"]  = df[\"token_ids\"].apply(lambda x: encoder.decode_tokens_bytes(x))\ndf[\"word_token_ratio\"] = df[\"token_count\"] / df[\"word_count\"]"
  },
  {
    "objectID": "posts/20230505_tiktoken/index.html#計算",
    "href": "posts/20230505_tiktoken/index.html#計算",
    "title": "OpenAIのGPTのAPIのToken数に関する調査",
    "section": "計算",
    "text": "計算\nまず、処理するデータの様子を実際に見てみましょう。\n\nprint(\"ドキュメントのサンプル：\")\nfor i in txt_list[0][1].split('\\n')[:10]:\n    print(i)\nprint(\"...\")\n\nドキュメントのサンプル：\n【DVDエンター！】誘拐犯に育てられた女が目にした真実は、孤独か幸福か\n　2005年11月から翌2006年7月まで読売新聞にて連載された、直木賞作家・角田光代による初の長編サスペンス『八日目の蝉』。2010年に檀れいと北乃きいの出演によりテレビドラマ化された同作が、2011年4月に永作博美と井上真央の出演によって映画化。そして、劇場公開から半年が過ぎた10月28日、DVD＆ブルーレイとなって発売されました。\n\n八日目の蝉\n　妻子ある男と愛し合い、その子を身ごもりながら、あきらめざるをえなかった女。彼女は同時に、男の妻が子供を産んだことを知る。その赤ん坊を見に行った女は、突発的にその子を連れ去り、逃避行を続けた挙句、小豆島に落ち着き、母と娘として暮らしはじめる。\n\n\n不倫相手の子供を誘拐し、4年間育てた女\n　永作博美が演じる野々宮希和子は、不倫相手の子を宿しながらも、彼の「いずれ妻と別れるから、それまで待ってくれ」という常套句を信じて、中絶。後遺症により、二度と子供を産めない身体となってしまいます。その後、不倫相手から彼の妻が出産したことを知らされ、別れを決意。最後に諦めをつけるため、彼らの生後6ヶ月の赤ん坊・恵理菜の顔を見た希和子でしたが、自分に笑顔で向けた恵理菜を見て、思わず誘拐。名前を変えて恵理菜を薫と名付けると、人目を避けて各地を転々とし、二人で幸せな時間を過ごしますが、辿り着いた最後の場所・小豆島で4年の逃避行に終止符を打ちます。\n\n...\n\n\n合計7,376件のドキュメントがあり、平均文字数は1,200文字程度です。\n\ndf.word_count.describe().astype(int)\n\ncount     7376\nmean      1259\nstd        763\nmin         37\n25%        730\n50%       1069\n75%       1602\nmax      12163\nName: word_count, dtype: int64\n\n\n 文字数とトークン数の割合を見ると、以外に1文字が1トークンになっていることがわかります。  また、この傾向が記事の種類によりますが、大きな違いはありません。\n\ndf.word_token_ratio.mean()\n\n1.008244127016698\n\n\n\ndf.groupby(\"category\").word_token_ratio.mean().sort_values().plot.barh(figsize=(10, 5))\n\n&lt;Axes: ylabel='category'&gt;\n\n\n\n\n\nこれで文字数とトークン数の関係がわかることによって、ドキュメントを処理する課金を概算計算することができます。概算ロジックは以下と仮定します。\n\nインプットの長さは2000字とする。内訳は以下の通り。\n\n処理するドキュメントの長さは1000字とする。\nタスクの説明や、処理の例は1000字とする。\n\nアウトプットは200字とする。\n為替レートは1ドル=135円とする。\n\nこれで計算すると1ドキュメントを処理するためには:\n\nスピードを求めるgpt-3.5-turboの場合は、0.002 * 2200 / 1000 * 135 = 0.59円 かかります。\n性能を重視するgpt-4-32kを利用する場合は(0.03 * 2000 + 0.06 * 200) / 1000 * 135 = 9.7円 かかります。"
  },
  {
    "objectID": "posts/20230505_tiktoken/index.html#tictokenの挙動",
    "href": "posts/20230505_tiktoken/index.html#tictokenの挙動",
    "title": "OpenAIのGPTのAPIのToken数に関する調査",
    "section": "TicTokenの挙動",
    "text": "TicTokenの挙動\n\nBPEモデルが違う\n日本語は英語よりトークン数が多いと話している投稿は過去Twitterで見たことがあります。今回実際に計算してみると、日本語の1文字は大よそ1Tokenに等しいことがわかりました。それはDecodingするモデルが違うためです。\nここからはちょっと深い話をします。TikTokenはBPE(Byte Pair Encoding)というデータ圧縮法に基づいて開発しました。コンピューターは文字を扱うことができないので、文字を数値に変換する必要があります。BPEは文字列をシンボルに置き換えることで、文字列を数値に変換します。BPEは頻繁に現れる文字のペアや、複数の文字を組み合わせたシンボルを生成します。それにより、入力するシーケンスの長さを短くすることができます。\n例えば、“ab ab b”の文字列について、“ab”を0に、“b”を1に置き換えると、“0 0 1”という文字列になります。このように、BPEは文字列をシンボルに置き換えることで、もともと長さが7の文字列を長さが5のシーケンスに変換できました。\nまた、どの組み合わせをシンボルにするかはデータから学習することによって決められています。gpt-3.5-turboとgpt-4-32kのモデルは以前のGPT3のモデルが違うので、Tokenizeした結果も違います。\n実際の例を見ましょう。\n\ngpt4_encoder = tiktoken.encoding_for_model(\"gpt-4-32k\")\ngpt3_encoder = tiktoken.encoding_for_model(\"text-davinci-003\")\n\nprint(\"GPT3のトークン数：\")\nprint(f\"こんにちは: {len(gpt3_encoder.encode('こんにちは'))}\")\nprint()\nprint(\"GPT4のトークン数：\")\nprint(f\"こんにちは: {len(gpt4_encoder.encode('こんにちは'))}\")\n\nGPT3のトークン数：\nこんにちは: 6\n\nGPT4のトークン数：\nこんにちは: 1\n\n\nGPT3のToken数がGPT4より多いことがわかります。例えば、「こんにちは」はGPT3で6Tokenになりますが、GPT4では1Tokenになります。\n\n\nGPTのBPEモデルは日本語をバイト化してからトークン化している\n「こんにちは」については5文字はしかないですが、なぜ6Tokenになっているかに疑問を思うかもしれません。それはGPT3が多言語に対応するために、直接テキストで切っていなくて、日本語をまずバイトに変換して切っているからです。バイト化することにより違う言語でも共通のTokenで表現することができます。\n\ntokeinzer_result_byte = gpt3_encoder.decode_tokens_bytes(gpt3_encoder.encode('こんにちは'))\nprint(\"Tokenize結果:\", tokeinzer_result_byte)\ntokeinzer_result = [i.decode() if len(i) == 3 else i for i in tokeinzer_result_byte ]\nprint(\"Decoding結果:\", tokeinzer_result)\n\nTokenize結果: [b'\\xe3\\x81\\x93', b'\\xe3\\x82\\x93', b'\\xe3\\x81\\xab', b'\\xe3\\x81', b'\\xa1', b'\\xe3\\x81\\xaf']\nDecoding結果: ['こ', 'ん', 'に', b'\\xe3\\x81', b'\\xa1', 'は']\n\n\n上記の結果からわかることとしては、日本語1キャラクターは3バイトで表示しています。「こんにちは」の中の「ち」のみ2Tokenに分解されました。\n\n\n実際の比較\nつぎに、実際にデータでGPT3とGPT4のTokenizeの結果を比較してみましょう。\n\ndf[\"token_ids_gpt3\"] = df[\"text\"].apply(lambda x: gpt3_encoder.encode(x))\ndf[\"token_count_gpt3\"] = df[\"token_ids_gpt3\"].apply(lambda x: len(x))\ndf[\"word_token_ratio_gpt3\"] = df[\"token_count_gpt3\"] / df[\"word_count\"]\ndf[\"word_token_ratio_gpt3\"].mean()\n\n1.317645208825121"
  },
  {
    "objectID": "posts/20230505_tiktoken/index.html#まとめ",
    "href": "posts/20230505_tiktoken/index.html#まとめ",
    "title": "OpenAIのGPTのAPIのToken数に関する調査",
    "section": "まとめ",
    "text": "まとめ\n過去にGPT3を使う場合は日本語のToken数は英語の2倍になる噂があります。GPTモデルで日本語のドキュメントを処理する際、1文字はおおよそ1トークンに等しいことがわかりました。千文字のドキュメントを処理するための概算コストは、スピード重視のgpt-3.5-turboを使う場合は0.59円、性能重視のgpt-4-32kを利用する場合は9.7円です。\nまた、GPT3とGPT4が使うTokenizerが違い、GPT3のトークン数はおおよそGPT4の1.3倍になります。"
  },
  {
    "objectID": "posts/20230501_inpars_v2/index.html",
    "href": "posts/20230501_inpars_v2/index.html",
    "title": "InPars V2 論文解読",
    "section": "",
    "text": "InPars V2論文では、Query生成に使用するLLMがGPT3からオープンソースのGPT-J(6B)に変更され、生成したQueryのフィルタリング方法がLog Probabilityからmonot5(3B)をRerankerとして利用する方法に変更された点を挙げている。実験結果としては、V2の精度がV1と比べてわずかに向上したことが報告されている。\n論文URL：https://arxiv.org/abs/2301.01820"
  },
  {
    "objectID": "posts/20230501_inpars_v2/index.html#introduction",
    "href": "posts/20230501_inpars_v2/index.html#introduction",
    "title": "InPars V2 論文解読",
    "section": "1 Introduction",
    "text": "1 Introduction\nInPars v1とv2の違いは、主に以下の2点：\n\n\n\n\n\n\n\n\nDifference\nInPars v1\nInPars v2\n\n\n\n\nQueryを生成するLLM\nGPT3\nGPT-J(6B) (オープンソース)\n\n\n生成したQueryのフィルタリング方法\n生成時のLog Probabilityでフィルタリング\nmonot5(3B)をRerankerとしてフィルタリング"
  },
  {
    "objectID": "posts/20230501_inpars_v2/index.html#methodology",
    "href": "posts/20230501_inpars_v2/index.html#methodology",
    "title": "InPars V2 論文解読",
    "section": "2 Methodology",
    "text": "2 Methodology\nBEIRの各データセットに対して100kのドキュメントをサンプリングする。MS MARCOからの3つの例を利用してGBQの形式でPromptを作成し、各ドキュメントに対して一個のQueryを生成する。GPT-J(6B)を利用してQueryを生成した。A100一枚で100kのQueryを生成するためには30時間かかる。\nフィルタリングについては以前は生成時のLog Probabilityが上位の10kのペアを選んだが、今回はMS-MARCOでFine-tuningしたものT5-3BをRerankerとして使った。100kのQueryとDocumentのペアについて相関度を出して、上位の10kペアを利用した。\nNegative sampleはまた各QueryについてBM25で上位1000ドキュメント中で1個ランダム選んだ。"
  },
  {
    "objectID": "posts/20230501_inpars_v2/index.html#result",
    "href": "posts/20230501_inpars_v2/index.html#result",
    "title": "InPars V2 論文解読",
    "section": "3 Result",
    "text": "3 Result\n\n\n\nresult\n\n\n実験結果を見ると、v2はv1と比べて精度が少し良くなった(0.006)。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "自然言語処理技術ブログ",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n日本語Embeddingモデルのベンチマーク比較: OpenAIが圧倒的な精度でリード\n\n\n\n\n\n\n\nNLP\n\n\nLLMs\n\n\nLangChain\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\n音声基礎モデルの紹介 Part2\n\n\n\n\n\n\n\nSpeech Recognition\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2023\n\n\n\n\n\n\n  \n\n\n\n\n音声基礎モデルの紹介 Part1\n\n\n\n\n\n\n\nSpeech Recognition\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nGPT文書校正アシスタント\n\n\n\n\n\n\n\nNLP\n\n\nLLMs\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLangChain Hypothetical Document Embeddings (HyDE) 全面解説\n\n\n\n\n\n\n\nNLP\n\n\nLLMs\n\n\nLangChain\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAzure OpenAI Serviceの利用について\n\n\n\n\n\n\n\nNLP\n\n\nLLMs\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLangChain Agentの全面解説\n\n\n\n\n\n\n\nNLP\n\n\nLLMs\n\n\nLangChain\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLangChainのベーシックを全面解説する\n\n\n\n\n\n\n\nNLP\n\n\nLLMs\n\n\nLangChain\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\n\n\n\n\n  \n\n\n\n\nOpenAIのGPTのAPIのToken数に関する調査\n\n\n\n\n\n\n\nNLP\n\n\nLLMs\n\n\nLangChain\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\n\n\n\n\n  \n\n\n\n\nInPars light 論文解読\n\n\n\n\n\n\n\nNLP\n\n\nInformation_retrieval\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nMay 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nInPars V2 論文解読\n\n\n\n\n\n\n\nNLP\n\n\nInformation_retrieval\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHyDE 論文解読\n\n\n\n\n\n\n\nNLP\n\n\nInformation_retrieval\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPromptagator 論文解読\n\n\n\n\n\n\n\nNLP\n\n\nInformation_retrieval\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\n\n\n\n\n  \n\n\n\n\nInPars 論文解読\n\n\n\n\n\n\n\nNLP\n\n\nInformation_retrieval\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "このブログでは、自分が学んだ知識や経験を共有し、皆さんと一緒に成長していくことを目指しています。どうぞよろしくお願いいたします。"
  },
  {
    "objectID": "about.html#自己紹介",
    "href": "about.html#自己紹介",
    "title": "About",
    "section": "自己紹介",
    "text": "自己紹介\n私は2017年から社会人として働き始め、これまでに数々のデータサイエンスプロジェクトを経験してきました。自然言語処理に強い関心を持ち、KaggleやSignateなどでコンペに参加しています。詳細はLinkedInをご覧ください。"
  },
  {
    "objectID": "about.html#このブログの目的",
    "href": "about.html#このブログの目的",
    "title": "About",
    "section": "このブログの目的",
    "text": "このブログの目的\nブログを書く目的は主に以下の3つです。\n\n学んだ知識の理解を深めるため： 学んだことを他人に教えることは、自分自身の理解を深めることに繋がります。自分が理解できていない内容は他人に教えられないため、教えることを通じて自然と理解が深まります。\nプロフィールを充実させるため： 今後自己紹介が必要になった際に、このブログのリンクを共有することで相手に自分の興味や技術スタックを知ってもらえるようになります。\n他人の役に立つ情報を提供するため： これまでインターネット上の無料コンテンツの恩恵を受けてきました。私もコンテンツの消費者だけでなく、提供者としても活躍したいと考えています。"
  }
]