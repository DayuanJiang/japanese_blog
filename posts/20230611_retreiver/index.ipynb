{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"RAG質問応答システムに使うRetrieverの精度比較\"\n",
    "date: 2023-07-16\n",
    "description-meta: \"今回はRAGのRetrieverの性能を比較しました。 その結果としては、  - Dense Retrieverの中でデフォルトのGPTのEmbeddingモデル+Cosine類似度の組み合わせるが一番良かったです。  - Sparse Retrieverの中でBM25は計算スピードが早くてそこそこ良いパフォーマンスを出せています。  - Hybridのやり方で、Dense RetrieverとSparse Retrieverを組み合わせると一番良い結果を出せています。\"\n",
    "categories: [NLP, LLM]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "回はRAGのRetrieverの性能を比較しました。 その結果としては、\n",
    "\n",
    "- Dense Retrieverの中でデフォルトのGPTのEmbeddingモデル+Cosine類似度の組み合わせるが一番良かったです。\n",
    "\n",
    "- Sparse Retrieverの中でBM25は計算スピードが早くてそこそこ良いパフォーマンスを出せています。\n",
    "\n",
    "- Hybridのやり方で、Dense RetrieverとSparse Retrieverを組み合わせると一番良い結果を出せています。\n",
    "\n",
    "## RAG(Retrieval Augmented Generation)とは？\n",
    "\n",
    "RAG（Retrieval-Augmented Generation）は、自然言語処理（NLP）タスクのための最新の機械学習モデルの一つです。RAGは、質問応答、文章生成、要約作成などのタスクに適用されます。このモデルは、あらかじめ学習された情報を取得（retrieval）し、その情報を利用して文を生成（generation）することが特徴です。\n",
    "\n",
    "RAGは、主に以下の2つのコンポーネントから構成されています。\n",
    "\n",
    "- 検索器（Retriever）：質問や入力文に関連する情報をデータセットから見つけ出す役割を担います。検索された情報は、文書や段落といった形式で提供されます。\n",
    "\n",
    "- 生成器（Generator）：検索器から提供された情報を基に、適切な応答や文章を生成します。生成器は、Transformerベースのモデル（例：GPT-3、BART）で構築されることが一般的です。最近はOpenAIのGPTのAPIのを利用することが多いです。\n",
    "\n",
    "\n",
    "![RAG](image.png)\n",
    "\n",
    "生成器の部分はLLMを使うため、RAGの性能は検索器の性能に依存します。今回は、検索器の性能を比較しました。そのまとめは以下の図になります。\n",
    "\n",
    "![Alt text](image-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用するデータセット\n",
    "\n",
    "今回使用するデータは東京都立大学のeラーニングシステムのQ&Aデータです。このデータは、東京都立大学で導入されたeラーニングシステムのユーザーから2015年4月から2018年7月までに報告された問題点としてのQ&Aデータを収集したものです。427の質問と79の回答が含まれています。質問にどの回答に紐づくかのラベルがあります。\n",
    "\n",
    "データの様子は下記の通りです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_df.shape: (427, 2)\n",
      "a_df.shape: (79, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>AID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>履修している授業で先生が資料をアップロードしているはずだが、コース上に資料が見当たらない。</td>\n",
       "      <td>A001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>資料をマイページに置いたが、学生からは見えなかった。</td>\n",
       "      <td>A001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>前期の科目の「資料」を学生から見られないようにするにはどうしたら良いか？</td>\n",
       "      <td>A001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Text   AID\n",
       "0  履修している授業で先生が資料をアップロードしているはずだが、コース上に資料が見当たらない。  A001\n",
       "1                     資料をマイページに置いたが、学生からは見えなかった。  A001\n",
       "2           前期の科目の「資料」を学生から見られないようにするにはどうしたら良いか？  A001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A001</td>\n",
       "      <td>資料が見つからない場合は、以下の点を確認してください。&lt;br&gt;&lt;br&gt;&lt;br&gt;【受講生編】&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A002</td>\n",
       "      <td>資料のアップロードやお知らせ作成時の電子メールでの通知の有無は、各授業の担当教員が設定できま...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A003</td>\n",
       "      <td>kibacoにはファイルへパスワードを設定する機能はありません。資料は受講生全員に開示されま...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AID                                               Text\n",
       "0  A001  資料が見つからない場合は、以下の点を確認してください。<br><br><br>【受講生編】<...\n",
       "1  A002  資料のアップロードやお知らせ作成時の電子メールでの通知の有無は、各授業の担当教員が設定できま...\n",
       "2  A003  kibacoにはファイルへパスワードを設定する機能はありません。資料は受講生全員に開示されま..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# https://zenodo.org/record/2783642\n",
    "q_df = pd.read_csv(\"https://zenodo.org/record/2783642/files/Questions.csv\")\n",
    "a_df = pd.read_csv(\"https://zenodo.org/record/2783642/files/Answers.csv\")\n",
    "print(\"q_df.shape:\", q_df.shape)\n",
    "print(\"a_df.shape:\", a_df.shape)\n",
    "q_df.columns = [c.strip() for c in q_df.columns]\n",
    "a_df.columns = [c.strip() for c in a_df.columns]\n",
    "df = q_df.merge(a_df, on=\"AID\")\n",
    "df.columns = [\"query\",\"AID\",\"document\"]\n",
    "\n",
    "metadata = a_df[[\"AID\"]].to_dict(orient=\"records\")\n",
    "documents = a_df[\"Text\"].tolist()\n",
    "query_list = list(zip(q_df[\"Text\"], q_df[\"AID\"]))\n",
    "display(q_df.head(3))\n",
    "display(a_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価対象と評価方法、評価指標\n",
    "\n",
    "以前の[ポスト](http://www.jiang.jp/posts/20230601_embedding_benchmark/)は主にDense RetrieverのEmbeddingモデルを比較しました。今回は以下の角度で比較します。\n",
    "\n",
    "- 類似度の計算方法\n",
    "    - Cosine類似度\n",
    "    - MMR（Maximal Marginal Relevance）類似度\n",
    "    - SVM\n",
    "- Sparse Retrieverのモデル\n",
    "    - BM25\n",
    "    - TF-IDF\n",
    "\n",
    "評価方法は以下の3つのステップです。\n",
    "\n",
    "1. 79のドキュメントをEmbeddingに変換し、FAISSのVectorstoreとして保存する。\n",
    "2. 427の質問をEmbeddingに変換し、FAISSのVectorstoreを使用して、79のドキュメントを近い順に並べる。\n",
    "3. 並んだ順番でEmbeddingの性能を評価する\n",
    "\n",
    "評価指標は以下の3つです。\n",
    "\n",
    "1. Mean Reciprocal Rank（MRR）: 正解ドキュメントの順位の平均の逆数で、ランク全体を評価する指標。\n",
    "2. Recall@1: 正解ドキュメントが1番目に並んでいるかどうかを評価する指標。\n",
    "3. Recall@5: 正解ドキュメントが上位5位以内に入っているかどうかを評価する指標。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 類似度の計算方法\n",
    "\n",
    "Embeddingを得た後、類似度の計算によく使う方法はCosine類似度です。それを今回のベースラインとします。\n",
    "\n",
    "MMR（Maximal Marginal Relevance）とは、は検索クエリとの関連性を維持しつつも、検索結果多様性を持たすように検索結果の順位を並べ替える手法。推薦システムで使うことが多いです。(https://yolo-kiyoshi.com/2020/05/08/post-1781/)\n",
    "\n",
    "SVMはもともと分類の機械学習モデルですが、それを検索に使うことをOpenAIの創立者karpathyがTwitterで提案しました。\n",
    "「感覚」としてはCosine Similarlyより良いだけでLangchainに取り込まされました。\n",
    "(https://github.com/karpathy/randomfun/blob/master/knn_vs_svm.ipynb)\n",
    "\n",
    "評価用コードがないので折り畳みしました。興味がある方は下の「Show the code」をクリックしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DOC_NUM = len(a_df)\n",
    "@dataclass\n",
    "class EvaluationResults:\n",
    "    result_df: pd.DataFrame\n",
    "    mrr: float\n",
    "    recall_at_1: float\n",
    "    recall_at_5: float\n",
    "\n",
    "def mrr(rank_array):\n",
    "    return (1 / rank_array).mean()\n",
    "\n",
    "def recall_at_k(rank_array, k):\n",
    "    return (rank_array <= k).mean()\n",
    "\n",
    "def evaluate(query_list, search_func):\n",
    "    result_list = []\n",
    "    for query, aid in tqdm(query_list):\n",
    "        rank_result = get_rank(query, search_func=search_func)\n",
    "        if aid not in rank_result:\n",
    "            rank = DOC_NUM + 1\n",
    "        else:\n",
    "            rank = rank_result.index(aid) + 1\n",
    "        \n",
    "        result_list.append((query, rank, rank_result))\n",
    "\n",
    "    result_df = pd.DataFrame(result_list, columns=[\"query\", \"rank\", \"rank_result\"])\n",
    "    return EvaluationResults(result_df, mrr(result_df[\"rank\"]), recall_at_k(result_df[\"rank\"], 1), recall_at_k(result_df[\"rank\"], 5))\n",
    "\n",
    "def get_rank(query, search_func):\n",
    "    results = search_func(query)\n",
    "    aid_list = []\n",
    "    for doc in results:\n",
    "        aid = metadata[documents.index(doc.page_content)][\"AID\"]\n",
    "        aid_list.append(aid)\n",
    "    return aid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.retrievers import KNNRetriever\n",
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.retrievers import ElasticSearchBM25Retriever\n",
    "\n",
    "embedding_openai = OpenAIEmbeddings()\n",
    "\n",
    "faiss_vectorstore = FAISS.from_texts(documents, embedding_openai)\n",
    "\n",
    "svm_retriever = SVMRetriever.from_texts(documents, embedding_openai)\n",
    "svm_retriever.k = DOC_NUM\n",
    "\n",
    "faiss_similarity_result = evaluate(query_list, lambda q: faiss_vectorstore.similarity_search(q, k=DOC_NUM))\n",
    "faiss_mmr_result = evaluate(query_list, lambda q: faiss_vectorstore.max_marginal_relevance_search(q, k=DOC_NUM))\n",
    "svm_result = evaluate(query_list, lambda q: svm_retriever.get_relevant_documents(q))\n",
    "\n",
    "result_df = pd.DataFrame(\n",
    "    [\n",
    "        [\"faiss_similarity\", faiss_similarity_result.mrr, faiss_similarity_result.recall_at_1, faiss_similarity_result.recall_at_5],\n",
    "        [\"faiss_mmr\", faiss_mmr_result.mrr, faiss_mmr_result.recall_at_1, faiss_mmr_result.recall_at_5],\n",
    "        [\"svm\", svm_result.mrr, svm_result.recall_at_1, svm_result.recall_at_5],\n",
    "        ], \n",
    "    columns = [\"model_id\",\"mrr\",\"recall_at_1\",\"recall_at_5\"]\n",
    "    ).sort_values(\"mrr\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果はこちらです。ご覧の通り、MMRとSVM両方ともベースラインより弱いです。特にSVMの性能がひどいです。またくCosine類似度の方法に比べにならないです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>mrr</th>\n",
       "      <th>recall_at_1</th>\n",
       "      <th>recall_at_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faiss_similarity</td>\n",
       "      <td>0.685327</td>\n",
       "      <td>0.550351</td>\n",
       "      <td>0.868852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faiss_mmr</td>\n",
       "      <td>0.622978</td>\n",
       "      <td>0.550351</td>\n",
       "      <td>0.679157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.520898</td>\n",
       "      <td>0.388759</td>\n",
       "      <td>0.683841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_id       mrr  recall_at_1  recall_at_5\n",
       "0  faiss_similarity  0.685327     0.550351     0.868852\n",
       "1         faiss_mmr  0.622978     0.550351     0.679157\n",
       "2               svm  0.520898     0.388759     0.683841"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Retrieverのモデル\n",
    "次にSparse RetriewerのBM25とTF-IDFを見ます。\n",
    "\n",
    "TF-IDFとBM25は、情報検索において、ドキュメントと検索クエリの関連性を評価するために使用される代表的な手法です。単語のカウントをベースとしているため、得たドキュメントのベクトルはsparseになります。そのため、この手法をSparse Retrieverと呼びます。\n",
    "\n",
    "このポストはその効果の紹介を目的しているので、TF-IDFとBM25の詳細な説明は折り畳みしました。興味がある方は下の青いバーをクリックください。\n",
    "\n",
    "::: {.callout-note collapse=\"true\"}\n",
    "## TF-IDFとBM25とは\n",
    "**TF-IDF**: TF-IDFは、単語の重要度を評価するために使用される統計的手法です。TF-IDFは、2つの主要な要素で構成されています。\n",
    "- TF（Term Frequency）：文書内での単語の出現頻度。単語が文書内で頻繁に出現するほど、その単語は文書内で重要であると考えられます。\n",
    "\n",
    "- IDF（Inverse Document Frequency）：文書全体のセット（コーパス）における単語の希少性を測定する指標。単語が多くの文書で出現するほど、その単語は一般的であると考えられ、IDFの値は低くなります。逆に、単語が少数の文書でのみ出現する場合、その単語は特定の文書に特有であり、IDFの値が高くなります。\n",
    "\n",
    "TF-IDFスコアは、これら2つの要素の積として計算され、このスコアが高い単語ほど、検索クエリと関連性が高いと考えられます。\n",
    "\n",
    "**BM25**: BM25（Best Matching 25）は、TF-IDFを拡張した検索アルゴリズムで、検索クエリとドキュメントの関連性をより正確に評価することができます。BM25は、以下の特徴を持っています。\n",
    "\n",
    "- 長さ正規化：長いドキュメントは、短いドキュメントよりも多くの単語を持つため、TF-IDFでは不利になる可能性があります。BM25では、ドキュメントの長さを正規化することで、この問題を解決しています。\n",
    "\n",
    "- 単語の出現頻度の飽和：単語がある一定の出現回数を超えると、その単語の重要度が飽和し、それ以上の出現回数が重要度に大きな影響を与えなくなります。これにより、ある単語が特定のドキュメントで極端に多く出現する場合でも、適切な関連性評価が可能になります。\n",
    "\n",
    "BM25スコアは、TF-IDFスコアを改善したものであり、検索クエリとドキュメントの関連性をより正確に評価することができます。多くの情報検索システムや検索エンジンでは、BM25が関連性スコアとして使用されています。\n",
    "::: \n",
    "\n",
    "\n",
    "実装について、LangchainにあるTF-IDFモジュールには前処理がないため、それを追加しました。また、TF-IDFのモジュールに模倣してBM25を実装しました。\n",
    "\n",
    "一点注意すべきところとしては、Sparse Retriewerは前処理が非常に重要です。今回の前処理は簡単にMecabでテキストを単語単位に分割し、ストップワードを除去しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, Iterable, List, Optional, Callable\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "\n",
    "\n",
    "class TFIDFRetriever(BaseRetriever, BaseModel):\n",
    "    vectorizer: Any\n",
    "    docs: List[Document]\n",
    "    tfidf_array: Any\n",
    "    k: int = 4\n",
    "    preprocess_func: Callable[[str], str] = None\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @classmethod\n",
    "    def from_texts(\n",
    "        cls,\n",
    "        texts: Iterable[str],\n",
    "        metadatas: Optional[Iterable[dict]] = None,\n",
    "        tfidf_params: Optional[Dict[str, Any]] = None,\n",
    "        preprocess_func: Optional[Callable[[str], str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> TFIDFRetriever:\n",
    "        try:\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Could not import scikit-learn, please install with `pip install \"\n",
    "                \"scikit-learn`.\"\n",
    "            )\n",
    "\n",
    "        tfidf_params = tfidf_params or {}\n",
    "        vectorizer = TfidfVectorizer(**tfidf_params)\n",
    "        if preprocess_func:\n",
    "            processed_texts = [preprocess_func(t) for t in texts]\n",
    "            tfidf_array = vectorizer.fit_transform(processed_texts)\n",
    "        else:\n",
    "            tfidf_array = vectorizer.fit_transform(texts)\n",
    "        metadatas = metadatas or ({} for _ in texts)\n",
    "        docs = [Document(page_content=t, metadata=m) for t, m in zip(texts, metadatas)]\n",
    "        return cls(vectorizer=vectorizer, docs=docs, tfidf_array=tfidf_array,preprocess_func=preprocess_func,  **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(\n",
    "        cls,\n",
    "        documents: Iterable[Document],\n",
    "        *,\n",
    "        tfidf_params: Optional[Dict[str, Any]] = None,\n",
    "        preprocess_func: Optional[Callable[[str], str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> TFIDFRetriever:\n",
    "        texts, metadatas = zip(*((d.page_content, d.metadata) for d in documents))\n",
    "        return cls.from_texts(\n",
    "            texts=texts, tfidf_params=tfidf_params, metadatas=metadatas, preprocess_func=preprocess_func, **kwargs\n",
    "        )\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        if self.preprocess_func:\n",
    "            query = self.preprocess_func(query)\n",
    "        query_vec = self.vectorizer.transform(\n",
    "            [query]\n",
    "        )  # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\n",
    "        results = cosine_similarity(self.tfidf_array, query_vec).reshape(\n",
    "            (-1,)\n",
    "        )  # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
    "        return_docs = []\n",
    "        for i in results.argsort()[-self.k :][::-1]:\n",
    "            return_docs.append(self.docs[i])\n",
    "        return return_docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "\n",
    "class BM25Retriever(BaseRetriever, BaseModel):\n",
    "    vectorizer: Any\n",
    "    docs: List[Document]\n",
    "    k: int = 4\n",
    "    preprocess_func: Callable[[str], str] = None\n",
    "    tokenizer: Callable[[str], List[str]] = None\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @classmethod\n",
    "    def from_texts(\n",
    "        cls,\n",
    "        texts: Iterable[str],\n",
    "        metadatas: Optional[Iterable[dict]] = None,\n",
    "        bm25_params: Optional[Dict[str, Any]] = None,\n",
    "        preprocess_func: Optional[Callable[[str], str]] = None,\n",
    "        tokenizer : Optional[Callable[[str], List[str]]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> BM25Retriever:\n",
    "        try:\n",
    "            from rank_bm25 import BM25Okapi\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Could not import rank_bm25, please install with `pip install \"\n",
    "                \"rank_bm25`.\"\n",
    "            )\n",
    "            \n",
    "        if preprocess_func:\n",
    "            texts_processed = [preprocess_func(t) for t in texts]\n",
    "        else:\n",
    "            texts_processed = texts\n",
    "            \n",
    "        if tokenizer:\n",
    "            tokenized_texts = [tokenizer(t) for t in texts_processed]\n",
    "        else:   \n",
    "            tokenized_texts = [t.split() for t in texts_processed]\n",
    "        \n",
    "        bm25_params = bm25_params or {}\n",
    "        vectorizer = BM25Okapi(tokenized_texts, **bm25_params)\n",
    "        metadatas = metadatas or ({} for _ in texts)\n",
    "        docs = [Document(page_content=t, metadata=m) for t, m in zip(texts, metadatas)]\n",
    "        return cls(vectorizer=vectorizer, docs=docs, preprocess_func=preprocess_func,  **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(\n",
    "        cls,\n",
    "        documents: Iterable[Document],\n",
    "        *,\n",
    "        bm25_params: Optional[Dict[str, Any]] = None,\n",
    "        preprocess_func: Optional[Callable[[str], str]] = None,\n",
    "        tokenizer : Optional[Callable[[str], List[str]]] = None,        \n",
    "        **kwargs: Any,\n",
    "    ) -> BM25Retriever:\n",
    "        texts, metadatas = zip(*((d.page_content, d.metadata) for d in documents))\n",
    "        return cls.from_texts(\n",
    "            texts=texts, tfidf_params=bm25_params, metadatas=metadatas,preprocess_func=preprocess_func, tokenizer=tokenizer, **kwargs\n",
    "        )\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        if self.preprocess_func:\n",
    "            query = self.preprocess_func(query)\n",
    "        tokenized_query = query.split()\n",
    "        return_docs = self.vectorizer.get_top_n(tokenized_query,self.docs, n=DOC_NUM)\n",
    "        return return_docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "\n",
    "import requests\n",
    "# download japanese stopwords\n",
    "url = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-ja/master/stopwords-ja.txt\"\n",
    "stopwords = requests.get(url).text.split(\"\\n\")\n",
    "\n",
    "import MeCab\n",
    "import ipadic\n",
    "\n",
    "\n",
    "# parser = MeCab.Tagger(\"-Owakati\")\n",
    "def extract_nouns_verbs(text):\n",
    "    parser = MeCab.Tagger(ipadic.MECAB_ARGS)\n",
    "    parsed_text = parser.parse(text)\n",
    "    lines = parsed_text.split('\\n')\n",
    "    nouns_verbs = []\n",
    "\n",
    "    for line in lines:\n",
    "        if '名詞' in line or '動詞' in line or \"形状詞\" in line:\n",
    "            parts = line.split('\\t')\n",
    "            word = parts[0]\n",
    "            if not word.isascii():\n",
    "                nouns_verbs.append(word)\n",
    "    return nouns_verbs\n",
    "\n",
    "def preprocess(text):\n",
    "    token_list = [token for token in extract_nouns_verbs(text) if token not in stopwords]\n",
    "    return \" \".join(token_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "tfidf_search = TFIDFRetriever.from_texts(a_df[\"Text\"].tolist(), preprocess_func=preprocess)\n",
    "tfidf_search.k = DOC_NUM\n",
    "\n",
    "bm25_search = BM25Retriever.from_texts(a_df[\"Text\"].tolist(), preprocess_func=preprocess)\n",
    "bm25_search.k = DOC_NUM\n",
    "\n",
    "tfidf_result = evaluate(query_list, lambda query: tfidf_search.get_relevant_documents(query))\n",
    "bm25_result = evaluate(query_list, lambda query: bm25_search.get_relevant_documents(query))\n",
    "\n",
    "result_df = pd.DataFrame(\n",
    "    [\n",
    "    [\"bm25\", bm25_result.mrr, bm25_result.recall_at_1, bm25_result.recall_at_5],\n",
    "    [\"tfidf\", tfidf_result.mrr, tfidf_result.recall_at_1, tfidf_result.recall_at_5]],\n",
    "    columns = [\"model_id\",\"mrr\",\"recall_at_1\",\"recall_at_5\"]\n",
    "    ).sort_values(\"mrr\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果を見ましょう。Sparse Retriewerのほうは意外に良い結果がを得ています。計算の速さやコスト等を考えると相当試すべき手法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>mrr</th>\n",
       "      <th>recall_at_1</th>\n",
       "      <th>recall_at_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bm25</td>\n",
       "      <td>0.608793</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>0.798595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.592323</td>\n",
       "      <td>0.454333</td>\n",
       "      <td>0.763466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_id       mrr  recall_at_1  recall_at_5\n",
       "0     bm25  0.608793     0.475410     0.798595\n",
       "1    tfidf  0.592323     0.454333     0.763466"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Retriever\n",
    "これまでDense RetrieverとSparse Retrieverを比較しました。機械学習の中でEnsembleという概念がありまして、つまり複数のモデルを組み合わせて、より良い結果を得ることができます。それと似たような概念で、Dense RetrieverとSparse Retrieverを組み合わせて、Hybrid Retrieverを作るとより良い結果を得ることができます。\n",
    "\n",
    "Hybridのほうほうとしては、Reciprocal Rank Fusion（RRF）という手法を使います。RRFは、複数の検索結果を組み合わせて、より良い検索結果を得るための手法です。RRFは、検索結果のランキングを組み合わせることで、検索結果のランキングを改善します。RRFは、以下の手順で実行されます。\n",
    "\n",
    "$$\\operatorname{RRF}(d) = \\sum_{i=1}^{n} \\frac{1}{k + r_i(d)}$$\n",
    "\n",
    "ここで、\n",
    "\n",
    "$d$ は検索結果のドキュメントです。\n",
    "$n$ は統合する検索結果の数です。\n",
    "$r_i(d)$ は、$i$番目の検索結果におけるドキュメント$d$の順位です。\n",
    "$k$ は、RRFのパラメータです。この値を大きくすることで、検索結果の順位に対するペナルティを調整できます。通常、$k$は60などの定数値をとります。\n",
    "\n",
    "実際の例で語りましょう。\n",
    "\n",
    "| BM25 Ranking | Dense Ranking | Results          |\n",
    "|--------------|---------------|------------------|\n",
    "| A            | B             | A: 1/1 + 1/3 = 1.3|\n",
    "| B            | C             | B: 1/2 + 1/1 = 1.5|\n",
    "| C            | A             | C: 1/3 + 1/2 = 0.83|\n",
    "\n",
    "\n",
    "(https://weaviate.io/blog/hybrid-search-explained)\n",
    "\n",
    "上のように、BM25とDense Retriever両方の結果が出た後、それぞれのドキュメントのランクの逆数を足して、その結果をもとに並べ替えます。例えば、AはBM25の結果で1位、Dense Retrieverの結果で3位なので、Kが0の場合はAの最終のスコアは1/1 + 1/3 = 1.3となります。\n",
    "\n",
    "::: {.callout-note collapse=\"false\"}\n",
    "## パラメーター$k$の影響\n",
    "$k$パラメータは、RRFの式において検索結果の順位に対するペナルティを調整する役割を持っています。$k$が大きいほど、順位が低いドキュメントへのペナルティが緩やかになります。逆に、$k$が小さいほど、順位が低いドキュメントへのペナルティが厳しくなります。\n",
    "\n",
    "具体的には、$k$が大きい場合、異なる検索結果間で順位が低いドキュメントでも、それらの組み合わせによってRRFスコアが上がる可能性があります。これにより、検索結果の多様性が増すことが期待されます。\n",
    "\n",
    "一方で、$k$が小さい場合、順位が高いドキュメントがより重視されるため、検索結果の集中度が高まることが期待されます。ただし、あまりにも$k$が小さいと、異なる検索結果間でのバランスが悪くなり、検索結果の統合効果が十分に発揮されない可能性があります。\n",
    "\n",
    "通常、$k$は60などの定数値をとりますが、実際の検索タスクや評価指標によって最適な$k$の値は異なる場合があります。実際の応用においては、パラメータチューニングや交差検証を用いて適切な$k$の値を決定することが望ましいです。\n",
    ":::\n",
    "\n",
    "その実装は以下の通りです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_reciprocal_rank_fusion(rank_lists, weights, k=-1):\n",
    "    \"\"\"\n",
    "    Perform weighted Reciprocal Rank Fusion on multiple rank lists.\n",
    "\n",
    "    Args:\n",
    "        rank_lists (list of lists): A list of rank lists, where each rank list contains unique items.\n",
    "        weights (list of float): A list of weights corresponding to the rank lists.\n",
    "        k (float, optional): A constant added to the rank, controlling the balance between the importance\n",
    "            of high-ranked items and the consideration given to lower-ranked items. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "        list: The final aggregated list of items sorted by their weighted RRF scores in descending order.\n",
    "    \"\"\"\n",
    "    if k == -1:\n",
    "        k = 0.5 * len(rank_lists[0])\n",
    "\n",
    "    if len(rank_lists) != len(weights):\n",
    "        raise ValueError(\"Number of rank lists must be equal to the number of weights.\")\n",
    "    \n",
    "    rrf_scores = {}\n",
    "    \n",
    "    for rank_list, weight in zip(rank_lists, weights):\n",
    "        for rank, item in enumerate(rank_list, start=1):\n",
    "            rrf_score = weight * (1 / (rank + k))\n",
    "            \n",
    "            if item in rrf_scores:\n",
    "                rrf_scores[item] += rrf_score\n",
    "            else:\n",
    "                rrf_scores[item] = rrf_score\n",
    "\n",
    "    # Sort items by their RRF scores in descending order\n",
    "    sorted_items = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)\n",
    "\n",
    "    return sorted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "bm25_rank_result = bm25_result.result_df[\"rank_result\"]\n",
    "tfidf_rank_result = tfidf_result.result_df[\"rank_result\"]\n",
    "faiss_rank_result = faiss_similarity_result.result_df[\"rank_result\"]\n",
    "\n",
    "fused_rank_result = [\n",
    "    weighted_reciprocal_rank_fusion(\n",
    "        [bm25_rank_result[i],  faiss_rank_result[i]],\n",
    "        [0.2, 0.8]\n",
    ")\n",
    "    for i in range(len(bm25_rank_result))\n",
    "]\n",
    "\n",
    "fused_rank_s = [fused_rank_result[i].index(query_list[i][1]) + 1 for i in range(len(query_list))]\n",
    "fused_df = bm25_result.result_df.copy()\n",
    "fused_df[\"rank\"] = fused_rank_s\n",
    "\n",
    "fused_result = EvaluationResults(fused_df,  mrr=mrr(fused_df[\"rank\"]), recall_at_1=recall_at_k(fused_df[\"rank\"], 1), recall_at_5=recall_at_k(fused_df[\"rank\"], 5))\n",
    "\n",
    "result_dict = {\n",
    "    \"faiss__cosine_similarity\": faiss_similarity_result,\n",
    "    \"faiss_mmr\": faiss_mmr_result,\n",
    "    \"svm\": svm_result,\n",
    "    \"tfidf\": tfidf_result,\n",
    "    \"bm25\": bm25_result,\n",
    "    \"hybrid\": fused_result,\n",
    "}\n",
    "result_df = pd.DataFrame(\n",
    "    [\n",
    "        [k, v.mrr, v.recall_at_1, v.recall_at_5]\n",
    "        for k, v in result_dict.items()\n",
    "    ],\n",
    "    columns=[\"model_id\", \"mrr\", \"recall_at_1\", \"recall_at_5\"],\n",
    ").sort_values(\"mrr\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでテストすると、Hyperのやり方は一番精度が良いことがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>mrr</th>\n",
       "      <th>recall_at_1</th>\n",
       "      <th>recall_at_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>0.703478</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.882904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faiss__cosine_similarity</td>\n",
       "      <td>0.685327</td>\n",
       "      <td>0.550351</td>\n",
       "      <td>0.868852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faiss_mmr</td>\n",
       "      <td>0.622978</td>\n",
       "      <td>0.550351</td>\n",
       "      <td>0.679157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bm25</td>\n",
       "      <td>0.608793</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>0.798595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.592323</td>\n",
       "      <td>0.454333</td>\n",
       "      <td>0.763466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.520898</td>\n",
       "      <td>0.388759</td>\n",
       "      <td>0.683841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model_id       mrr  recall_at_1  recall_at_5\n",
       "5                    hybrid  0.703478     0.573770     0.882904\n",
       "0  faiss__cosine_similarity  0.685327     0.550351     0.868852\n",
       "1                 faiss_mmr  0.622978     0.550351     0.679157\n",
       "4                      bm25  0.608793     0.475410     0.798595\n",
       "3                     tfidf  0.592323     0.454333     0.763466\n",
       "2                       svm  0.520898     0.388759     0.683841"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "今回はRAGのRetrieverの性能を比較しました。 その結果としては、\n",
    "\n",
    "- Dense Retrieverの中でデフォルトのGPTのEmbeddingモデル+Cosine類似度の組み合わせるが一番良かったです。\n",
    "\n",
    "- Sparse Retrieverの中でBM25は計算スピードが早くてそこそこ良いパフォーマンスを出せています。\n",
    "\n",
    "- Hybridのやり方で、Dense RetrieverとSparse Retrieverを組み合わせると一番良い結果を出せています。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notion-db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
