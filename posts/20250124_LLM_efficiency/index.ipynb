{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"なぜLLMはGPUが弱小なMacBookの中でもちゃんと動くか 計算力とメモリスピードの視点から\"\n",
        "date: 2025-1-24\n",
        "description-meta: \"\"\n",
        "categories: [LLM, Web]\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-summary: \"Show the code\"\n",
        "---"
      ],
      "id": "dbf4ef80"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Macbookでも意外にLLMを動くことができる\n",
        "\n",
        "最近、OpenAI O1 レベルのOSSモデルDeepSeek R1がでました。それで、XではM4 64GBのMac miniを8台繋いてクラスターにして671Bのモデルを動いたスレットがありました。\n",
        "\n",
        "<blockquote class=\"twitter-tweet\" data-media-max-width=\"560\">\n",
        "\n",
        "<p lang=\"en\" dir=\"ltr\">\n",
        "\n",
        "Running DeepSeek-V3 on M4 Mac Mini AI Cluster<br><br>671B MoE model distributed across 8 M4 Pro 64GB Mac Minis.<br><br>Apple Silicon with unified memory is a great fit for MoE. <a href=\"https://t.co/FmeARutaxq\">pic.twitter.com/FmeARutaxq</a>\n",
        "\n",
        "</p>\n",
        "\n",
        "— EXO Labs (@exolabs) <a href=\"https://twitter.com/exolabs/status/1872444906851229814?ref_src=twsrc%5Etfw\">December 27, 2024</a>\n",
        "\n",
        "</blockquote>\n",
        "\n",
        "\n",
        "```{=html}\n",
        "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
        "```\n",
        "\n",
        "\n",
        "スレットを見た時に、「嘘らだろう、671BのモデルをMac miniで動く？」は最初の感想でした。色々調べた後、LLMを動く際には、制限としては「計算スピード」と「メモリのスピード」の2つあることがわかりました。Batch sizeが1で推論する場合は、だいたい計算スピードではなく、メモリのスピードで足が引っ張られています。\n",
        "\n",
        "例えば、`int4` で量子化したモデルで推論する場合、A100とMac mini M4を比較して見たら、\n",
        "\n",
        "|    デバイス     | メモリスピード |    計算力 |\n",
        "|:---------------:|---------------:|----------:|\n",
        "|      A100       |      1935 GB/s | 1248 TOPS |\n",
        "| Mac mini M4 |       120 GB/s |   38 TOPS |\n",
        "|  A100/ Macmini  |             16x |       32x |\n",
        "\n",
        "A100の計算力はMac miniの32倍ですが、メモリスピードは7倍しかないです。なので理論上はMac miniはA100の1/7のスピードで出力することができます。\n",
        "\n",
        "これからは、この話を更に展開して原理までわかるようにします。\n",
        "\n",
        "## GPUの仕組みを理解する\n",
        "\n",
        "CPU の構造とよく似ていて、GPUもキャッシュがあります。下図でメモリのアーキテクチャを示しています。\n",
        "\n",
        "![](images/paste-4.png){width=\"456\"}\n",
        "\n",
        "-   **GPU SRAM：** GPUの計算ユニット内蔵のメモリ。最速だが、最も容量が小さい（19TB/s、20MB）。\n",
        "\n",
        "-   **GPU HBM：** GPUのメインメモリ。中間の速度と容量（1.5TB/s、40GB）。\n",
        "\n",
        "-   **メインメモリ DRAM：** 最も低速だが、最も容量が大きい（12.8GB/s、\\>1TB）。\n",
        "\n",
        "計算する前に、まずモデルのパラメータをCPUのメモリからGPU HBMに送る必要があります。`model.to(\"cuda\")`はこれをやっています。計算する際に、必要なデータをHBMからGPUチップに内装されるSRAMに転送し、そこで計算をしています。計算が終わっていても、次のデータがまだ来ていない場合は、計算を止めて、データの転送を待たないと行けないことです。バッチサイズが小さい場合は、計算量が少ないため、データの転送を待つことになり、これはいわゆる「メモリバンド」のことです。\n",
        "\n",
        "## 実際に推論スピードを概算してみる\n",
        "\n",
        "### 概算方法\n",
        "\n",
        "まず、概算するための式を出します。\n",
        "\n",
        "$$\n",
        "\\text{latency}_\\text{memory} := \\dfrac{P\\cdot n_{\\text{bytes}}}{n_{\\text{memory bandwidth}}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{latency}_\\text{compute} := \\dfrac{2 \\cdot P \\cdot B}{n_{\\text{flops}}}\n",
        "$$\n",
        "\n",
        "この中で、\n",
        "\n",
        "-   $P$はモデルのパラメータ数。\n",
        "\n",
        "-   $n_{\\text{bytes}}$はデータタイプに必要なバイト数。例えば、デフォルトのfp32を使う場合は4バイト、fp16の場合は2バイト、int4の場合は0.5バイト。\n",
        "\n",
        "-   $n_{\\text{memory bandwidth}}$は名前の通り、メモリ帯域幅のこと。\n",
        "\n",
        "-   $B$はバッチサイズ。\n",
        "\n",
        "-   $n_{\\text{flops}}$は計算スピード\n",
        "\n",
        "メモリのレイテンシーは割とわかりやすいです。分子は1トークンを計算するために計算ユニットのメモリ(SRAM)に送るデータ量のことです。それをメモリ帯域幅に割ると、データ転送の時間を概算することができます。\n",
        "\n",
        "計算のレイテンシーだと少しややこしいです。概算する際には、経験則で1トークンにかかる計算量を$2P$とします(この後で詳細に計算してみます)。それをかけるバッチサイズ$B$にすると、$B$個のトークンを計算するために必要な計算量になります。それを計算スピードで割ると、計算の時間を概算することができます。\n",
        "\n",
        "\n",
        "### A100とMac mini M4を比較する\n",
        "\n",
        "これで、計算スピードとメモリスピードの比較ができるようになりました。表にある内容を式に代入して、A100とMac mini M4を実際に比較して比較してみます。\n",
        "\n",
        "|    デバイス     | メモリ観点で<br>1秒処理できるトークン数 |    計算力観点で<br>1秒処理できるトークン数 |\n",
        "|:---------------:|---------------:|----------:|\n",
        "|      A100       |      552 | 89,142 |\n",
        "| Mac mini M4 |       34 |   2,714 |\n",
        "|  A100/ Macmini  |             16x |       32x |\n"
      ],
      "id": "c6e7ae9f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "P = 7e9  # 7Bモデル\n",
        "n_bytes = 0.5  # int4\n",
        "n_memory_bandwidth_A100 = 1935e9\n",
        "n_memory_bandwidth_Macmini = 120e9\n",
        "\n",
        "n_tops_A100 = 1248e12\n",
        "n_tops_Macmini = 38e12\n",
        "\n",
        "\n",
        "def memory_latency(n_bytes, n_memory_bandwidth, P):\n",
        "    return P * n_bytes / n_memory_bandwidth\n",
        "\n",
        "\n",
        "def compute_latency(n_tops, P, B=1):\n",
        "    return 2 * P * B / n_tops\n",
        "\n",
        "\n",
        "memory_latency_A100 = memory_latency(n_bytes, n_memory_bandwidth_A100, P)\n",
        "memory_latency_Macmini = memory_latency(n_bytes, n_memory_bandwidth_Macmini, P)\n",
        "\n",
        "compute_latency_A100 = compute_latency(n_tops_A100, P)\n",
        "compute_latency_Macmini = compute_latency(n_tops_Macmini, P)\n",
        "\n",
        "print('1/memory_bandwidth_A100: ', int(1 / memory_latency_A100))\n",
        "print('1/memory_bandwidth_Macmini: ', int(1 / memory_latency_Macmini))\n",
        "print('1/compute_A100: ', int(1 / compute_latency_A100))\n",
        "print('1/compute_Macmini: ', int(1 / compute_latency_Macmini))\n",
        "print('A100/Macmini memory: ', int(memory_latency_Macmini / memory_latency_A100))\n",
        "print('A100/Macmini compute: ', int(compute_latency_Macmini / compute_latency_A100))"
      ],
      "id": "30542fad",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}