---
title: "HyDE 論文解読"
date: "2023-04-30"
description-meta: "HyDE論文では、教師なしのZero-shot dense retrievalシステムを提案。従来のDense retrieverとは異なり、HyDEはQueryから仮想的なDocumentを生成し、その類似度でランキング。InstructGPTで仮想ドキュメントを生成し、Contrieverを使ってEmbeddingに変換。様々なデータセットでテストした結果、HyDEは教師なし領域で従来のContrieverを凌駕し、教師ありモデルとも遜色ない精度を示した。実装はLangChainで利用可能。"
categories: [NLP, Information_retrieval, paper]
---

HyDE論文では、教師なしのZero-shot dense retrievalシステムを提案。従来のDense retrieverとは異なり、HyDEはQueryから仮想的なDocumentを生成し、その類似度でランキング。InstructGPTで仮想ドキュメントを生成し、Contrieverを使ってEmbeddingに変換。様々なデータセットでテストした結果、HyDEは教師なし領域で従来のContrieverを凌駕し、教師ありモデルとも遜色ない精度を示した。実装はLangChainで利用可能。

論文URL：<https://arxiv.org/abs/2204.07496>

# 1 Introduction

Dense retrievalについて様々な研究が行われているが、Zero-shot dense retrievalはまだ難しい。多くの研究はMS-MARCOのような大規模なデータセットを使って転移学習をしているが、MS-MARCOが商用不可の制限があるし、他のドメインに汎化が難しい課題がある。一方、新たなデータをラベリングするためには莫大なコストがかかる。

![HyDE](images/paste-1.png){fig-alt="HyDE"}

この論文では教師なしのZero-shot dense retrievalの仕組HyDEを提案した。 従来のDense retrieverはQueryとDocumentとの類似度でランクを決めている。HyDEはQueryを利用して、まずLLMでそのQueryを答える仮想なDocumentを生成する。生成したDocumentとDocumentの類似度でランキングしている。

## 2 Related works

## 3 Methology

## 4 Experiments

仮想なDocumentはInstructGPTで生成した。生成したDocumentをContrieverを用いてEmbeddingに変換した。

テストのデータとしては、MS-MARCOをベースとしたTREC DL19 DL20があり、BEIRからもLow-resourceのデータセットをいくつ利用した。また、英語以外、韓国語、日本語等データセットも使った。

![web search query sets](images/paste-2.png)

![low-resource datasets](images/paste-4.png)

![non-English retrieval](images/paste-5.png){width="400"}

結果を見ると、教師なしの領域でHyDEは全面的に以前のContrieverを超えた。また、教師あるのモデルから比較しても遜色しない精度を出した。

## 5 Analysis

![LLM difference](images/paste-6.png){fig-alt="LLM difference" width="300"}

当たり前だが、仮想なドキュメントを生成するLLMによって最終の精度が違う。また、HyDEは教師なしの手法だが、教師ありのRetrieverの精度も向上できる。

## 実装

HyDEはすでに[LangChain](https://python.langchain.com/en/latest/modules/chains/index_examples/hyde.html)で実装されている。

``` python
from langchain.llms import OpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import HypotheticalDocumentEmbedder

base_embeddings = OpenAIEmbeddings()
llm = OpenAI()

# Load with `web_search` prompt
embeddings = HypotheticalDocumentEmbedder.from_llm(llm, base_embeddings, "web_search")

# Now we can use it as any embedding class!
result = embeddings.embed_query("Where is the Taj Mahal?")
```