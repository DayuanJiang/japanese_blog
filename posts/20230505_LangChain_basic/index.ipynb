{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"LangChainのベーシックを全面解説する\"\n",
    "date: 2023-05-06\n",
    "description-meta: \"InPars-lightは、無料で利用可能な言語モデルBLOOMをランキングモデルを使用し、1000個ではなく100個の候補レコードを再ランクしした。 先行研究の主要な知見を再現するだけでなく、Consistency checkingとAll-domain pre-trainingを組み合わせることで、非常に効率的で小型なモデルMiniLM-L6-30Mを訓練し、すべてのデータセットでBM25を上回る性能を達成した。最後に、大きなDeBERTA-v3-435Mモデルを使用して、7倍大きなMonoT5-3Bの性能をほぼマッチさせることができた。\"\n",
    "categories: [NLP, LLMs, LangChain]\n",
    "format:\n",
    "    html:\n",
    "        mermaid:\n",
    "            theme: neutral\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前書き\n",
    "\n",
    "OpenAIのGPTのAPIを利用してアプリを作成するには、今まで一番使いやすいパッケージはLangChain🦜️🔗 だと思います。本文では、LangChainの基本的な使い方を優しく説明します。\n",
    "\n",
    "![LangChain](langchain.png){width=300}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境設定"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは定番の`pip`からインストールすることです。\n",
    "```bash\n",
    "pip install langchain, openai\n",
    "```\n",
    "そのつぎに、OpenAIのAPIキーを取得して、環境変数に設定します。\n",
    "APIは[ここ](https://platform.openai.com/account/api-keys)から取得できます。\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "```\n",
    "直接にAPIキーを書くのはセキュリティ上の問題があるので、スクリプトを共有する場合は(例えば本文)、APIキーを別ファイルに保存し、ファイルから読み込んだほうがよいです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "with open(\"../../.env\", \"r\") as f: \n",
    "    os.environ.update(dict([line.strip().split(\"=\") for line in f.readlines()]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAIのGPTモデル\n",
    "\n",
    "LangChainの中にOpenAIのGPTモデルを使うラッパーがあります。現在使えるモデルはテキスト補完モデルとChatモデルの2種類あります。生成モデルの場合は以下のように使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "東京です。\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0)\n",
    "output = llm(\"日本の首都は?\")\n",
    "print(output.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、Chatモデルを利用して対話を行うこともできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love programming.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "output = chat([HumanMessage(content=\"文法を修正してください:I loves programming.\")])\n",
    "print(output.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各モデルの特性のまとめ\n",
    "各モデルの値段や、最大トークン数、モデルサイズは以下の表にまとめました。\n",
    "\n",
    "**テキスト補完モデル**\n",
    "\n",
    "<table style= 'width:100%'>\n",
    "  <tr>\n",
    "    <th>モデル名</th>\n",
    "    <th>値段(1k tokensごと)</th>\n",
    "    <th>最大トークン数</th>\n",
    "    <th>モデルサイズ(推測)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Davinci</td>\n",
    "    <td>$0.0200</td>\n",
    "    <td>4,097</td>\n",
    "    <td>175B</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Curie</td>\n",
    "    <td>$0.0020</td>\n",
    "    <td>4,097</td>\n",
    "    <td>6.7B</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Babbage</td>\n",
    "    <td>$0.0005</td>\n",
    "    <td>4,097</td>\n",
    "    <td>1.3B</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Ada</td>\n",
    "    <td>$0.0004</td>\n",
    "    <td>4,097</td>\n",
    "    <td>350M</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "**Chatモデル**\n",
    "<table style= 'width:100%'>\n",
    "  <tr>\n",
    "    <th>モデル名</th>\n",
    "    <th>値段(**Prompt)</th>\n",
    "    <th>値段(**補完)</th>\n",
    "    <th>最大トークン数</th>\n",
    "    <th>モデルサイズ(推測)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>gpt-3.5-turbo</td>\n",
    "    <td>$0.002</td>\n",
    "    <td>$0.002</td>\n",
    "    <td>4,096</td>\n",
    "    <td>6.7B</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>gpt-4</td>\n",
    "    <td>$0.03</td>\n",
    "    <td>$0.06</td>\n",
    "    <td>8,192</td>\n",
    "    <td>6.7B</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>gpt-4-32k</td>\n",
    "    <td>$0.06</td>\n",
    "    <td>$0.12</td>\n",
    "    <td>32,768</td>\n",
    "    <td>1.3B</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    ":::{.callout-note}\n",
    "ここで注意することとしてはGPT4の値段です。インプットするテキストが`prompt`、生成したテキストは`completion`に分かれていて、`prompt`の値段と`completion`の値段を足したものがGPT4の値段になります。 \n",
    ":::\n",
    " \n",
    "\n",
    "### モデルの使い分け\n",
    "モデルの使い分けについては、最も使われているのはChatモデルの`gpt-3.5-turbo`と`gpt-4`です。`gpt-3.5-turbo`はモデルのサイズが小さいので、生成時間が短く、値段も安いです。一方、`gpt-4`は性能が良いので、性能を求める場合は`gpt-4`のほうが良いです。また、`gpt-4`の最大トークン数が8Kになっているので、生成するテキストの長さが長い場合もこちらを使うほうがいいです。\n",
    "\n",
    "他のモデルはほとんど使われないので、必要に応じて詳細を見れば良いです。\n",
    "\n",
    "### Tokenの計算方法\n",
    "Tokenの計算方法については、[こちら](https://www.jiang.jp/posts/20230505_tiktoken/#tictoken%E3%81%AE%E6%8C%99%E5%8B%95)で紹介したので、本文では割愛します。要するに、日本語千文字のドキュメントはおおよそ1,000トークンになり、それを処理するには`gpt-3.5-turbo`の場合は概算で0.59円、`gpt-4`の場合は概算で$9.7円かかります。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "LangChainのPrompt TemplateはPromptを簡単に作成するためのモジュールです。Example selector付きのPromptを作るにはとても役に立ちます。でもそれはよりアドバンス的なやり方なので、入門の段階では単純にPythonのf-stringとして使えれば良いです。\n",
    "\n",
    "Promptのテンプレートを書いた後、それを`PromptTemplate`のインスタンスに渡して、`PromptTemplate`の`format`メソッドを呼び出すと、Promptが生成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私はりんごが好きです。\n",
      "私はみかんが好きです。\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"私は{fruit}が好きです。\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "print(prompt_template.format(fruit=\"りんご\"))\n",
    "print(prompt_template.format(fruit=\"みかん\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStore\n",
    "ドキュメントを検索するためには、`VectorStore`を作成する必要があります。`VectorStore`はドキュメントのリストを受け取って、それをベクトルに変換して保存します。検索する際に、検索クエリをベクトルに変換して、ベクトルの類似度を計算して、類似度が高いドキュメントを返します。\n",
    "\n",
    ":::{.callout-tip}\n",
    "## FAISSについて\n",
    "FAISSはMetaが開発した高速な類似性検索ライブラリです。Faissは、大量のベクトルデータを格納し、高速な検索を行うことができます。\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 私はりんごが好きです。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import FAISS\n",
    "# create test data\n",
    "with open(\"./test_data.txt\", \"w\") as f:\n",
    "    fruits = [\"りんご\", \"みかん\", \"バナナ\", \"パイナップル\", \"ぶどう\"]\n",
    "    for fruit in fruits:\n",
    "        f.write(f\"私は{fruit}が好きです。\\n\")\n",
    "        \n",
    "# load test data\n",
    "loader = TextLoader('./test_data.txt', encoding='utf8')\n",
    "\n",
    "# query test data\n",
    "index = VectorstoreIndexCreator(vectorstore_cls=FAISS).from_loaders([loader])\n",
    "index.query(\"りんご\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain\n",
    "ChainはLangChainの中心的な概念です。今まで紹介した複数の部品を組み合わせでChainを作ることができます。インプットが入力された後、Chainの内部で処理し、アウトプットを出す。\n",
    "\n",
    "例えば、PromptTemplateとLLMをつなぐChainを作ることができます。PromptTemplateはPromptを生成するので、LLMのインプットになります。LLMはPromptを受け取って、それを補完して、アウトプットを生成します。こうしてPromptTemplateとLLMをつなぐChainを作ることができます。\n",
    "\n",
    "```{mermaid}\n",
    "%%| fig-cap: \"Chainのダイアグラムの例\"\n",
    "flowchart LR\n",
    "    Input([Input])-->PromptTemplate\n",
    "    LLM-->Output([Output])\n",
    "    subgraph Chain\n",
    "    PromptTemplate-->formattedPrompt([Formatted Prompt])\n",
    "    formattedPrompt-->LLM\n",
    "    end\n",
    "    style PromptTemplate stroke:#333,stroke-width:4px\n",
    "    style LLM stroke:#333,stroke-width:4px\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate.from_template(\"{country}の首都は何ですか？\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで各国の首都は簡単に検索できるようになりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "東京です。\n",
      "ワシントンD.C.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run({\"country\": \"日本\"}).strip())\n",
    "print(chain.run({\"country\": \"アメリカ\"}).strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "AgentはChainよりも高いレベルの概念です。Agentはツールを使うことができます。それにより、Agentは内部環境にとどまらず、外部環境ともやり取りできます。\n",
    "\n",
    "一番シンプルの例としてはBingChatがあげられます。ユーザーのクエリーを受けた後、BingChatはインタネットから情報を検索し、それをサマリーして、ユーザーのクエリに答えます。\n",
    "\n",
    "Agentの中身は複雑でドキュメントに書いていないので、今回は挙動だけ見せます。ここでBingChatに似ている機能を実現するAgentを作ります。このAgentはユーザーのクエリーを受け取って、それをインタネットで検索し、その答えを返すことができます。また、外部の電卓ツールを利用して計算もできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out the temperature and then calculate its square.\n",
      "Action: Search\n",
      "Action Input: 今日の気温\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mニューヨーク, NY, アメリカ合衆国 の天気. 4. 今日 · 1時間ごと · 10日間 · レーダー. 1時間ごとの天気-ニューヨーク, NY, アメリカ合衆国. 13:48 EDT時点 ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find the temperature from the search results\n",
      "Action: Search\n",
      "Action Input: 今日の気温 ニューヨーク\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m16:00 · 体感温度16° · 風南東 8 km/h · 湿度47% · 紫外線指数2/10 · 雲量78% · 雨量0 cm ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now have the temperature, I need to calculate its square\n",
      "Action: Calculator\n",
      "Action Input: 16^2\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 256\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 今日の気温は16度で、その2乗は256です。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'今日の気温は16度で、その2乗は256です。'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"今日の気温は何度ですか？その2乗は何ですか？\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「今日の気温は何度ですか？その2乗は何ですか？」のクエリーを投げた後、Agentのほうはまずやるべきことを決めました。やるべきことをプランニングしながら、自分が持っているツールを駆使し、クエリーに答えました。\n",
    "\n",
    "## まとめ\n",
    "これでLangChainの中にあるMemory以外のものをひと通り浅く紹介しました。LangChainの開発はまだ初期の段階なので、APIの設計や、ドキュメントの充実さなどの問題があります。今後は各概念を解剖する記事を書いていきます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notion-db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
