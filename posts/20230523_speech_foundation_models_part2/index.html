<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.340">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-05-28">
<meta name="description" content="Textless NLPやAudioLMなどのモデルは、音声翻訳や音声合成、音声生成などのタスクに対応できます。また、VALL-EやWhisper、USMなどのモデルは、音声認識やテキストから音声への変換などのタスクに適用されています。">

<title>blog - 音声基礎モデルの紹介 Part2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-56TE34D1Z4"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-56TE34D1Z4', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="http://www.linkedin.com/in/jiang-dayuan" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#part2-speech-large-language-models" id="toc-part2-speech-large-language-models" class="nav-link active" data-scroll-target="#part2-speech-large-language-models">Part2 Speech Large Language Models</a></li>
  <li><a href="#textless-nlp-project" id="toc-textless-nlp-project" class="nav-link" data-scroll-target="#textless-nlp-project">Textless NLP Project</a>
  <ul class="collapse">
  <li><a href="#generative-spoken-language-modeling" id="toc-generative-spoken-language-modeling" class="nav-link" data-scroll-target="#generative-spoken-language-modeling">Generative Spoken Language Modeling</a></li>
  <li><a href="#speech-resynthesis" id="toc-speech-resynthesis" class="nav-link" data-scroll-target="#speech-resynthesis">Speech Resynthesis</a></li>
  <li><a href="#speech-translation-unit-bart" id="toc-speech-translation-unit-bart" class="nav-link" data-scroll-target="#speech-translation-unit-bart">Speech Translation: Unit BART</a></li>
  <li><a href="#unity" id="toc-unity" class="nav-link" data-scroll-target="#unity">UnitY</a></li>
  </ul></li>
  <li><a href="#audiolm" id="toc-audiolm" class="nav-link" data-scroll-target="#audiolm">AudioLM</a></li>
  <li><a href="#vall-e" id="toc-vall-e" class="nav-link" data-scroll-target="#vall-e">VALL-E</a></li>
  <li><a href="#part-3-other-speech-foundation-models" id="toc-part-3-other-speech-foundation-models" class="nav-link" data-scroll-target="#part-3-other-speech-foundation-models">Part 3 Other Speech Foundation Models</a>
  <ul class="collapse">
  <li><a href="#whisper" id="toc-whisper" class="nav-link" data-scroll-target="#whisper">Whisper</a></li>
  <li><a href="#usm" id="toc-usm" class="nav-link" data-scroll-target="#usm">USM</a></li>
  </ul></li>
  <li><a href="#まとめ" id="toc-まとめ" class="nav-link" data-scroll-target="#まとめ">まとめ</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">音声基礎モデルの紹介 Part2</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Speech Recognition</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 28, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>音声基礎モデルに関する内容を3部分に分けて紹介しています。このポストはPart2と3について紹介します。</p>
<p>前のパートは<a href="https://www.jiang.jp/posts/20230523_speech_foundation_models_part1/">こちら</a>です。</p>
<p>内容はYoutubeで音声認識に関する講義に基づいています。</p>
<p>資料のリンクは<a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbWJTZTdHTl9oR2JqaFhjbFJpYk1tVWo5LWQtZ3xBQ3Jtc0tta2pBZjY3dGQ5NFF3WkxuMWJvVXF2U1pwTU1MZktFTlRMYnFxWHBMY00yNUZ2UEdUT1lULWV3Ml9kYmZfLUt1aXQtQ1BlUV9WRnFhd1FHU0JmOFgxS1ozRERLR3MtVEhWdXJMZEhONTVyV00zZVg0WQ&amp;q=https%3A%2F%2Fspeech.ee.ntu.edu.tw%2F%7Ehylee%2Fml%2Fml2023-course-data%2F%25E5%25BC%25B5%25E5%2587%25B1%25E7%2588%25B2-x-%25E6%25A9%259F%25E5%2599%25A8%25E5%25AD%25B8%25E7%25BF%2592-x-%25E8%25AA%259E%25E9%259F%25B3%25E5%259F%25BA%25E7%259F%25B3%25E6%25A8%25A1%25E5%259E%258B.pdf&amp;v=m7Be7ppR6q0">こちら</a>です。</p>
<p>今回の内容は音声基礎モデルです。全体は3部分に分けられます。</p>
<ol type="1">
<li>Speech Representation Learning(音声表現学習)</li>
<li>Speech Large Lanuage Models(音声大型言語モデル)</li>
<li>Other Speech Foundation Models(その他の音声基礎モデル)</li>
</ol>
<p>このポストはPart2から始まります。</p>
<section id="part2-speech-large-language-models" class="level2">
<h2 class="anchored" data-anchor-id="part2-speech-large-language-models">Part2 Speech Large Language Models</h2>
<p><img src="images/paste-1.png" class="img-fluid"></p>
</section>
<section id="textless-nlp-project" class="level2">
<h2 class="anchored" data-anchor-id="textless-nlp-project">Textless NLP Project</h2>
<p>音声翻訳を行なう場合、通常はCascaded Systemという複数のモデルを繋ぐ方法が使われます。まず音声をテキストに変換し、変換されたテキストを機械翻訳のモデルに通して翻訳します。翻訳したテキストを再びTTSモデルで音声に変換します。</p>
<p>しかし、この方法だと、モデルを学習するためにテキストデータが必要になります。また、複数の中間ステップがあるため、中間ステップで一つ間違える最終結果が間違ってしまいます。</p>
<p><img src="images/paste-2.png" class="img-fluid"></p>
<p>Textless NLPでは、中間ステップで生成されるテキストを「疑似テキスト（Psuedo text）」に置き換えることで、テキストを必要とせず、音声から音声へのエンドツーエンド学習が可能になりました。また、疑似テキストを用いて、現在のNLP技術を活用して中間ステップの改善ができます。</p>
<p><img src="images/paste-3.png" class="img-fluid"></p>
<p>さらに、音声翻訳のみではなく、中間のモデルを差し替えることで別のタスクに対応することができます。例えば、Speech continuationの場合はGPTを使います。音声合成の場合は中間のモデルを使わないです。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Speech continuation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Speech continuation（スピーチ・コンティニュエーション）は、音声認識や自然言語処理技術を用いて、話者の発言を他の話者が続けることができるようにする研究です。この研究は、人間と人工知能（AI）がスムーズにコミュニケーションできるようにすることを目的としています。具体的には、話者の言語パターンやスタイルを学習し、それに基づいて自然で適切なレスポンスを生成する技術を開発しています</p>
</div>
</div>
<p><img src="images/paste-4.png" class="img-fluid"></p>
<p>Psuedo textの作り方はPart 1で紹介した音声の量子化と同じです。HuBERTを使って音声の特徴を抽出し、K-meansで得たカテゴリをTokenとして扱います。</p>
<p><img src="images/paste-5.png" class="img-fluid"></p>
<section id="generative-spoken-language-modeling" class="level3">
<h3 class="anchored" data-anchor-id="generative-spoken-language-modeling">Generative Spoken Language Modeling</h3>
<p>Textless NLPでSpeech continuationの例を説明します。まず、HuBERTを使用して音声をトークンに変換します。ここでTokenを得たことで、次のステップは完全にNLPの領域になります。GPTと同じようなモデルを使って次のTokenを予測するモデルを作ることができます。</p>
<p><img src="images/paste-6.png" class="img-fluid"></p>
<p>最後に予測されたTokenをDecoderに渡し、音声を合成してもらいます。音声合成モデルも学習する必要があります。学習の方法としては、音声とTokenのペアを用意して、Tokenをインプットし、音声をアウトプットするモデルを学習します。</p>
<p><img src="images/paste-7.png" class="img-fluid"></p>
<p>中間のモデルは言語モデルなので、条件なしの音声生成もできます。一方で、条件なしでは、一見意味があるように見えるものが出力されますが、よく観察すると、意味が通じないものが生成されることがあります。</p>
<p><img src="images/paste-8.png" class="img-fluid"></p>
</section>
<section id="speech-resynthesis" class="level3">
<h3 class="anchored" data-anchor-id="speech-resynthesis">Speech Resynthesis</h3>
<p>Textless NLPは音声合成もできます。一方、この場合はインプットから内容、音調、そして話者の3つの特徴量を抽出しています。</p>
<p><img src="images/paste-9.png" class="img-fluid"></p>
<p>なぜここで別の特徴量が必要かというと、量子化することにより音声内容が抽出され、逆に話者の情報が落ちてしましました。下図の右の表でその実験結果があります。話者識別の精度について、量子化しないHuBERTだと99%の精度を得られますが、量子化の粒度が粗いほど話者識別の精度が悪くなります。CPCモデルも同じ傾向です。</p>
<p><img src="images/paste-10.png" class="img-fluid"></p>
<p>同じモデルでコナンの蝶ネクタイ型変声機のようなVoice Conversionもできます。Vioce Conversionとは、音声の内容は同じですが、話者がほかの人にすることです。Speaker Embeddingを差し替えるだけでできます。</p>
<p>また、この研究でHuBERTがいいモデルということもわかりましが。</p>
<p><img src="images/paste-11.png" class="img-fluid"></p>
<p>同じフレームワークでSpeech codecもできます。speech codecは、音声信号をデジタルデータに変換し、デジタルデータを音声信号に戻す技術の研究分野です。この分野の目的は、音声データを効率的に圧縮し、伝送やストレージに適した形式にすることです。</p>
<p>このモデルはまず音声を特徴量化(下図の赤枠)にして、転送先にDecoderがあれば似た音声を生成することができます。右の図からその効果がわかります。横軸は圧縮した後の情報量、縦は復元した音声の品質、HuBERTはを使ったモデルはこの図の左上にあります。つまり、少ない情報量で高品質な音声を復元できることが示されています。</p>
<p><img src="images/paste-12.png" class="img-fluid"></p>
</section>
<section id="speech-translation-unit-bart" class="level3">
<h3 class="anchored" data-anchor-id="speech-translation-unit-bart">Speech Translation: Unit BART</h3>
<p>音声翻訳もできます。英語とスペイン語の音声翻訳を例に挙げます。この時真ん中にあるものはBARTというEncoderとDecoderを兼ね備えたTransfomerです。</p>
<p>英語とスペイン語のデータは、ペアではなくても大丈夫です。最初に、英語またはスペイン語の音声をマルチ言語のHuBERTに通してトークンを取得します。続いて、BARTにトークンを入力し、Auto-encoderによる学習を行います。つまり、入力トークンの一部をマスキングして復元することで学習が進みます。</p>
<p>最終的に、英語とスペイン語の両方を適切に復元できるBARTモデルが完成します。</p>
<p><img src="images/paste-13.png" class="img-fluid"></p>
<p>ただし、学習されたBARTモデルは入力言語のみを復元できるため、翻訳を行うにはペアデータを使って学習する必要があります。この論文では、エンコーダーをWav2vec 2.0に置き換えて実験が行われています。元のBARTを使用しても問題ないですが、Wav2vec 2.0の方が性能が良かったため採用されました。</p>
<p><img src="images/paste-14.png" class="img-fluid"></p>
<p>このアプローチにより、Textless NLPはテキストデータがない状況でも、従来のCascaded systemと同等の精度を達成しています。</p>
<p><img src="images/paste-15.png" class="img-fluid"></p>
</section>
<section id="unity" class="level3">
<h3 class="anchored" data-anchor-id="unity">UnitY</h3>
<p>これまで紹介したモデルはテキストデータが必要ではないですが、逆に言えば、テキストデータがあっても使えないです。その問題を解決できるのがUnitYモデルです。</p>
<p>このモデルは4つの部分によって構成されています。音声が入力されると、まずSpeech encoderを通過し、得られた隠れ層をText decoderに入力します。その後、処理が分岐します。</p>
<p>音声とテキストのペアデータが存在する場合、補助タスク(Auxiliary task)としてテキストの予測が可能です。それと同時に、Text decoderから出力された別のベクトルが次のEncoderに送られ、最終的に音声が出力されます。</p>
<p><img src="images/paste-16.png" class="img-fluid"></p>
<p>このモデルは、音声とテキストのペアだけでなく、すべての形式のデータを学習に活用できます（ラベルなしテキスト、ラベルなし音声、ラベル付き音声、ペア音声のすべてのデータ形式）。</p>
<p><img src="images/paste-17.png" class="img-fluid"></p>
<p>UnitYはCascaded systemより精度が良いです。</p>
<p><img src="images/paste-18.png" class="img-fluid"></p>
<p>以前Metaが出した英語と福建語の音声翻訳デモはこのUnitYを使いました。下のビデオをご覧ください。</p>
<iframe src="https://www.facebook.com/plugins/video.php?href=https%3A%2F%2Fwww.facebook.com%2Fzuck%2Fvideos%2F2725795187550922%2F&amp;show_text=0&amp;width=380" width="380" height="476" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowfullscreen="true" allow="autoplay; clipboard-write; encrypted-media; picture-in-picture; web-share">
</iframe>
<p>ビデオでは、音声翻訳時に出力される声が本人の声ではないことがわかります。これは、音声を量子化する際に話者の情報が失われるためです。</p>
<p><img src="images/paste-19.png" class="img-fluid"></p>
</section>
</section>
<section id="audiolm" class="level2">
<h2 class="anchored" data-anchor-id="audiolm">AudioLM</h2>
<p>AudioLMはGoogleが2022年10月出した研究です。<a href="https://google-research.github.io/seanet/audiolm/examples/">ここ</a>で実際に生成したサンプルを見ることができます。</p>
<p>Textless NLPとは異なり、AudioLMは話者の情報を維持しています。音声の意味(Sementic)以外に、音響(Acoustic)も同じような方法で量子化し、モデルに入力しています。</p>
<p>Acoustic Tokenは、Codec Encoderによって抽出され、Codec Decoderから出力されます。 <img src="images/paste-21.png" class="img-fluid"></p>
<p>Codecモデルは、オートエンコーダーの方法で入力された音声を復元するタスクを学習します。</p>
<p>通常の自然言語処理のTokenとは異なり、Acoustic Tokenには複数の層があります。抽出方法は、まず音声をエンコーダーに通し、出力された複数のベクトルに対して、最も近いTokenを探すところから始まります。ここではA3, A2, A6がその結果でした。次に、一層目で得たベクトルとA3, A2, A6の差分を取って、次の層にインプットして同じようなことをします。これによって、最終的にN層のTokenを得られるます。得たTokenをDecoderに渡してDecodeします。</p>
<p>各層の入力は、前の層の出力とTokenのベクトルの差分であり、値が徐々に小さくなります。これによって、必要に応じ層を少なくして、音声の質をあまり落とさないまま、音声の圧縮することもできます。</p>
<p><img src="images/paste-22.png" class="img-fluid"></p>
<p>音声生成において、中間のunit LMは3つのTransformerを連結して構成されます。</p>
<p><img src="images/paste-24.png" class="img-fluid"></p>
<p>音声出力のプロセスは以下の通りです。</p>
<ol type="1">
<li>最初のモデルは過去のSemantic Tokenを入力とし、次に話す内容、つまり未来のSemantic Tokenを予測します。</li>
<li>生成された未来のSemantic Tokenと過去の荒いAcoustic Tokenを組み合わせて、2番目のモデルに入力し、未来の荒いAcoustic Tokenを予測します。</li>
<li>2で生成された荒いAcoustic Tokenを3番目のモデルに入力し、良いAcoustic Tokenを生成します。</li>
<li>2と3の結果を組み合わせてデコーダーに渡し、音声が生成されます。</li>
</ol>
<p><img src="images/paste-23.png" class="img-fluid"></p>
<p>この方法では、音声だけでなく音楽の生成も可能で、以下のビデオでサンプルを聞くことができます。</p>
<iframe width="500" height="300" src="https://www.youtube.com/embed/_xkZwJ0H9IU" title="AudioLM - Google AI Blog post" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="">
</iframe>
</section>
<section id="vall-e" class="level2">
<h2 class="anchored" data-anchor-id="vall-e">VALL-E</h2>
<p>VALL-Eは、2023年1月にMicrosoftが公開したTTS（Text to Speech）モデルです。その特徴は、テキストだけでなく、最大3秒の音声も入力できる点です。この機能により、出力される音声が入力音声と同じ人が話しているかのように聞こえます。実際の例は<a href="https://www.microsoft.com/en-us/research/project/vall-e/">このブログ</a>で見れます。</p>
<p><img src="images/paste-25.png" class="img-fluid"></p>
<p>前も言及したが、Acoustic Tokenの中で一番重要なのは最初の層です。VALL-Eは生成音声の質と生成速度をトレードオフのため、重要な一層目は質が高いがスピードが遅い自己回帰モデル(Autoregressive)を使いました。残りの層だとスピードが早いが、質が良くない非自己回帰モデル(Non-Autoregressive)を使いました。 以前も触れましたが、Acoustic Tokenの中で最も重要なのは最初の層です。VALL-Eは、生成音声の品質と生成速度のトレードオフを考慮し、最初の層には品質が高いものの速度が遅い自己回帰モデル(Autoregressive)を採用しています。その後の層には、速度が速いが品質は劣る非自己回帰モデル(Non-Autoregressive)を使用しています。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
AutoregressiveとNon-Autoregressiveの違いは？
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Autoregressive（自己回帰）とNon-Autoregressive（非自己回帰）は、機械学習や自然言語処理の文脈で使われる用語です。これらは、特にシーケンスデータ（テキスト、音声など）の生成や予測に関連しています。</p>
<p>Autoregressiveモデルは、シーケンスの各要素を直前の要素に依存して生成するモデルです。つまり、出力の予測には、すでに生成された過去の要素が必要です。例えば、自然言語処理の場合、次の単語を予測するために、前の単語の情報を使用します。具体的な例としては、言語モデルの一種である従来のリカレントニューラルネットワーク（RNN）があります。このモデルでは、過去の出力を入力として使用して次の出力を生成します。</p>
<p>一方、Non-Autoregressiveモデルは、各要素の生成が直前の要素に依存せず、同時に独立して生成されるモデルです。これは、逐次的な生成を必要とせず、並列的に処理できる利点があります。具体的な例としては、Transformerモデルがあります。Transformerモデルでは、異なる位置の要素を同時に処理し、出力を生成します。これにより、高速な並列処理が可能となります。</p>
<p>Autoregressiveモデルは、文脈の長さに依存するため、生成に時間がかかる場合があります。一方、Non-Autoregressiveモデルは、並列処理が可能なため、生成速度が速くなる傾向がありますが、生成の品質や文脈の考慮が制限される可能性もあります。</p>
</div>
</div>
</div>
<p><img src="images/paste-26.png" class="img-fluid"></p>
<p>VALL-Eは、最先端のTTSシステムであり、生成された音声と元の音声の話者の類似度が高いことが特徴です。さらに、声の感情や音の環境も入力音声に近いものを出力することができます。</p>
<p><img src="images/paste-27.png" class="img-fluid"></p>
</section>
<section id="part-3-other-speech-foundation-models" class="level2">
<h2 class="anchored" data-anchor-id="part-3-other-speech-foundation-models">Part 3 Other Speech Foundation Models</h2>
<p>Part1とPart2で説明したモデルは主にラベルなしのデータを活用する目的でした。WhisperとUSMでは、そもそも学習データの量がものすごく多いため、モデルの構造上新規性がないですが、良い効果が出ています。</p>
<p><img src="images/paste-28.png" class="img-fluid"></p>
<section id="whisper" class="level3">
<h3 class="anchored" data-anchor-id="whisper">Whisper</h3>
<p>Whisperは、OpenAIが2022年9月にリリースしたSTT（Speech to Text）モデルです。このモデルは、680,000時間の膨大なラベル付きデータを使用して学習されています。モデルの構造は普通のTransfomerのEncoder-Decoder構造です。</p>
<p><img src="images/paste-29.png" class="img-fluid"></p>
<p>Whisperは、マルチタスク学習を行っています。最初に音声が存在するかどうかを判断し、音声がない場合は<code>&lt;EOS&gt;</code>トークンを出力して終了します。音声が存在する場合は、まず音声の言語を判断し、続いてTranscribeタスクを実行してテキストに変換します。翻訳タスクの場合は、目標言語のテキストを出力します。</p>
<p><img src="images/paste-30.png" class="img-fluid"></p>
<p>Whisperの学習データは非常に多く、例えば多言語音声データが12万時間、英語音声データが44万時間、他言語と英語のテキストを含む音声データが12万時間利用されています。</p>
<p><img src="images/paste-31.png" class="img-fluid"></p>
<p>Whisperはオープンソースのモデルであるため、自分で実行することが可能です。また、OpenAIはAPIも提供しています。</p>
<p><img src="images/paste-32.png" class="img-fluid"></p>
</section>
<section id="usm" class="level3">
<h3 class="anchored" data-anchor-id="usm">USM</h3>
<p>USMは、2023年3月にGoogleが開発した音声認識モデルです。1200万の音声データ(さすがGoogle)を用いて事前学習が行われ、その後、Whisperの1/7のラベル付きデータを使ってFine-Tuningが実施されました。</p>
<p><img src="images/paste-33.png" class="img-fluid"></p>
<p>USMモデルは、音声データだけでなく、テキストデータも活用しています。</p>
<p>モデルの構造としては、上流にSpeech EncoderとText Embeddingが設置され、下流にEncoderが存在します。</p>
<p>学習する際に、3つのタスクを学習しました。</p>
<ol type="1">
<li>音声データのみを用いる場合、量子化手法を使って音声を学習する。</li>
<li>テキストのみの場合は、RNN-T decoderを使ってテキストを復元するタスクで学習する</li>
<li>音声とテキストのペアがある場合は、普通のFine-Tuinigをする</li>
</ol>
<p><img src="images/paste-34.png" class="img-fluid"></p>
<p>この方法により、Whisperよりも優れたモデルが構築されました。ただし、USMはオープンソースではなく、GoogleのAPIを通じてのみ利用可能です。</p>
<p><img src="images/paste-35.png" class="img-fluid"></p>
</section>
</section>
<section id="まとめ" class="level2">
<h2 class="anchored" data-anchor-id="まとめ">まとめ</h2>
<p>近年の進歩により、疑似テキストを活用したエンドツーエンドの音声学習が実現し、Textless NLPやAudioLMなどのモデルが音声翻訳や音声合成などのタスクで活躍しています。また、VALL-E、Whisper、USMなどのモデルは、音声認識やテキストから音声への変換などの分野で効果を発揮しています。これらのモデルは、大量のデータを利用して学習され、音声関連タスクの精度向上に寄与しています。</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="AlbertRapp/blogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>