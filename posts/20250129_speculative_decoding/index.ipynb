{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"LLMの推論速度を劇的に加速する方法 Speculative Decoding の解説\"\n",
    "date: 2024-02-23\n",
    "description-meta: \"LLMの推論速度を劇的に加速するSpeculative Decoding技術を解説。Google DeepMindとUC Berkeley共同開発、精度を落とさず推論速度を2倍に。仕組み、従来手法との違い、ビジネス応用、コード例、実験結果、制限事項まで網羅的に解説。NLP、LLM技術者必見の推論高速化の決定版。\"\n",
    "categories: [NLP, LLM]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## はじめに\n",
    "皆さんに質問です。\n",
    "「モデルの精度を落とさず、計算リソースも増やさず、推論速度だけを2倍にする方法」\n",
    "があるとしたら——それは魔法でしょうか？それとも現実の技術でしょうか？\n",
    "\n",
    "答えは後者です。Google DeepMindとUC Berkeleyが共同開発したSpeculative Decodingは、まさにこの不可能を可能にする「推論加速のブラックボックス」。自動車で例えれば、ナビの予測ルート候補を事前計算しつつ、実際の走行で最適経路を選択するような巧妙な手法で、LLMの生成速度に革命を起こします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 「Speculative Decoding」って何？\n",
    "\n",
    "「Speculative Decoding」は日本語で「**推測的デコーディング**」と訳されることが多く、直訳に近い表現として「投機的デコーディング」と呼ばれることもあります。この手法を簡単に言うと、**小さなモデル（ドラフトモデル）で複数トークンをまとめて推測・生成し、それを大きなモデル（ターゲットモデル）で一括検証する**というものです。推測が正しければ、その結果をそのまま利用することで、テキスト生成を高速化できます。\n",
    "\n",
    "もう少し詳しく見ていきましょう。\n",
    "\n",
    "例えば、LLaMa3 70Bを使ってテキストを生成したいとします。LLaMa3 70Bは非常に大きいモデルのため、テキスト生成に時間がかかります。そこで、より小さなモデル、例えばLLaMa3 7Bをドラフトモデルとして利用してSpeculative Decodingを行います。\n",
    "\n",
    "入力として「The quick brown」というテキストを与えてみましょう。これは英語で有名な文「The quick brown fox jumps over the lazy dog」の冒頭部分であり、小さなモデルでも比較的容易に続きを推測できると考えられます。\n",
    "\n",
    "まず、小さなモデルに「The quick brown」を入力し、続くトークンを推測させます。ここでは、一度に推測するトークン数（チャンクサイズ）を2と設定しましょう。\n",
    "\n",
    "すると、小さなモデルは「The quick brown **fox jumps**」と生成しました。\n",
    "\n",
    "次に、この結果を大きなモデルで検証します。小さなモデルが推測したトークンをプロンプトに含めて大きなモデルに入力すると、「**fox jumps over**」と生成しました。小さなモデルが生成した「fox jumps」を検証できたのみではなく、その先の「over」まで予測できています。\n",
    "\n",
    "ここで、処理時間について考えてみましょう。7Bモデルが1トークンを生成するのにかかる時間を $t$ 、70Bモデルが1トークンを生成するのにかかる時間を $T$ とします。\n",
    "\n",
    "小さなモデルは2つのトークンを生成するのに $2t$ の時間がかかります。一方、大きなモデルは検証のために1回だけ推論を行うので、かかる時間は $T$ です。したがって、合計時間は $2t + T$ となります。もしSpeculative Decodingを使わずに70Bモデルだけで3トークンを生成しようとすると、$3T$ の時間がかかります。一般的に、$t$ は $T$ よりもずっと小さいため、Speculative Decodingを利用することで生成時間を大幅に短縮できることがわかります。\n",
    "\n",
    "上記でSpeculative Decodingの概要を説明しましたが、まだ疑問点が残るかもしれません。\n",
    "\n",
    "- なぜ70Bモデルで複数のトークンを検証するのに、検証時間が$1T$だけで済むのか？\n",
    "- なぜ2つのトークンを検証する際に、次のトークンも得られるのか？\n",
    "- 小さいモデルの推測が間違っていた場合はどうなるのか？\n",
    "- なぜ毎回2トークン推測するのか？もっと多くのトークンを一度に推測すれば、さらに高速化できるのではないか？\n",
    "\n",
    "これらの疑問について、次の章で詳しく解説していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMの生成プロセスの説明\n",
    "\n",
    "これからコードで「Speculative Decoding」を再現するにあたり、その前に、LLMがどのようにトークンを生成しているかを前置きとして説明します。\n",
    "\n",
    "LLMはトークンを生成する際、一つずつ順番に生成します。一つのトークンを生成するプロセスは以下の通りです。\n",
    "\n",
    "<img src=\"image-2.png\" alt=\"alt text\" height=\"400\"/>\n",
    "\n",
    "1. **「The quick brown」** の3つのトークンをモデルに入力します。\n",
    "2. モデルは入力トークンに対応する**logits**を出力します。この例では3つのトークンを入力したため、3つのlogitsが出力されます。\n",
    "3. **最後のトークンに対応するlogitsのみ** を用いて、Softmax関数を適用し、次のトークンの**確率分布**を得ます。\n",
    "4. その確率分布から、次のトークンを**サンプリング**します。ここでは「fox」がサンプリングされました。\n",
    "5. 次のステップでは、元の入力にトークン「fox」を加えた **「The quick brown fox」** を新たな入力とします。\n",
    "\n",
    "このプロセスを、必要なトークン数になるまで繰り返すことで、文章を生成することができます。\n",
    "\n",
    "ここで重要なポイントは、**LLMが出力するlogitsの数が入力トークン数と同じである**という点です。加えて、**各入力トークンに対応するlogitsは、そのトークンの次のトークンの確率分布を予測するために利用されます。** この特性を利用することで、小さいモデルが提案したトークン列の妥当性を、大きいモデルを使って効率的に検証することができます。\n",
    "\n",
    "<img src=\"image-3.png\" alt=\"alt text\" height=\"400\"/>\n",
    "\n",
    "例えば、上の図のように「The quick brown」の3つのトークンを入力として、小さいモデルが「fox jumps」という2つのトークンを提案したとします。この場合、大きいモデルは「The quick brown」を入力とし、3つのlogitsを出力します。これらのlogitsを用いることで、「fox」が「The quick brown」の、そして「jumps」が「The quick brown fox」の次のトークンとして適切かどうかを、一度のフォワードパスで検証できます。さらに、「fox jumps」が正しいと判断された場合、大きいモデルは「jumps」の次のトークンのlogitsも出力していることから、次のトークンの予測も同時に得られます。\n",
    "\n",
    "一方、提案が正しくなかった場合、大きいモデルは最初に間違ったトークンを特定し、正しいトークンに修正できます。次のステップでは、修正されたトークンまでを入力として使用します。\n",
    "\n",
    "<img src=\"image-4.png\" alt=\"alt text\" height=\"400\"/>\n",
    "\n",
    "例えば、上の図のように小さいモデルの提案が「fox run」で、大きいモデルの出力が「fox jumps over」である場合、「fox」までは正しいが「run」が間違っていると判断できます。この場合、「fox」の次のトークンとして「jumps」を採択します。次のステップでは **「The quick brown fox jumps」** を入力として、再び生成プロセスを続行します。\n",
    "\n",
    "仕組みを理解したうえで上の4つの質問を回答することができます。\n",
    "\n",
    "- なぜ70Bのモデルで検証する際にかかる時間が$1T$のみか？\n",
    "    - 回答：一度のフォワードパスで、入力されたすべてのトークンを並列で検証できるためです。\n",
    "- なぜ新しく生成した2個のトークンを検証するのに、更にその次のトークンも得られるか？\n",
    "    - 回答：第$t$トークンのlogitsを用いて第$t+1$トークンの確率分布を予測するためです。提案が正しい場合、次のトークンも同時に得られます。\n",
    "- 小さいモデルの提案が正しくない場合はどうするのか？\n",
    "    - 回答：提案が誤っている場合は、大きいモデルが最初に誤ったトークンを特定し、正しいトークンに修正します。次のステップでは、修正されたトークンまでを入力として使用します\n",
    "- なぜ毎回2個のみ推測するのか？もっと多くのトークンを一度に推測すればもっと速くなるのでは？\n",
    "    - 回答：一度に推測するトークン数を増やすと、提案が誤る可能性も高くなるためです。例えば、一度に100トークン提案し、2個目が誤っていた場合、98個のトークンが無駄になってしまいます。Transformersの実装案では、最初に3個とし、誤りの場合は1個減少し、正解の場合は2個増加させるという方法を採用しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コードでの再現\n",
    "\n",
    "これからは上記のことをコードで再現します。今所持しているPCのGPUはRTX4070で、メモリは12GBのみなので、今回はLLaMa3ではなく、4Bitで量子化したQwen2.5の0.5Bと3Bを利用します。\n",
    "\n",
    "まずモデルをローディングし、インプットデータを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Model Loading and Setup\n",
    "small_model_name = \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\"\n",
    "big_model_name = \"Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\"\n",
    "\n",
    "# the tokenizer is the same for both models\n",
    "tokenizer = AutoTokenizer.from_pretrained(small_model_name)\n",
    "\n",
    "small_model = AutoModelForCausalLM.from_pretrained(\n",
    "    small_model_name, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "big_model = AutoModelForCausalLM.from_pretrained(\n",
    "    big_model_name, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [785, 3974, 13876]\n",
      "Input tokens: The quick brown\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The quick brown\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(small_model.device)\n",
    "print(\"Input IDs:\", input_ids[0].tolist())\n",
    "print(\"Input tokens:\", tokenizer.decode(input_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つぎに、小さいモデルで2個のトークンを生成します。結果として、予想通りに「fox jumps」が新しく生成されました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate IDs:               [785, 3974, 13876, 38835, 34208]\n",
      "Candidate new IDs:                             [38835, 34208]\n",
      "Candidate tokens:                   The quick brown fox jumps\n",
      "Candidate new tokens:                               fox jumps\n"
     ]
    }
   ],
   "source": [
    "candidate_length = 2\n",
    "candidate_ids = small_model.generate(input_ids, max_new_tokens=candidate_length)\n",
    "candidate_new_ids = candidate_ids[:, input_ids.shape[1] :]  # remove the prompt\n",
    "\n",
    "\n",
    "def formatted_print(var_name, var):\n",
    "    length_str = len(var_name)\n",
    "    print(f\"{var_name}:{str(var):>{60 - length_str}}\")\n",
    "\n",
    "\n",
    "formatted_print(\"Candidate IDs\", candidate_ids[0].tolist())\n",
    "formatted_print(\"Candidate new IDs\", candidate_new_ids[0].tolist())\n",
    "formatted_print(\"Candidate tokens\", tokenizer.decode(candidate_ids[0]))\n",
    "formatted_print(\"Candidate new tokens\", tokenizer.decode(candidate_new_ids[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、生成されたトークンを大きなモデルで検証します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_ids:              [[2701, 13876, 38835, 34208, 916]]\n",
      "verified_tokens:               following brown fox jumps over\n"
     ]
    }
   ],
   "source": [
    "big_model_logits = big_model(candidate_ids).logits\n",
    "big_model_ids = big_model_logits.argmax(dim=-1)  # validation result\n",
    "\n",
    "formatted_print(\"verified_ids\", big_model_ids.tolist())\n",
    "formatted_print(\"verified_tokens\", tokenizer.decode(big_model_ids[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得た「following brown fox jumps over」の由来は以下の図でわかります。  \n",
    "もしインプットが「The」の場合は、7Bのモデルによると次のトークンが「following」である確率が最も高いです。  \n",
    "もしインプットが「The quick」の場合は、7Bのモデルによると次のトークンが「brown」である確率が最も高いです。  \n",
    "...\n",
    "\n",
    "<img src=\"image-5.png\" alt=\"alt text\" height=\"400\"/>\n",
    "\n",
    "\n",
    "最初から一個の予測が間違っていますが、でもこれは大丈夫です。なぜかというと、検証の対象は新しく生成された「fox jumps」だけのためです。  \n",
    "次のセルに「fox jumps over」がでましたが、「over」はボーナストークンです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_ids:                             [38835, 34208, 916]\n",
      "varified_tokens:                               fox jumps over\n"
     ]
    }
   ],
   "source": [
    "verified_ids = big_model_ids[:, -(candidate_length + 1) :]\n",
    "formatted_print(\"verified_ids\", verified_ids[0].tolist())\n",
    "formatted_print(\"varified_tokens\", tokenizer.decode(verified_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5Bモデルが提案した結果を7Bモデルの検証結果と比較し、全部合っていることがわかりました。次に「fox jump」プラス最後のボーナストークン「over」をインプットに結合して次の生成に回すことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_matches:                                                  2\n",
      "valid_ids:                                [38835, 34208, 916]\n",
      "valid_tokens:                                  fox jumps over\n",
      "next_input_ids:         [785, 3974, 13876, 38835, 34208, 916]\n",
      "next_input_tokens:             The quick brown fox jumps over\n"
     ]
    }
   ],
   "source": [
    "# validation result\n",
    "n_matches = (\n",
    "    (~(candidate_new_ids == verified_ids[:, :-1])).cumsum(dim=-1) < 1\n",
    ").sum()  # fancy way to count the number of matches\n",
    "valid_ids = verified_ids[:, : n_matches + 1]\n",
    "next_input_ids = torch.cat((input_ids, valid_ids), dim=-1)\n",
    "\n",
    "\n",
    "formatted_print(\"n_matches\", n_matches.item())\n",
    "formatted_print(\"valid_ids\", valid_ids[0].tolist())\n",
    "formatted_print(\"valid_tokens\", tokenizer.decode(valid_ids[0]))\n",
    "formatted_print(\"next_input_ids\", next_input_ids[0].tolist())\n",
    "formatted_print(\"next_input_tokens\", tokenizer.decode(next_input_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、Speculative Decodingの一個の循環が完了しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験\n",
    "コードの分解もしたので、次に実際に実験してみましょう。今回はコード生成の結果を比較してみます。まず、上記のコードを関数として整理します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def speculative_decoding(\n",
    "    big_model,\n",
    "    small_model,\n",
    "    input_ids,\n",
    "    max_length=1000,\n",
    "    candidate_length=3,\n",
    "    tokenizer=None,\n",
    "):\n",
    "    generated_ids = input_ids.clone()\n",
    "    total_generated = 0\n",
    "    accepted = 0\n",
    "    generated_ids_list = [(\"prompt\", tokenizer.decode(input_ids[0]))]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while generated_ids.shape[-1] < max_length:\n",
    "            # 1. Candidate Generation (Small Model)\n",
    "            candidate_input_ids = small_model.generate(\n",
    "                generated_ids, max_new_tokens=candidate_length, do_sample=False\n",
    "            )\n",
    "\n",
    "            # 2. Big Model Filtering\n",
    "            new_logits = big_model(candidate_input_ids).logits[\n",
    "                :, -(candidate_length + 1) :\n",
    "            ]  # +1 because we have a bonus token\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, generated_ids.shape[1] :]\n",
    "\n",
    "            # Determine the actual number of generated tokens\n",
    "            num_generated_tokens = candidate_new_tokens.shape[1]\n",
    "\n",
    "            # Compare only the relevant portion of selected_tokens\n",
    "            n_matches = (\n",
    "                (\n",
    "                    ~(candidate_new_tokens == selected_tokens[:, :num_generated_tokens])\n",
    "                ).cumsum(dim=-1)\n",
    "                < 1\n",
    "            ).sum()\n",
    "\n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            generated_ids_list.append(\n",
    "                (\"accepted\", tokenizer.decode(valid_tokens[0, :n_matches]))\n",
    "            )\n",
    "            generated_ids_list.append(\n",
    "                (\"generated\", tokenizer.decode(valid_tokens[0, -1:]))\n",
    "            )\n",
    "            # 3. Update Generated Sequence\n",
    "            generated_ids = torch.cat((generated_ids, valid_tokens), dim=-1)\n",
    "            total_generated += candidate_length\n",
    "            accepted += n_matches\n",
    "\n",
    "            if valid_tokens.shape[1] == 0 or (\n",
    "                valid_tokens.shape[1] > 0 and tokenizer.pad_token_id in valid_tokens\n",
    "            ):\n",
    "                # delete tokens from padding\n",
    "                idx_pad = (generated_ids == tokenizer.pad_token_id).nonzero()\n",
    "                if idx_pad.numel() > 0:\n",
    "                    generated_ids = generated_ids[:, : idx_pad[0, 1] + 1]\n",
    "                break\n",
    "\n",
    "    return generated_ids, total_generated, accepted, generated_ids_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、Speculative Decodingを使わない場合どうなるかを確認します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 13.21s\n",
      "Generated code:\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def below_zero(operations: List[int]) -> bool:\n",
      "    \"\"\"You're given a list of deposit and withdrawal operations on a bank account that starts with zero balance. Your task is to detect if at any point the balance of account fallls below zero, and at that point function should return True. Otherwise it should return False. >>> below_zero([1, 2, 3]) False >>> below_zero([1, 2, -4, 5]) True\"\"\"\n",
      "    balance = 0\n",
      "    for operation in operations:\n",
      "        balance += operation\n",
      "        if balance < 0:\n",
      "            return True\n",
      "    return False\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    print(below_zero([1, 2, 3]))\n",
      "    print(below_zero([1, 2, -4, 5]))<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = '''from typing import List\n",
    "\n",
    "\n",
    "def below_zero(operations: List[int]) -> bool:\n",
    "    \"\"\"You're given a list of deposit and withdrawal operations on a bank account that starts with zero balance. Your task is to detect if at any point the balance of account fallls below zero, and at that point function should return True. Otherwise it should return False. >>> below_zero([1, 2, 3]) False >>> below_zero([1, 2, -4, 5]) True\"\"\"\n",
    "'''\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(big_model.device)\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    big_model_generated_ids = big_model.generate(\n",
    "        input_ids.clone(), max_length=1000, do_sample=False\n",
    "    )\n",
    "cost = time.time() - start\n",
    "\n",
    "print(\"Time cost:\", f\"{cost:.2f}s\")\n",
    "print(\"Generated code:\")\n",
    "print(tokenizer.decode(big_model_generated_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、Speculative Decodingを使ってみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total generated tokens: 63\n",
      "Accepted tokens: tensor(51, device='cuda:0')\n",
      "Acceptance rate: tensor(0.8095, device='cuda:0')\n",
      "Time cost: 8.47s\n",
      "Same result generated by big model: True\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "generated_ids, total_generated, accepted, generated_ids_list = speculative_decoding(\n",
    "    big_model,\n",
    "    small_model,\n",
    "    input_ids,\n",
    "    max_length=1000,\n",
    "    candidate_length=3,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "cost = time.time() - start\n",
    "\n",
    "print(\"Total generated tokens:\", total_generated)\n",
    "print(\"Accepted tokens:\", accepted)\n",
    "print(\"Acceptance rate:\", accepted / total_generated)\n",
    "print(\"Time cost:\", f\"{cost:.2f}s\")\n",
    "print(\n",
    "    \"Same result generated by big model:\",\n",
    "    (big_model_generated_ids == generated_ids).all().item(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際にテストしてみると、「Speculative Decoding」を使ってコードを生成する際に、0.5Bモデルが提案したトークンのうち**77%が正しく**、その結果、生成速度が13/8=1.6倍まで**高速化されました**。\n",
    "\n",
    "一般的に、コード生成やコード修正などのタスクは、出力のランダム性が低いため、**高いAcceptance Rate（受理率）**が期待できます。そのため、Speculative Decodingは特にこれらのタスクに適しています。 OpenAIの[predicted output](https://platform.openai.com/docs/guides/predicted-outputs)機能も、この手法を**利用していると考えられます**。\n",
    "\n",
    "実験の最後に、**おまけとして**、生成されたコードのどの部分が0.5Bモデルの提案によるもので、どの部分が7Bモデルの検証結果によるものかを可視化してみましょう。\n",
    "**緑色の部分が0.5Bモデルの提案、オレンジ色の部分が7Bモデルの検証結果です。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre><span style='background-color: transparent;'>from typing import List\n",
       "\n",
       "\n",
       "def below_zero(operations: List[int]) -> bool:\n",
       "    \"\"\"You're given a list of deposit and withdrawal operations on a bank account that starts with zero balance. Your task is to detect if at any point the balance of account fallls below zero, and at that point function should return True. Otherwise it should return False. >>> below_zero([1, 2, 3]) False >>> below_zero([1, 2, -4, 5]) True\"\"\"\n",
       "</span><span style='background-color: lightgreen;'>   </span><span style='background-color: orange;'> balance</span><span style='background-color: lightgreen;'> = 0</span><span style='background-color: orange;'>\n",
       "</span><span style='background-color: lightgreen;'>    for operation</span><span style='background-color: orange;'> in</span><span style='background-color: lightgreen;'> operations:\n",
       "       </span><span style='background-color: orange;'> balance</span><span style='background-color: lightgreen;'> += operation\n",
       "</span><span style='background-color: orange;'>       </span><span style='background-color: lightgreen;'> if balance <</span><span style='background-color: orange;'> </span><span style='background-color: lightgreen;'>0:\n",
       "           </span><span style='background-color: orange;'> return</span><span style='background-color: lightgreen;'> True\n",
       "   </span><span style='background-color: orange;'> return</span><span style='background-color: lightgreen;'> False</span><span style='background-color: orange;'>\n",
       "\n",
       "\n",
       "</span><span style='background-color: lightgreen;'>if __name</span><span style='background-color: orange;'>__</span><span style='background-color: lightgreen;'> ==</span><span style='background-color: orange;'> \"__</span><span style='background-color: lightgreen;'>main__\":\n",
       "   </span><span style='background-color: orange;'> print</span><span style='background-color: lightgreen;'></span><span style='background-color: orange;'>(b</span><span style='background-color: lightgreen;'>elow_zero([</span><span style='background-color: orange;'>1</span><span style='background-color: lightgreen;'>, 2</span><span style='background-color: orange;'>,</span><span style='background-color: lightgreen;'> 3</span><span style='background-color: orange;'>]))\n",
       "</span><span style='background-color: lightgreen;'>   </span><span style='background-color: orange;'> print</span><span style='background-color: lightgreen;'>(below_zero</span><span style='background-color: orange;'>([</span><span style='background-color: lightgreen;'>1, </span><span style='background-color: orange;'>2</span><span style='background-color: lightgreen;'>, -4</span><span style='background-color: orange;'>,</span><span style='background-color: lightgreen;'> 5]))</span><span style='background-color: orange;'><|endoftext|></span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | code-fold: true\n",
    "# | code-summary: \"Click here to show the visualization code\"\n",
    "\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "html_output = \"<pre>\"  # Wrap the entire output in <pre> tags\n",
    "\n",
    "for type, text in generated_ids_list:\n",
    "    text = text.replace(\" \", \" \")  # Replace spaces with\n",
    "    if type == \"prompt\":\n",
    "        html_output += f\"<span style='background-color: transparent;'>{text}</span>\"\n",
    "    elif type == \"accepted\":\n",
    "        html_output += f\"<span style='background-color: lightgreen;'>{text}</span>\"\n",
    "    elif type == \"generated\":\n",
    "        html_output += f\"<span style='background-color: orange;'>{text}</span>\"\n",
    "\n",
    "html_output += \"</pre>\"  # Close the <pre> tag\n",
    "\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speculative Decodingを使う際の制限\n",
    "\n",
    "Speculative Decodingは生成速度を向上させるための強力な手法ですが、いくつかの制限があります。\n",
    "まず、提案を検証するためには、小さいモデルと大きいモデルのTokenizerが一緒でないといけません。この点については、Huggingfaceのほうで[Universal assisted generation](https://huggingface.co/blog/universal_assisted_generation)を提案しました。つまり、提案したトークンをテキストに変換した後、また大きいモデルのTokenizerにトークンを変換することで、この問題を解決できます。\n",
    "次に、Speculative Decodingが役に立つ前提としては、計算する際にメモリのスピードがボトルネックになることが必要です。言い換えると、バッチサイズを上げる場合は、メモリスピードより計算スピードがボトルネックになるため、あまりこの手法は効果がありません。\n",
    "\n",
    "\n",
    "## まとめ\n",
    "\n",
    "本文では、Speculative Decodingの仕組みと実装方法について解説しました。この手法は、小さいモデルで提案したトークンを大きいモデルで検証することで、生成速度を向上させることができます。また、実験結果からも、この手法が生成タスクにおいて有効であることがわかりました。最後に、Speculative Decodingを使う際の制限についても触れました。\n",
    "\n",
    "\n",
    "\n",
    "## 参考文献\n",
    "1. [Assisted Generation: a new direction toward low-latency text generation](https://huggingface.co/blog/assisted-generation)\n",
    "2. [Transformers `generate` code](https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src/transformers/generation/utils.py#L1880)\n",
    "3. [Karpathy's X post on speculative decoding](https://x.com/karpathy/status/1697318534555336961)\n",
    "4. [Philkrav's blog about speculative decoding](https://philkrav.com/posts/speculative/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
