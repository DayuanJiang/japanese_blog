<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.340">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-01-29">
<meta name="description" content="LLMの推論速度を劇的に加速するSpeculative Decoding技術を解説。Google DeepMindとUC Berkeley共同開発、精度を落とさず推論速度を2倍に。仕組み、従来手法との違い、ビジネス応用、コード例、実験結果、制限事項まで網羅的に解説。NLP、LLM技術者必見の推論高速化の決定版。">

<title>blog - LLMの推論速度を劇的に加速する方法 Speculative Decoding の解説</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-56TE34D1Z4"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-56TE34D1Z4', { 'anonymize_ip': true});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="http://www.linkedin.com/in/jiang-dayuan" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#はじめに" id="toc-はじめに" class="nav-link active" data-scroll-target="#はじめに">はじめに</a></li>
  <li><a href="#speculative-decodingって何" id="toc-speculative-decodingって何" class="nav-link" data-scroll-target="#speculative-decodingって何">「Speculative Decoding」って何？</a></li>
  <li><a href="#llmの生成プロセスの説明" id="toc-llmの生成プロセスの説明" class="nav-link" data-scroll-target="#llmの生成プロセスの説明">LLMの生成プロセスの説明</a></li>
  <li><a href="#コードでの再現" id="toc-コードでの再現" class="nav-link" data-scroll-target="#コードでの再現">コードでの再現</a></li>
  <li><a href="#実験" id="toc-実験" class="nav-link" data-scroll-target="#実験">実験</a></li>
  <li><a href="#speculative-decodingを使う際の制限" id="toc-speculative-decodingを使う際の制限" class="nav-link" data-scroll-target="#speculative-decodingを使う際の制限">Speculative Decodingを使う際の制限</a></li>
  <li><a href="#まとめ" id="toc-まとめ" class="nav-link" data-scroll-target="#まとめ">まとめ</a></li>
  <li><a href="#参考文献" id="toc-参考文献" class="nav-link" data-scroll-target="#参考文献">参考文献</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">LLMの推論速度を劇的に加速する方法 Speculative Decoding の解説</h1>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 29, 2025</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="はじめに" class="level2">
<h2 class="anchored" data-anchor-id="はじめに">はじめに</h2>
<p>皆さんに質問です。 「モデルの精度を落とさず、計算リソースも増やさず、推論速度だけを2倍にする方法」 があるとしたら——それは魔法でしょうか？それとも現実の技術でしょうか？</p>
<p>答えは後者です。Google DeepMindとUC Berkeleyが共同開発したSpeculative Decodingは、まさにこの不可能を可能にする「推論加速のブラックボックス」。自動車で例えれば、ナビの予測ルート候補を事前計算しつつ、実際の走行で最適経路を選択するような巧妙な手法で、LLMの生成速度に革命を起こします。</p>
</section>
<section id="speculative-decodingって何" class="level2">
<h2 class="anchored" data-anchor-id="speculative-decodingって何">「Speculative Decoding」って何？</h2>
<p>「Speculative Decoding」は日本語で「<strong>推測的デコーディング</strong>」と訳されることが多く、直訳に近い表現として「投機的デコーディング」と呼ばれることもあります。この手法を簡単に言うと、<strong>小さなモデル（ドラフトモデル）で複数トークンをまとめて推測・生成し、それを大きなモデル（ターゲットモデル）で一括検証する</strong>というものです。推測が正しければ、その結果をそのまま利用することで、テキスト生成を高速化できます。</p>
<p>もう少し詳しく見ていきましょう。</p>
<p>例えば、LLaMa3 70Bを使ってテキストを生成したいとします。LLaMa3 70Bは非常に大きいモデルのため、テキスト生成に時間がかかります。そこで、より小さなモデル、例えばLLaMa3 7Bをドラフトモデルとして利用してSpeculative Decodingを行います。</p>
<p>入力として「The quick brown」というテキストを与えてみましょう。これは英語で有名な文「The quick brown fox jumps over the lazy dog」の冒頭部分であり、小さなモデルでも比較的容易に続きを推測できると考えられます。</p>
<p>まず、小さなモデルに「The quick brown」を入力し、続くトークンを推測させます。ここでは、一度に推測するトークン数（チャンクサイズ）を2と設定しましょう。</p>
<p>すると、小さなモデルは「The quick brown <strong>fox jumps</strong>」と生成しました。</p>
<p>次に、この結果を大きなモデルで検証します。小さなモデルが推測したトークンをプロンプトに含めて大きなモデルに入力すると、「<strong>fox jumps over</strong>」と生成しました。小さなモデルが生成した「fox jumps」を検証できたのみではなく、その先の「over」まで予測できています。</p>
<p>ここで、処理時間について考えてみましょう。7Bモデルが1トークンを生成するのにかかる時間を <span class="math inline">\(t\)</span> 、70Bモデルが1トークンを生成するのにかかる時間を <span class="math inline">\(T\)</span> とします。</p>
<p>小さなモデルは2つのトークンを生成するのに <span class="math inline">\(2t\)</span> の時間がかかります。一方、大きなモデルは検証のために1回だけ推論を行うので、かかる時間は <span class="math inline">\(T\)</span> です。したがって、合計時間は <span class="math inline">\(2t + T\)</span> となります。もしSpeculative Decodingを使わずに70Bモデルだけで3トークンを生成しようとすると、<span class="math inline">\(3T\)</span> の時間がかかります。一般的に、<span class="math inline">\(t\)</span> は <span class="math inline">\(T\)</span> よりもずっと小さいため、Speculative Decodingを利用することで生成時間を大幅に短縮できることがわかります。</p>
<p>上記でSpeculative Decodingの概要を説明しましたが、まだ疑問点が残るかもしれません。</p>
<ul>
<li>なぜ70Bモデルで複数のトークンを検証するのに、検証時間が<span class="math inline">\(1T\)</span>だけで済むのか？</li>
<li>なぜ2つのトークンを検証する際に、次のトークンも得られるのか？</li>
<li>小さいモデルの推測が間違っていた場合はどうなるのか？</li>
<li>なぜ毎回2トークン推測するのか？もっと多くのトークンを一度に推測すれば、さらに高速化できるのではないか？</li>
</ul>
<p>これらの疑問について、次の章で詳しく解説していきます。</p>
</section>
<section id="llmの生成プロセスの説明" class="level2">
<h2 class="anchored" data-anchor-id="llmの生成プロセスの説明">LLMの生成プロセスの説明</h2>
<p>これからコードで「Speculative Decoding」を再現するにあたり、その前に、LLMがどのようにトークンを生成しているかを前置きとして説明します。</p>
<p>LLMはトークンを生成する際、一つずつ順番に生成します。一つのトークンを生成するプロセスは以下の通りです。</p>
<p><img src="image-2.png" alt="alt text" height="400"></p>
<ol type="1">
<li><strong>「The quick brown」</strong> の3つのトークンをモデルに入力します。</li>
<li>モデルは入力トークンに対応する<strong>logits</strong>を出力します。この例では3つのトークンを入力したため、3つのlogitsが出力されます。</li>
<li><strong>最後のトークンに対応するlogitsのみ</strong> を用いて、Softmax関数を適用し、次のトークンの<strong>確率分布</strong>を得ます。</li>
<li>その確率分布から、次のトークンを<strong>サンプリング</strong>します。ここでは「fox」がサンプリングされました。</li>
<li>次のステップでは、元の入力にトークン「fox」を加えた <strong>「The quick brown fox」</strong> を新たな入力とします。</li>
</ol>
<p>このプロセスを、必要なトークン数になるまで繰り返すことで、文章を生成することができます。</p>
<p>ここで重要なポイントは、<strong>LLMが出力するlogitsの数が入力トークン数と同じである</strong>という点です。加えて、<strong>各入力トークンに対応するlogitsは、そのトークンの次のトークンの確率分布を予測するために利用されます。</strong> この特性を利用することで、小さいモデルが提案したトークン列の妥当性を、大きいモデルを使って効率的に検証することができます。</p>
<p><img src="image-3.png" alt="alt text" height="400"></p>
<p>例えば、上の図のように「The quick brown」の3つのトークンを入力として、小さいモデルが「fox jumps」という2つのトークンを提案したとします。この場合、大きいモデルは「The quick brown」を入力とし、3つのlogitsを出力します。これらのlogitsを用いることで、「fox」が「The quick brown」の、そして「jumps」が「The quick brown fox」の次のトークンとして適切かどうかを、一度のフォワードパスで検証できます。さらに、「fox jumps」が正しいと判断された場合、大きいモデルは「jumps」の次のトークンのlogitsも出力していることから、次のトークンの予測も同時に得られます。</p>
<p>一方、提案が正しくなかった場合、大きいモデルは最初に間違ったトークンを特定し、正しいトークンに修正できます。次のステップでは、修正されたトークンまでを入力として使用します。</p>
<p><img src="image-4.png" alt="alt text" height="400"></p>
<p>例えば、上の図のように小さいモデルの提案が「fox run」で、大きいモデルの出力が「fox jumps over」である場合、「fox」までは正しいが「run」が間違っていると判断できます。この場合、「fox」の次のトークンとして「jumps」を採択します。次のステップでは <strong>「The quick brown fox jumps」</strong> を入力として、再び生成プロセスを続行します。</p>
<p>仕組みを理解したうえで上の4つの質問を回答することができます。</p>
<ul>
<li>なぜ70Bのモデルで検証する際にかかる時間が<span class="math inline">\(1T\)</span>のみか？
<ul>
<li>回答：一度のフォワードパスで、入力されたすべてのトークンを並列で検証できるためです。</li>
</ul></li>
<li>なぜ新しく生成した2個のトークンを検証するのに、更にその次のトークンも得られるか？
<ul>
<li>回答：第<span class="math inline">\(t\)</span>トークンのlogitsを用いて第<span class="math inline">\(t+1\)</span>トークンの確率分布を予測するためです。提案が正しい場合、次のトークンも同時に得られます。</li>
</ul></li>
<li>小さいモデルの提案が正しくない場合はどうするのか？
<ul>
<li>回答：提案が誤っている場合は、大きいモデルが最初に誤ったトークンを特定し、正しいトークンに修正します。次のステップでは、修正されたトークンまでを入力として使用します</li>
</ul></li>
<li>なぜ毎回2個のみ推測するのか？もっと多くのトークンを一度に推測すればもっと速くなるのでは？
<ul>
<li>回答：一度に推測するトークン数を増やすと、提案が誤る可能性も高くなるためです。例えば、一度に100トークン提案し、2個目が誤っていた場合、98個のトークンが無駄になってしまいます。Transformersの実装案では、最初に3個とし、誤りの場合は1個減少し、正解の場合は2個増加させるという方法を採用しています。</li>
</ul></li>
</ul>
</section>
<section id="コードでの再現" class="level2">
<h2 class="anchored" data-anchor-id="コードでの再現">コードでの再現</h2>
<p>これからは上記のことをコードで再現します。今所持しているPCのGPUはRTX4070で、メモリは12GBのみなので、今回はLLaMa3ではなく、4Bitで量子化したQwen2.5の0.5Bと3Bを利用します。</p>
<p>まずモデルをローディングし、インプットデータを準備します。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> time</span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co"># Model Loading and Setup</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>small_model_name <span class="op">=</span> <span class="st">'Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4'</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>big_model_name <span class="op">=</span> <span class="st">'Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4'</span></span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co"># the tokenizer is the same for both models</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(small_model_name)</span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a>small_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb1-13"><a href="#cb1-13"></a>    small_model_name, torch_dtype<span class="op">=</span><span class="st">'auto'</span>, device_map<span class="op">=</span><span class="st">'auto'</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>).<span class="bu">eval</span>()</span>
<span id="cb1-15"><a href="#cb1-15"></a></span>
<span id="cb1-16"><a href="#cb1-16"></a>big_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb1-17"><a href="#cb1-17"></a>    big_model_name, torch_dtype<span class="op">=</span><span class="st">'auto'</span>, device_map<span class="op">=</span><span class="st">'auto'</span></span>
<span id="cb1-18"><a href="#cb1-18"></a>).<span class="bu">eval</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>prompt <span class="op">=</span> <span class="st">'The quick brown'</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>input_ids <span class="op">=</span> tokenizer.encode(prompt, return_tensors<span class="op">=</span><span class="st">'pt'</span>).to(small_model.device)</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="bu">print</span>(<span class="st">'Input IDs:'</span>, input_ids[<span class="dv">0</span>].tolist())</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="bu">print</span>(<span class="st">'Input tokens:'</span>, tokenizer.decode(input_ids[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input IDs: [785, 3974, 13876]
Input tokens: The quick brown</code></pre>
</div>
</div>
<p>つぎに、小さいモデルで2個のトークンを生成します。結果として、予想通りに「fox jumps」が新しく生成されました。</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>candidate_length <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>candidate_ids <span class="op">=</span> small_model.generate(input_ids, max_new_tokens<span class="op">=</span>candidate_length)</span>
<span id="cb4-3"><a href="#cb4-3"></a>candidate_new_ids <span class="op">=</span> candidate_ids[:, input_ids.shape[<span class="dv">1</span>] :]  <span class="co"># remove the prompt</span></span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="kw">def</span> formatted_print(var_name, var):</span>
<span id="cb4-7"><a href="#cb4-7"></a>    length_str <span class="op">=</span> <span class="bu">len</span>(var_name)</span>
<span id="cb4-8"><a href="#cb4-8"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>var_name<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span><span class="bu">str</span>(var)<span class="sc">:</span><span class="op">&gt;</span>{<span class="dv">60</span> <span class="op">-</span> length_str}<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-9"><a href="#cb4-9"></a></span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a>formatted_print(<span class="st">'Candidate IDs'</span>, candidate_ids[<span class="dv">0</span>].tolist())</span>
<span id="cb4-12"><a href="#cb4-12"></a>formatted_print(<span class="st">'Candidate new IDs'</span>, candidate_new_ids[<span class="dv">0</span>].tolist())</span>
<span id="cb4-13"><a href="#cb4-13"></a>formatted_print(<span class="st">'Candidate tokens'</span>, tokenizer.decode(candidate_ids[<span class="dv">0</span>]))</span>
<span id="cb4-14"><a href="#cb4-14"></a>formatted_print(<span class="st">'Candidate new tokens'</span>, tokenizer.decode(candidate_new_ids[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Candidate IDs:               [785, 3974, 13876, 38835, 34208]
Candidate new IDs:                             [38835, 34208]
Candidate tokens:                   The quick brown fox jumps
Candidate new tokens:                               fox jumps</code></pre>
</div>
</div>
<p>次に、生成されたトークンを大きなモデルで検証します。</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>big_model_logits <span class="op">=</span> big_model(candidate_ids).logits</span>
<span id="cb6-2"><a href="#cb6-2"></a>big_model_ids <span class="op">=</span> big_model_logits.argmax(dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># validation result</span></span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a>formatted_print(<span class="st">'verified_ids'</span>, big_model_ids.tolist())</span>
<span id="cb6-5"><a href="#cb6-5"></a>formatted_print(<span class="st">'verified_tokens'</span>, tokenizer.decode(big_model_ids[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>verified_ids:              [[2701, 13876, 38835, 34208, 916]]
verified_tokens:               following brown fox jumps over</code></pre>
</div>
</div>
<p>得た「following brown fox jumps over」の由来は以下の図でわかります。<br>
もしインプットが「The」の場合は、7Bのモデルによると次のトークンが「following」である確率が最も高いです。<br>
もしインプットが「The quick」の場合は、7Bのモデルによると次のトークンが「brown」である確率が最も高いです。<br>
…</p>
<p><img src="image-5.png" alt="alt text" height="400"></p>
<p>最初から一個の予測が間違っていますが、でもこれは大丈夫です。なぜかというと、検証の対象は新しく生成された「fox jumps」だけのためです。<br>
次のセルに「fox jumps over」がでましたが、「over」はボーナストークンです。</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>verified_ids <span class="op">=</span> big_model_ids[:, <span class="op">-</span>(candidate_length <span class="op">+</span> <span class="dv">1</span>) :]</span>
<span id="cb8-2"><a href="#cb8-2"></a>formatted_print(<span class="st">'verified_ids'</span>, verified_ids[<span class="dv">0</span>].tolist())</span>
<span id="cb8-3"><a href="#cb8-3"></a>formatted_print(<span class="st">'varified_tokens'</span>, tokenizer.decode(verified_ids[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>verified_ids:                             [38835, 34208, 916]
varified_tokens:                               fox jumps over</code></pre>
</div>
</div>
<p>0.5Bモデルが提案した結果を7Bモデルの検証結果と比較し、全部合っていることがわかりました。次に「fox jump」プラス最後のボーナストークン「over」をインプットに結合して次の生成に回すことができます。</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># validation result</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>n_matches <span class="op">=</span> (</span>
<span id="cb10-3"><a href="#cb10-3"></a>    (<span class="op">~</span>(candidate_new_ids <span class="op">==</span> verified_ids[:, :<span class="op">-</span><span class="dv">1</span>])).cumsum(dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">&lt;</span> <span class="dv">1</span></span>
<span id="cb10-4"><a href="#cb10-4"></a>).<span class="bu">sum</span>()  <span class="co"># fancy way to count the number of matches</span></span>
<span id="cb10-5"><a href="#cb10-5"></a>valid_ids <span class="op">=</span> verified_ids[:, : n_matches <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb10-6"><a href="#cb10-6"></a>next_input_ids <span class="op">=</span> torch.cat((input_ids, valid_ids), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-7"><a href="#cb10-7"></a></span>
<span id="cb10-8"><a href="#cb10-8"></a></span>
<span id="cb10-9"><a href="#cb10-9"></a>formatted_print(<span class="st">'n_matches'</span>, n_matches.item())</span>
<span id="cb10-10"><a href="#cb10-10"></a>formatted_print(<span class="st">'valid_ids'</span>, valid_ids[<span class="dv">0</span>].tolist())</span>
<span id="cb10-11"><a href="#cb10-11"></a>formatted_print(<span class="st">'valid_tokens'</span>, tokenizer.decode(valid_ids[<span class="dv">0</span>]))</span>
<span id="cb10-12"><a href="#cb10-12"></a>formatted_print(<span class="st">'next_input_ids'</span>, next_input_ids[<span class="dv">0</span>].tolist())</span>
<span id="cb10-13"><a href="#cb10-13"></a>formatted_print(<span class="st">'next_input_tokens'</span>, tokenizer.decode(next_input_ids[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>n_matches:                                                  2
valid_ids:                                [38835, 34208, 916]
valid_tokens:                                  fox jumps over
next_input_ids:         [785, 3974, 13876, 38835, 34208, 916]
next_input_tokens:             The quick brown fox jumps over</code></pre>
</div>
</div>
<p>これで、Speculative Decodingの一個の循環が完了しました。</p>
</section>
<section id="実験" class="level2">
<h2 class="anchored" data-anchor-id="実験">実験</h2>
<p>コードの分解もしたので、次に実際に実験してみましょう。今回はコード生成の結果を比較してみます。まず、上記のコードを関数として整理します。</p>
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="im">import</span> torch</span>
<span id="cb12-2"><a href="#cb12-2"></a></span>
<span id="cb12-3"><a href="#cb12-3"></a></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="kw">def</span> speculative_decoding(</span>
<span id="cb12-5"><a href="#cb12-5"></a>    big_model,</span>
<span id="cb12-6"><a href="#cb12-6"></a>    small_model,</span>
<span id="cb12-7"><a href="#cb12-7"></a>    input_ids,</span>
<span id="cb12-8"><a href="#cb12-8"></a>    max_length<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb12-9"><a href="#cb12-9"></a>    candidate_length<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb12-10"><a href="#cb12-10"></a>    tokenizer<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb12-11"><a href="#cb12-11"></a>):</span>
<span id="cb12-12"><a href="#cb12-12"></a>    generated_ids <span class="op">=</span> input_ids.clone()</span>
<span id="cb12-13"><a href="#cb12-13"></a>    total_generated <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-14"><a href="#cb12-14"></a>    accepted <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-15"><a href="#cb12-15"></a>    generated_ids_list <span class="op">=</span> [(<span class="st">'prompt'</span>, tokenizer.decode(input_ids[<span class="dv">0</span>]))]</span>
<span id="cb12-16"><a href="#cb12-16"></a></span>
<span id="cb12-17"><a href="#cb12-17"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-18"><a href="#cb12-18"></a>        <span class="cf">while</span> generated_ids.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">&lt;</span> max_length:</span>
<span id="cb12-19"><a href="#cb12-19"></a>            <span class="co"># 1. Candidate Generation (Small Model)</span></span>
<span id="cb12-20"><a href="#cb12-20"></a>            candidate_input_ids <span class="op">=</span> small_model.generate(</span>
<span id="cb12-21"><a href="#cb12-21"></a>                generated_ids, max_new_tokens<span class="op">=</span>candidate_length, do_sample<span class="op">=</span><span class="va">False</span></span>
<span id="cb12-22"><a href="#cb12-22"></a>            )</span>
<span id="cb12-23"><a href="#cb12-23"></a></span>
<span id="cb12-24"><a href="#cb12-24"></a>            <span class="co"># 2. Big Model Filtering</span></span>
<span id="cb12-25"><a href="#cb12-25"></a>            new_logits <span class="op">=</span> big_model(candidate_input_ids).logits[</span>
<span id="cb12-26"><a href="#cb12-26"></a>                :, <span class="op">-</span>(candidate_length <span class="op">+</span> <span class="dv">1</span>) :</span>
<span id="cb12-27"><a href="#cb12-27"></a>            ]  <span class="co"># +1 because we have a bonus token</span></span>
<span id="cb12-28"><a href="#cb12-28"></a>            selected_tokens <span class="op">=</span> new_logits.argmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb12-29"><a href="#cb12-29"></a>            candidate_new_tokens <span class="op">=</span> candidate_input_ids[:, generated_ids.shape[<span class="dv">1</span>] :]</span>
<span id="cb12-30"><a href="#cb12-30"></a></span>
<span id="cb12-31"><a href="#cb12-31"></a>            <span class="co"># Determine the actual number of generated tokens</span></span>
<span id="cb12-32"><a href="#cb12-32"></a>            num_generated_tokens <span class="op">=</span> candidate_new_tokens.shape[<span class="dv">1</span>]</span>
<span id="cb12-33"><a href="#cb12-33"></a></span>
<span id="cb12-34"><a href="#cb12-34"></a>            <span class="co"># Compare only the relevant portion of selected_tokens</span></span>
<span id="cb12-35"><a href="#cb12-35"></a>            n_matches <span class="op">=</span> (</span>
<span id="cb12-36"><a href="#cb12-36"></a>                (</span>
<span id="cb12-37"><a href="#cb12-37"></a>                    <span class="op">~</span>(candidate_new_tokens <span class="op">==</span> selected_tokens[:, :num_generated_tokens])</span>
<span id="cb12-38"><a href="#cb12-38"></a>                ).cumsum(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb12-39"><a href="#cb12-39"></a>                <span class="op">&lt;</span> <span class="dv">1</span></span>
<span id="cb12-40"><a href="#cb12-40"></a>            ).<span class="bu">sum</span>()</span>
<span id="cb12-41"><a href="#cb12-41"></a></span>
<span id="cb12-42"><a href="#cb12-42"></a>            valid_tokens <span class="op">=</span> selected_tokens[:, : n_matches <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb12-43"><a href="#cb12-43"></a>            generated_ids_list.append(</span>
<span id="cb12-44"><a href="#cb12-44"></a>                (<span class="st">'accepted'</span>, tokenizer.decode(valid_tokens[<span class="dv">0</span>, :n_matches]))</span>
<span id="cb12-45"><a href="#cb12-45"></a>            )</span>
<span id="cb12-46"><a href="#cb12-46"></a>            generated_ids_list.append(</span>
<span id="cb12-47"><a href="#cb12-47"></a>                (<span class="st">'generated'</span>, tokenizer.decode(valid_tokens[<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>:]))</span>
<span id="cb12-48"><a href="#cb12-48"></a>            )</span>
<span id="cb12-49"><a href="#cb12-49"></a>            <span class="co"># 3. Update Generated Sequence</span></span>
<span id="cb12-50"><a href="#cb12-50"></a>            generated_ids <span class="op">=</span> torch.cat((generated_ids, valid_tokens), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb12-51"><a href="#cb12-51"></a>            total_generated <span class="op">+=</span> candidate_length</span>
<span id="cb12-52"><a href="#cb12-52"></a>            accepted <span class="op">+=</span> n_matches</span>
<span id="cb12-53"><a href="#cb12-53"></a></span>
<span id="cb12-54"><a href="#cb12-54"></a>            <span class="cf">if</span> valid_tokens.shape[<span class="dv">1</span>] <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> (</span>
<span id="cb12-55"><a href="#cb12-55"></a>                valid_tokens.shape[<span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> tokenizer.pad_token_id <span class="kw">in</span> valid_tokens</span>
<span id="cb12-56"><a href="#cb12-56"></a>            ):</span>
<span id="cb12-57"><a href="#cb12-57"></a>                <span class="co"># delete tokens from padding</span></span>
<span id="cb12-58"><a href="#cb12-58"></a>                idx_pad <span class="op">=</span> (generated_ids <span class="op">==</span> tokenizer.pad_token_id).nonzero()</span>
<span id="cb12-59"><a href="#cb12-59"></a>                <span class="cf">if</span> idx_pad.numel() <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb12-60"><a href="#cb12-60"></a>                    generated_ids <span class="op">=</span> generated_ids[:, : idx_pad[<span class="dv">0</span>, <span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb12-61"><a href="#cb12-61"></a>                <span class="cf">break</span></span>
<span id="cb12-62"><a href="#cb12-62"></a></span>
<span id="cb12-63"><a href="#cb12-63"></a>    <span class="cf">return</span> generated_ids, total_generated, accepted, generated_ids_list</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>まず、Speculative Decodingを使わない場合どうなるかを確認します。</p>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>prompt <span class="op">=</span> <span class="st">'''from typing import List</span></span>
<span id="cb13-2"><a href="#cb13-2"></a></span>
<span id="cb13-3"><a href="#cb13-3"></a></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="st">def below_zero(operations: List[int]) -&gt; bool:</span></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="st">    """You're given a list of deposit and withdrawal operations on a bank account that starts with zero balance. Your task is to detect if at any point the balance of account fallls below zero, and at that point function should return True. Otherwise it should return False. &gt;&gt;&gt; below_zero([1, 2, 3]) False &gt;&gt;&gt; below_zero([1, 2, -4, 5]) True"""</span></span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="st">'''</span></span>
<span id="cb13-7"><a href="#cb13-7"></a>input_ids <span class="op">=</span> tokenizer.encode(prompt, return_tensors<span class="op">=</span><span class="st">'pt'</span>).to(big_model.device)</span>
<span id="cb13-8"><a href="#cb13-8"></a></span>
<span id="cb13-9"><a href="#cb13-9"></a>start <span class="op">=</span> time.time()</span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb13-11"><a href="#cb13-11"></a>    big_model_generated_ids <span class="op">=</span> big_model.generate(</span>
<span id="cb13-12"><a href="#cb13-12"></a>        input_ids.clone(), max_length<span class="op">=</span><span class="dv">1000</span>, do_sample<span class="op">=</span><span class="va">False</span></span>
<span id="cb13-13"><a href="#cb13-13"></a>    )</span>
<span id="cb13-14"><a href="#cb13-14"></a>cost <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb13-15"><a href="#cb13-15"></a></span>
<span id="cb13-16"><a href="#cb13-16"></a><span class="bu">print</span>(<span class="st">'Time cost:'</span>, <span class="ss">f'</span><span class="sc">{</span>cost<span class="sc">:.2f}</span><span class="ss">s'</span>)</span>
<span id="cb13-17"><a href="#cb13-17"></a><span class="bu">print</span>(<span class="st">'Generated code:'</span>)</span>
<span id="cb13-18"><a href="#cb13-18"></a><span class="bu">print</span>(tokenizer.decode(big_model_generated_ids[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Time cost: 13.21s
Generated code:
from typing import List


def below_zero(operations: List[int]) -&gt; bool:
    """You're given a list of deposit and withdrawal operations on a bank account that starts with zero balance. Your task is to detect if at any point the balance of account fallls below zero, and at that point function should return True. Otherwise it should return False. &gt;&gt;&gt; below_zero([1, 2, 3]) False &gt;&gt;&gt; below_zero([1, 2, -4, 5]) True"""
    balance = 0
    for operation in operations:
        balance += operation
        if balance &lt; 0:
            return True
    return False


if __name__ == "__main__":
    print(below_zero([1, 2, 3]))
    print(below_zero([1, 2, -4, 5]))&lt;|endoftext|&gt;</code></pre>
</div>
</div>
<p>次に、Speculative Decodingを使ってみます。</p>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb15-2"><a href="#cb15-2"></a>generated_ids, total_generated, accepted, generated_ids_list <span class="op">=</span> speculative_decoding(</span>
<span id="cb15-3"><a href="#cb15-3"></a>    big_model,</span>
<span id="cb15-4"><a href="#cb15-4"></a>    small_model,</span>
<span id="cb15-5"><a href="#cb15-5"></a>    input_ids,</span>
<span id="cb15-6"><a href="#cb15-6"></a>    max_length<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb15-7"><a href="#cb15-7"></a>    candidate_length<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb15-8"><a href="#cb15-8"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb15-9"><a href="#cb15-9"></a>)</span>
<span id="cb15-10"><a href="#cb15-10"></a>cost <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb15-11"><a href="#cb15-11"></a></span>
<span id="cb15-12"><a href="#cb15-12"></a><span class="bu">print</span>(<span class="st">'Total generated tokens:'</span>, total_generated)</span>
<span id="cb15-13"><a href="#cb15-13"></a><span class="bu">print</span>(<span class="st">'Accepted tokens:'</span>, accepted)</span>
<span id="cb15-14"><a href="#cb15-14"></a><span class="bu">print</span>(<span class="st">'Acceptance rate:'</span>, accepted <span class="op">/</span> total_generated)</span>
<span id="cb15-15"><a href="#cb15-15"></a><span class="bu">print</span>(<span class="st">'Time cost:'</span>, <span class="ss">f'</span><span class="sc">{</span>cost<span class="sc">:.2f}</span><span class="ss">s'</span>)</span>
<span id="cb15-16"><a href="#cb15-16"></a><span class="bu">print</span>(</span>
<span id="cb15-17"><a href="#cb15-17"></a>    <span class="st">'Same result generated by big model:'</span>,</span>
<span id="cb15-18"><a href="#cb15-18"></a>    (big_model_generated_ids <span class="op">==</span> generated_ids).<span class="bu">all</span>().item(),</span>
<span id="cb15-19"><a href="#cb15-19"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total generated tokens: 63
Accepted tokens: tensor(51, device='cuda:0')
Acceptance rate: tensor(0.8095, device='cuda:0')
Time cost: 8.47s
Same result generated by big model: True</code></pre>
</div>
</div>
<p>実際にテストしてみると、「Speculative Decoding」を使ってコードを生成する際に、0.5Bモデルが提案したトークンのうち<strong>77%が正しく</strong>、その結果、生成速度が13/8=1.6倍まで<strong>高速化されました</strong>。</p>
<p>一般的に、コード生成やコード修正などのタスクは、出力のランダム性が低いため、<strong>高いAcceptance Rate（受理率）</strong>が期待できます。そのため、Speculative Decodingは特にこれらのタスクに適しています。 OpenAIの<a href="https://platform.openai.com/docs/guides/predicted-outputs">predicted output</a>機能も、この手法を<strong>利用していると考えられます</strong>。</p>
<p>実験の最後に、<strong>おまけとして</strong>、生成されたコードのどの部分が0.5Bモデルの提案によるもので、どの部分が7Bモデルの検証結果によるものかを可視化してみましょう。 <strong>緑色の部分が0.5Bモデルの提案、オレンジ色の部分が7Bモデルの検証結果です。</strong></p>
<div class="cell" data-execution_count="95">
<details>
<summary>Click here to show the visualization code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML, display</span>
<span id="cb17-2"><a href="#cb17-2"></a></span>
<span id="cb17-3"><a href="#cb17-3"></a>html_output <span class="op">=</span> <span class="st">'&lt;pre&gt;'</span>  <span class="co"># Wrap the entire output in &lt;pre&gt; tags</span></span>
<span id="cb17-4"><a href="#cb17-4"></a></span>
<span id="cb17-5"><a href="#cb17-5"></a><span class="cf">for</span> <span class="bu">type</span>, text <span class="kw">in</span> generated_ids_list:</span>
<span id="cb17-6"><a href="#cb17-6"></a>    text <span class="op">=</span> text.replace(<span class="st">' '</span>, <span class="st">' '</span>)  <span class="co"># Replace spaces with</span></span>
<span id="cb17-7"><a href="#cb17-7"></a>    <span class="cf">if</span> <span class="bu">type</span> <span class="op">==</span> <span class="st">'prompt'</span>:</span>
<span id="cb17-8"><a href="#cb17-8"></a>        html_output <span class="op">+=</span> <span class="ss">f"&lt;span style='background-color: transparent;'&gt;</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&lt;/span&gt;"</span></span>
<span id="cb17-9"><a href="#cb17-9"></a>    <span class="cf">elif</span> <span class="bu">type</span> <span class="op">==</span> <span class="st">'accepted'</span>:</span>
<span id="cb17-10"><a href="#cb17-10"></a>        html_output <span class="op">+=</span> <span class="ss">f"&lt;span style='background-color: lightgreen;'&gt;</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&lt;/span&gt;"</span></span>
<span id="cb17-11"><a href="#cb17-11"></a>    <span class="cf">elif</span> <span class="bu">type</span> <span class="op">==</span> <span class="st">'generated'</span>:</span>
<span id="cb17-12"><a href="#cb17-12"></a>        html_output <span class="op">+=</span> <span class="ss">f"&lt;span style='background-color: orange;'&gt;</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&lt;/span&gt;"</span></span>
<span id="cb17-13"><a href="#cb17-13"></a></span>
<span id="cb17-14"><a href="#cb17-14"></a>html_output <span class="op">+=</span> <span class="st">'&lt;/pre&gt;'</span>  <span class="co"># Close the &lt;pre&gt; tag</span></span>
<span id="cb17-15"><a href="#cb17-15"></a></span>
<span id="cb17-16"><a href="#cb17-16"></a>display(HTML(html_output))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><span style="background-color: transparent;">from typing import List


def below_zero(operations: List[int]) -&gt; bool:
    """You're given a list of deposit and withdrawal operations on a bank account that starts with zero balance. Your task is to detect if at any point the balance of account fallls below zero, and at that point function should return True. Otherwise it should return False. &gt;&gt;&gt; below_zero([1, 2, 3]) False &gt;&gt;&gt; below_zero([1, 2, -4, 5]) True"""
</span><span style="background-color: lightgreen;">   </span><span style="background-color: orange;"> balance</span><span style="background-color: lightgreen;"> = 0</span><span style="background-color: orange;">
</span><span style="background-color: lightgreen;">    for operation</span><span style="background-color: orange;"> in</span><span style="background-color: lightgreen;"> operations:
       </span><span style="background-color: orange;"> balance</span><span style="background-color: lightgreen;"> += operation
</span><span style="background-color: orange;">       </span><span style="background-color: lightgreen;"> if balance &lt;</span><span style="background-color: orange;"> </span><span style="background-color: lightgreen;">0:
           </span><span style="background-color: orange;"> return</span><span style="background-color: lightgreen;"> True
   </span><span style="background-color: orange;"> return</span><span style="background-color: lightgreen;"> False</span><span style="background-color: orange;">


</span><span style="background-color: lightgreen;">if __name</span><span style="background-color: orange;">__</span><span style="background-color: lightgreen;"> ==</span><span style="background-color: orange;"> "__</span><span style="background-color: lightgreen;">main__":
   </span><span style="background-color: orange;"> print</span><span style="background-color: lightgreen;"></span><span style="background-color: orange;">(b</span><span style="background-color: lightgreen;">elow_zero([</span><span style="background-color: orange;">1</span><span style="background-color: lightgreen;">, 2</span><span style="background-color: orange;">,</span><span style="background-color: lightgreen;"> 3</span><span style="background-color: orange;">]))
</span><span style="background-color: lightgreen;">   </span><span style="background-color: orange;"> print</span><span style="background-color: lightgreen;">(below_zero</span><span style="background-color: orange;">([</span><span style="background-color: lightgreen;">1, </span><span style="background-color: orange;">2</span><span style="background-color: lightgreen;">, -4</span><span style="background-color: orange;">,</span><span style="background-color: lightgreen;"> 5]))</span><span style="background-color: orange;">&lt;|endoftext|&gt;</span></pre>
</div>
</div>
</section>
<section id="speculative-decodingを使う際の制限" class="level2">
<h2 class="anchored" data-anchor-id="speculative-decodingを使う際の制限">Speculative Decodingを使う際の制限</h2>
<p>Speculative Decodingは生成速度を向上させるための強力な手法ですが、いくつかの制限があります。 まず、提案を検証するためには、小さいモデルと大きいモデルのTokenizerが一緒でないといけません。この点については、Huggingfaceのほうで<a href="https://huggingface.co/blog/universal_assisted_generation">Universal assisted generation</a>を提案しました。つまり、提案したトークンをテキストに変換した後、また大きいモデルのTokenizerにトークンを変換することで、この問題を解決できます。 次に、Speculative Decodingが役に立つ前提としては、計算する際にメモリのスピードがボトルネックになることが必要です。言い換えると、バッチサイズを上げる場合は、メモリスピードより計算スピードがボトルネックになるため、あまりこの手法は効果がありません。</p>
</section>
<section id="まとめ" class="level2">
<h2 class="anchored" data-anchor-id="まとめ">まとめ</h2>
<p>本文では、Speculative Decodingの仕組みと実装方法について解説しました。この手法は、小さいモデルで提案したトークンを大きいモデルで検証することで、生成速度を向上させることができます。また、実験結果からも、この手法が生成タスクにおいて有効であることがわかりました。最後に、Speculative Decodingを使う際の制限についても触れました。</p>
</section>
<section id="参考文献" class="level2">
<h2 class="anchored" data-anchor-id="参考文献">参考文献</h2>
<ol type="1">
<li><a href="https://huggingface.co/blog/assisted-generation">Assisted Generation: a new direction toward low-latency text generation</a></li>
<li><a href="https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src/transformers/generation/utils.py#L1880">Transformers <code>generate</code> code</a></li>
<li><a href="https://x.com/karpathy/status/1697318534555336961">Karpathy’s X post on speculative decoding</a></li>
<li><a href="https://philkrav.com/posts/speculative/">Philkrav’s blog about speculative decoding</a></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="AlbertRapp/blogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>