# InPars Data Augmentation for Information Retrieval using Large Language Models.pdf

# 1. Introduction

LLMの性能が良いが、推論に必要な計算量が大きいためあまりIR（情報検索）に使われていない。また費用も1つの課題。GPT3のEmbedingサービスを使う場合は、少なくとも全てのテキストを一回通す必要があるため、件数が多いと膨大な費用がかかる。

また、データ側にも問題がある。現存のデータが商用不能なものが多いし、かつ既存のデータで学習したものが他の領域に汎化できない課題がある。

この論文は検索の推論の中でLLMを使うではなく、LLMで偽データを生成し、そのデータでランキングモデルを学習する手法を提案した。

# 2. Related work

今までIR領域でデータを作る研究としては、BM25を使って類似度が高いドキュメントをペアとしてモデルを学習する研究がある。また、教師ありのモデルでテキストからQueryを生成し、それとテキストのペアを学習データとする研究もあった。

この研究と既往研究との違いとしては、以前Zeroーshotで翻訳データを生成する研究を参考し、学習なして、LLMからZeroーshotでデータを生成したこと（ちょっと新奇性が足りてない気がする）。

# 3. Our Method: InPars

1. 複数のドキュメントdとクエリーqのペアが存在すると仮定する。それを例として、Few-shotでLLMでドキュメントに関連するクエリーを生成してもらう。
2. 生成したドキュメントとクエリーのペアについて、LLMが出力する際に出している結果のLog probabilityが上位のものを選択する（このステップが大きく精度を改善している）
3. 生成したqとdのペアを学習データとしてretrievalモデルをFine-tuningする。

# 4. Experimental Setup

## 4.1 Datasets

### MS MARCO

Microsoftが出したBingの実際のユーザログをベースとした大型データセット。880万のドキュメントと50万のドキュメントとクエリーのペアがある。各クエリーが平均的に1ドキュメントに対応する。

### TREC-DL

MS MARCOと同じドキュメントを持っているが、クエリーは54件のみ、また、各クエリーについてアノテーションしたドキュメントが多い。

### Robust04

新聞領域のデータセット。52万のドキュメントがあり、249クエリーがある。各クエリーに対して、平均的に1250件のドキュメントをアノテーションした。

### Natural Questions

260万件のWikipediaテキストをベースとしたQuestion Answer データセット。QuestionはGoogleの検索エンジンのログから作ったもの。

### TREC-COVID

コロナの情報に関するデータセット

## 4.2 Training Data Generation

各学習データは（Query, Positive document, Negative document）のTripleによって構成させる。

10万のドキュメントをサンプリングし、GPT3のCurieでQueryを生成させる。最終的にLog probabilityが上位の1万件のペアのみ学習データとして使う。

生成する際に温度とTopーPのパラメータ設定は結果に影響しない。

長さが300文字のドキュメントは捨てられる。

BM25で検索した1000件の中でランダムに1件を抽出し、それをNegative Documentとする。（このやり方で多くのノイズを入れてしまうのでは？）

Query生成する際にPromptの書き方：

一般方法（Vanilla)：

MS MARCOからランダムに3つのデータを抽出し、それを例として、FewーshotでQueryを生成させる。

GBQ（Guided by Bad Questions）：

一般方法と同じように、MS MARCOからランダムに3つのデータを抽出する。しかし、MS MARCOのQueryが漠然すぎるため、それをBad questionとして提示する。Documentを読んで、より関連するGood Questionを手動で作った。（Document, Good Question, Bad Question)で例を提示し、生成したGood QuestionをQueryとする。



4.3 Retrieval Methods

２段階の検索にしている。まずBM25で上位1000のドキュメントを取り出す。その次、MonoT5を使ってRerankingをする。

MonoT5はTransfromerのEncoderとDecoder両方とも使っているモデル。IR領域のCrossーEncoderモデルである。今回はサイズは220Mと3Bのモデルをテストした。

各データセットで作った偽データでFine-tuningした。

# 5 Results

![Untitled](InPars%20Data%20Augmentation%20for%20Information%20Retrieval%200b683a64d1854d4a87243827e2cde107/Untitled%201.png)

以前のUnsupervised結果より優れている。また、16行目のMS MARCOでFine-tuningした後さらに偽データでFineーtuningした結果。単純にMS MARCOでFine-tuningするよりいい結果が出ている。

# 6 Ablation Study and Analysis

6.1 Prompt Selection and Source Corpus

比較が混乱のため、何が言いたいかがわからなかった。

6.2 Model Size Impact on IR Metrics

当たり前だけど、Questionを生成するモデルのサイズが大きいほど結果がよくなる。

6.3 Filtering by the Most Likely Questions

Top1万件のデータを利用することにより精度が向上した。

6.4 Was GPT-3 Trained on Supervised IR Data?

生成したQuestionがどのぐらい実際のMS MARCOデータにあるかで、GPT3の学習データにMS MARCOのデータを含まれているかを検証した。結果としては多くても10％前後のため影響がない。（検証のやり方が正しいかが疑問）

# 7 Conclusion and Future Work

本文は、LLMを利用してQueryを生成して、生成したQueryとドキュメントのペアを学習データでモデルを学習する手法を提案した。

今後の改善点としては、

1. 偽データでDense RetrieverをFine-tuningする（すでにやったのでは？）
2. データを生成する際に作った”BAD question”をNegative exampleとして使用する
3. 偽データの数を増やす
4. （Query, Document)のペアを探すもっと良い手法を開発する

[https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9](https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9)

![Untitled](InPars%20Data%20Augmentation%20for%20Information%20Retrieval%200b683a64d1854d4a87243827e2cde107/Untitled%202.png)