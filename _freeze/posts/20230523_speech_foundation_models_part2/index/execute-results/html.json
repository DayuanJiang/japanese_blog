{
  "hash": "cbbf197ad62148a48f448f229db0adf8",
  "result": {
    "markdown": "---\ntitle: 2. Speech Large Language Models\n---\n\n\nYoutubeで音声認識に関する講義を受けて、講義のノートを取ります。\n\n資料のリンクは[こちら](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWJTZTdHTl9oR2JqaFhjbFJpYk1tVWo5LWQtZ3xBQ3Jtc0tta2pBZjY3dGQ5NFF3WkxuMWJvVXF2U1pwTU1MZktFTlRMYnFxWHBMY00yNUZ2UEdUT1lULWV3Ml9kYmZfLUt1aXQtQ1BlUV9WRnFhd1FHU0JmOFgxS1ozRERLR3MtVEhWdXJMZEhONTVyV00zZVg0WQ&q=https%3A%2F%2Fspeech.ee.ntu.edu.tw%2F%7Ehylee%2Fml%2Fml2023-course-data%2F%25E5%25BC%25B5%25E5%2587%25B1%25E7%2588%25B2-x-%25E6%25A9%259F%25E5%2599%25A8%25E5%25AD%25B8%25E7%25BF%2592-x-%25E8%25AA%259E%25E9%259F%25B3%25E5%259F%25BA%25E7%259F%25B3%25E6%25A8%25A1%25E5%259E%258B.pdf&v=m7Be7ppR6q0)です。\n\n今回の内容は音声基礎モデルです。全体は3部分に分けられます。\n\n1.  Speech Representation Learning(音声表現学習)\n2.  Speech Large Lanuage Models(音声大型言語モデル)\n3.  Other Speech Foundation Models(その他の音声基礎モデル)\n\nこのブログはPart2から始まります。\n\n\n![](images/paste-1.png)\n\n## Textless NLP Project\n\n音声翻訳をやる場合、通用ではCascaded Systemつまり複数のモデルを繋いてやります。まず音声をテキストに変換し、変換できたテキストを機械翻訳のモデルを通して翻訳します。翻訳したテキストをまたTTSモデルで音声に変換します。\n\nでもこのやり方だと、モデルを学習するためにテキストデータが必要です。また、複数の中間ステップがあるため、中間ステップ一個間違えたら最終結果が間違います。\n\n![](images/paste-2.png)\n\nTextless NLPのモデルだと、中間ステップの結果をPsuedo textというものに差し替えました。これにより、テキストなしで、音声対音声的なEnd to End学習ができます。また、テキストデータがありませんが、Psuedo textを使うことにより、今までのNLPの技術を使って中間ステップの改善を行なうことができます。\n\n![](images/paste-3.png)\n\nまた、音声翻訳のみではなく、中間のモデルを差し替えて違うタスクに対応できます。例えば、Speech continuationの場合はGPTを使います。音声合成の場合は中間のモデルを使わないです。\n\n::: callout-note\n## Speech continuation\n\nSpeech continuation（スピーチ・コンティニュエーション）は、音声認識や自然言語処理技術を用いて、話者の発言を他の話者が続けることができるようにする研究です。この研究は、人間と人工知能（AI）がスムーズにコミュニケーションできるようにすることを目的としています。具体的には、話者の言語パターンやスタイルを学習し、それに基づいて自然で適切なレスポンスを生成する技術を開発しています\n:::\n\n![](images/paste-4.png)\n\nPsuedo textの作り方はPart 1で紹介した音声の量子化と同じです。HuBERTを使って音声の特徴を抽出して、K-meansで得たカテゴリをTokenとします。\n\n![](images/paste-5.png)\n\nTextless NLPでSpeech continuationの例を説明します。まず、音声をHuBERTでTokenに変換します。ここでTokenを得たことで、次のステップは完全にNLPの領域になります。GPTと同じようなモデルを使って次のTokenを予測モデルを作ることができます。\n\n![](images/paste-6.png)\n\n最後は予測したTokenをDecoderに渡して音声を合成してもらいます。音声の合成のモデルも学習する必要があります。学習の方法としては、音声とTokenのペアを用意して、Tokenをインプットし、音声をアウトプットするモデルを学習します。\n\n![](images/paste-7.png)\n\n中間のモデルは言語モデルなので、条件なしの音声生成もできます。一方、条件なしだと、一見それぽいものが出力されますが、よく見ると意味が通じないものが出力されています。\n\n![](images/paste-8.png)\n\nTextless NLPは音声合成もできます。一方、この場合はインプットから内容、音調と話者の3つの特徴量を抽出しています。\n\n![](images/paste-9.png)\n\nなぜここで別の特徴量が必要がかというと、量子化することにより音声の中の内容が抽出され、逆に話者の情報が落ちてしましました。下図の右の表でその実験結果があります。話者識別の精度について、量子化しないHuBERTだと99%の精度を得られますが、量子化の粒度が粗いほど話者識別の精度が悪いです。CPCモデルも同じ傾向です。\n\n![](images/paste-10.png)\n\n同じモデルでVoice Conversionもできます。Vioce Conversionとは、音声の内容は同じですが、話者がほかの人にすることです。コナンの蝶ネクタイ型変声機の機能と同じです。Speaker Embeddingを差し替えるだけでできます。\n\nまた、この研究でHuBERTがいいモデルということもわかりましが。\n\n![](images/paste-11.png)\n\n同じフレームワークでSpeech codecもできます。speech codecは、音声信号をデジタルデータに変換し、デジタルデータを音声信号に戻す技術の研究分野です。この分野の目的は、音声データを効率的に圧縮し、伝送やストレージに適した形式にすることです。\n\nこのモデルはまず音声を特徴量化(下図の赤枠)にして、転送先にDecoderがあれば似た音声を生成することができます。右の図からその効果がわかります。横軸は圧縮した後の情報量、縦は復元した音声の品質、HuBERTはを使ったモデルはこの図の左上にあります。つまり、小さい情報量で質が良い音声を復元できました。\n\n![](images/paste-12.png)\n\nまた、音声翻訳もできます。例えば英語とスペイン語の音声翻訳をします。この時真ん中にあるものはBARTというEncoderとDecoder両方ともあるTransfomerです。\n\nこの段階では、英語のとスペイン語のデータはペアにある必要がありません。まず英語もしくはスペイン語の音声をMulti-LanguageのHuBERTを通過してTokenを得ます。次に、それをBARTに入力し、Auto-encoderの学習を行います。つまり、インプットのTokenをいくつかマスキングしてそれを復元することで学習しています。\n\n最終的に英語とスペイン語両方ともうまく復元できるBARTモデルが出来上がります。\n\n![](images/paste-13.png)\n\n一方、その学習がインプットした言語のみ復元できるため、翻訳するためには、ペアのデータで学習する必要があります。この論文ではEncoderをWav2vec 2.0に差し替えてやっています。元のBARTを使っても良いですが、単純に性能が良かっただけでWav2vec 2.0を使いました。\n\n![](images/paste-14.png)\n\nこのやり方で、Textless NLPのやり方はテキストデータがなしでも従来のCascaded systemと同じ精度を達成しています。\n\n![](images/paste-15.png)\n\n今まで説明したモデルはテキストデータが必要ではないですが、逆にいうと、テキストデータがあっても使えないです。その問題を解決できるはUnitYのモデルです。\n\nそのモデルは4つの部分により構成されます。音声が入力された後、まずSpeech encoderを通して、得た隠れ層をText decoderにインプットします。次に処理が分岐されます。\n\n音声とテキストのペアデータがある場合は、ここで補助のタスク(Auxiliary task)としてテキストを予測することができます。それと同時に、Text decoderから出力した別のベクトルが次のEncoderに渡して最終的に音声をアウトプットされます。\n\n![](images/paste-16.png)\n\n音声とテキストのペアのみではなく、全部の形式のデータをこのモデルの学習に使えます。(ラベルなしテキスト、ラベルなし音声、ラベル付き音声、ペア音声の全形式のデータ)\n\n![](images/paste-17.png)\n\nUnitYはCascaded systemより精度が良いです。\n\n![](images/paste-18.png)\n\n以前Metaが出した英語と福建語の音声翻訳はこのUnitYを使いました。下のビデオを御覧ください。\n\n<iframe src=\"https://www.facebook.com/plugins/video.php?href=https%3A%2F%2Fwww.facebook.com%2Fzuck%2Fvideos%2F2725795187550922%2F&amp;show_text=0&amp;width=380\" width=\"380\" height=\"476\" style=\"border:none;overflow:hidden\" scrolling=\"no\" frameborder=\"0\" allowfullscreen=\"true\" allow=\"autoplay; clipboard-write; encrypted-media; picture-in-picture; web-share\" allowFullScreen=\"true\">\n\n</iframe>\n\nビデオの中で、音声翻訳する時出力する声が本人の声ではありません。それは音声を量子化する際に話者の情報がなくなったためです。\n\n![](images/paste-19.png)\n\n## AudioLM\n\nAudioLMはGoogleが2022年10月出した研究です。[ここ](https://google-research.github.io/seanet/audiolm/examples/)で実際の音声生成のサンプルを見ることができます。\n\nTextless NLPと違って、AudioLMは話者の情報を保留しています。保留の方法としては、音声の意味(Sementic)以外に、音響(Acoustic)も同じようなやり方で量子化し、モデルに取り組むことです。\n\nAcoustic TokenはCodec Encoderにより抽出し、また、アウトプットする際にCodec Decoderからアウトプットします。\n\n![](images/paste-21.png)\n\nCodec modelはAuto-encoderのやり方で、インプットした音声を復元するタスクで学習しています。\n\nAcoustic Tokenは一般的なTokenと違い、複数層があります。その抽出の方法は下図のように、まず音声をいつもようにEncoderに通して、アウトプットした複数のベクトルについて、一層目のCodebookの中に一番近いTokenを探します。ここではA3, A2, A6がその結果でした。次に、一層目で得たベクトルとA3, A2, A6の差分を取って、次の層にインプットして同じようなことをします。これによって、最終的にN層のTokenを得られるます。得たTokenをDecoderに渡してDecodeします。\n\n要は各層のインプットは前の層のアウトプットとTokenのベクトルの差分なので、値がどんどん小さくなります。つまり、含まれる情報量がどんどん減ります。これによって、必要に応じ層を少なくして、音声の質をあまり落とさないまま、音声の圧縮することもできます。\n\n![](images/paste-22.png)\n\n音声を生成する際に、中間にあるunit LMは3つのTransfomerを繋いて作ったものです。\n\n![](images/paste-24.png)\n\n音声をアウトプットする詳細は以下です。\n\n1.  1個目のモデルは過去のSemantic Tokenを受けて、次に話す内容、つまに未来のSemantic Tokenを予測します。\n2.  生成された未来のSemantic Tokenと過去の荒いAcoustic Tokenと繋いて、2個めのモデルにインプットし、未来の荒いAcoustic Tokenを予測します。\n3.  2で生成された荒いAcoustic Tokenを3個目のモデルにインプットし、良いAcoustic Tokenを生成します。\n4.  2と3の結果を繋いてDecoderに渡して音声を生成します。\n\n![](images/paste-23.png)\n\nこのやり方で、音声のみではなくて、音楽の生成もできます。以下のビデオでサンプルを聞くことができます。\n\n<iframe width=\"500\" height=\"300\"  src=\"https://www.youtube.com/embed/_xkZwJ0H9IU\" title=\"AudioLM - Google AI Blog post\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\n\n</iframe>\n\n## VALL-E\n\nVALL-EはMicrosoftが2023年1月に出したTTS(Text to Speech)モデルです。VALL-Eの特徴としては、テキストのみではなくて、3秒の音声もインプットできます。それでアウトプットした音声がインプット音声と同じ人で話したようになります。\\\n実際の例は[このブログ](https://www.microsoft.com/en-us/research/project/vall-e/)で見れます。\n\n![](images/paste-25.png)\n\n前も言及したが、Acoustic Tokenの中で一番重要なのは最初の層です。VALL-Eは生成音声の質と生成速度をトレードオフのため、重要な一層目は質が高いがスピードが遅い自己回帰モデル(Autoregressive)を使いました。残りの層だとスピードが早いが、質が良くない非自己回帰モデル(Non-Autoregressive)を使いました。\n\n\n``` {raw}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\n\n```\n\n\n![](images/paste-26.png)\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}