<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>blog</title>
<link>https://jiang.jp/index.html</link>
<atom:link href="https://jiang.jp/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.3.340</generator>
<lastBuildDate>Fri, 05 May 2023 15:00:00 GMT</lastBuildDate>
<item>
  <title>LangChainのベーシックを全面解説する</title>
  <link>https://jiang.jp/posts/20230505_LangChain_basic/index.html</link>
  <description><![CDATA[ 



<section id="前書き" class="level2">
<h2 class="anchored" data-anchor-id="前書き">前書き</h2>
<p>OpenAIのGPTのAPIを利用してアプリを作成するには、今まで一番使いやすいパッケージはLangChain🦜️🔗 だと思います。本文では、LangChainの基本的な使い方を優しく説明します。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230505_LangChain_basic/langchain.png" class="img-fluid figure-img" width="300"></p>
<figcaption class="figure-caption">LangChain</figcaption>
</figure>
</div>
</section>
<section id="環境設定" class="level2">
<h2 class="anchored" data-anchor-id="環境設定">環境設定</h2>
<p>まずは定番の<code>pip</code>からインストールすることです。</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install langchain, openai</span></code></pre></div>
<p>そのつぎに、OpenAIのAPIキーを取得して、環境変数に設定します。 APIは<a href="https://platform.openai.com/account/api-keys">ここ</a>から取得できます。</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb2-2">os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"OPENAI_API_KEY"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"..."</span></span></code></pre></div>
<p>直接にAPIキーを書くのはセキュリティ上の問題があるので、スクリプトを共有する場合は(例えば本文)、APIキーを別ファイルに保存し、ファイルから読み込んだほうがよいです。</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os </span>
<span id="cb3-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"../../.env"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"r"</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> f: </span>
<span id="cb3-3">    os.environ.update(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>([line.strip().split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"="</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> line <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> f.readlines()]))</span></code></pre></div>
</div>
</section>
<section id="openaiのgptモデル" class="level2">
<h2 class="anchored" data-anchor-id="openaiのgptモデル">OpenAIのGPTモデル</h2>
<p>LangChainの中にOpenAIのGPTモデルを使うラッパーがあります。現在使えるモデルはテキスト補完モデルとChatモデルの2種類あります。生成モデルの場合は以下のように使います。</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.llms <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAI</span>
<span id="cb4-2">llm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OpenAI(temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb4-3">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> llm(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"日本の首都は?"</span>)</span>
<span id="cb4-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(output.strip())</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>東京です。</code></pre>
</div>
</div>
<p>また、Chatモデルを利用して対話を行うこともできます。</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.chat_models <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ChatOpenAI</span>
<span id="cb6-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.schema <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> (</span>
<span id="cb6-3">    AIMessage,</span>
<span id="cb6-4">    HumanMessage,</span>
<span id="cb6-5">    SystemMessage</span>
<span id="cb6-6">)</span>
<span id="cb6-7"></span>
<span id="cb6-8">chat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ChatOpenAI(temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb6-9">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> chat([HumanMessage(content<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"文法を修正してください:I loves programming."</span>)])</span>
<span id="cb6-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(output.content)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>I love programming.</code></pre>
</div>
</div>
<section id="各モデルの特性のまとめ" class="level3">
<h3 class="anchored" data-anchor-id="各モデルの特性のまとめ">各モデルの特性のまとめ</h3>
<p>各モデルの値段や、最大トークン数、モデルサイズは以下の表にまとめました。</p>
<p><strong>テキスト補完モデル</strong></p>

<table style="width:100%">
<tbody><tr>
<th>
モデル名
</th>
<th>
値段(1k tokensごと)
</th>
<th>
最大トークン数
</th>
<th>
モデルサイズ(推測)
</th>
</tr>
<tr>
<td>
Davinci
</td>
<td>
$0.0200
</td>
<td>
4,097
</td>
<td>
175B
</td>
</tr>
<tr>
<td>
Curie
</td>
<td>
$0.0020
</td>
<td>
4,097
</td>
<td>
6.7B
</td>
</tr>
<tr>
<td>
Babbage
</td>
<td>
$0.0005
</td>
<td>
4,097
</td>
<td>
1.3B
</td>
</tr>
<tr>
<td>
Ada
</td>
<td>
$0.0004
</td>
<td>
4,097
</td>
<td>
350M
</td>
</tr>

</tbody></table>
<strong>Chatモデル</strong>

<table style="width:100%">
<tbody><tr>
<th>
モデル名
</th>
<th>
値段(**Prompt)
</th>
<th>
値段(**補完)
</th>
<th>
最大トークン数
</th>
<th>
モデルサイズ(推測)
</th>
</tr>
<tr>
<td>
gpt-3.5-turbo
</td>
<td>
$0.002
</td>
<td>
$0.002
</td>
<td>
4,096
</td>
<td>
6.7B
</td>
</tr>
<tr>
<td>
gpt-4
</td>
<td>
$0.03
</td>
<td>
$0.06
</td>
<td>
8,192
</td>
<td>
6.7B
</td>
</tr>
<tr>
<td>
gpt-4-32k
</td>
<td>
$0.06
</td>
<td>
$0.12
</td>
<td>
32,768
</td>
<td>
1.3B
</td>
</tr>

</tbody></table>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>ここで注意することとしてはGPT4の値段です。インプットするテキストが<code>prompt</code>、生成したテキストは<code>completion</code>に分かれていて、<code>prompt</code>の値段と<code>completion</code>の値段を足したものがGPT4の値段になります。</p>
</div>
</div>
</section>
<section id="モデルの使い分け" class="level3">
<h3 class="anchored" data-anchor-id="モデルの使い分け">モデルの使い分け</h3>
<p>モデルの使い分けについては、最も使われているのはChatモデルの<code>gpt-3.5-turbo</code>と<code>gpt-4</code>です。<code>gpt-3.5-turbo</code>はモデルのサイズが小さいので、生成時間が短く、値段も安いです。一方、<code>gpt-4</code>は性能が良いので、性能を求める場合は<code>gpt-4</code>のほうが良いです。また、<code>gpt-4</code>の最大トークン数が8Kになっているので、生成するテキストの長さが長い場合もこちらを使うほうがいいです。</p>
<p>他のモデルはほとんど使われないので、必要に応じて詳細を見れば良いです。</p>
</section>
<section id="tokenの計算方法" class="level3">
<h3 class="anchored" data-anchor-id="tokenの計算方法">Tokenの計算方法</h3>
<p>Tokenの計算方法については、<a href="https://www.jiang.jp/posts/20230505_tiktoken/#tictoken%E3%81%AE%E6%8C%99%E5%8B%95">こちら</a>で紹介したので、本文では割愛します。要するに、日本語千文字のドキュメントはおおよそ1,000トークンになり、それを処理するには<code>gpt-3.5-turbo</code>の場合は概算で0.59円、<code>gpt-4</code>の場合は概算で$9.7円かかります。</p>
</section>
</section>
<section id="prompt-template" class="level2">
<h2 class="anchored" data-anchor-id="prompt-template">Prompt Template</h2>
<p>LangChainのPrompt TemplateはPromptを簡単に作成するためのモジュールです。Example selector付きのPromptを作るにはとても役に立ちます。でもそれはよりアドバンス的なやり方なので、入門の段階では単純にPythonのf-stringとして使えれば良いです。</p>
<p>Promptのテンプレートを書いた後、それを<code>PromptTemplate</code>のインスタンスに渡して、<code>PromptTemplate</code>の<code>format</code>メソッドを呼び出すと、Promptが生成されます。</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PromptTemplate</span>
<span id="cb8-2"></span>
<span id="cb8-3">template <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"私は</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{fruit}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">が好きです。"</span></span>
<span id="cb8-4">prompt_template <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PromptTemplate.from_template(template)</span>
<span id="cb8-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(prompt_template.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(fruit<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"りんご"</span>))</span>
<span id="cb8-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(prompt_template.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(fruit<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"みかん"</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>私はりんごが好きです。
私はみかんが好きです。</code></pre>
</div>
</div>
</section>
<section id="vectorstore" class="level2">
<h2 class="anchored" data-anchor-id="vectorstore">VectorStore</h2>
<p>ドキュメントを検索するためには、<code>VectorStore</code>を作成する必要があります。<code>VectorStore</code>はドキュメントのリストを受け取って、それをベクトルに変換して保存します。検索する際に、検索クエリをベクトルに変換して、ベクトルの類似度を計算して、類似度が高いドキュメントを返します。</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
FAISSについて
</div>
</div>
<div class="callout-body-container callout-body">
<p>FAISSはMetaが開発した高速な類似性検索ライブラリです。Faissは、大量のベクトルデータを格納し、高速な検索を行うことができます。</p>
</div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.document_loaders <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> TextLoader</span>
<span id="cb10-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.indexes <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> VectorstoreIndexCreator</span>
<span id="cb10-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.vectorstores <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> FAISS</span>
<span id="cb10-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># create test data</span></span>
<span id="cb10-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./test_data.txt"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"w"</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> f:</span>
<span id="cb10-6">    fruits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"りんご"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"みかん"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"バナナ"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"パイナップル"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ぶどう"</span>]</span>
<span id="cb10-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> fruit <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> fruits:</span>
<span id="cb10-8">        f.write(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"私は</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>fruit<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">が好きです。</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb10-9">        </span>
<span id="cb10-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># load test data</span></span>
<span id="cb10-11">loader <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TextLoader(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'./test_data.txt'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'utf8'</span>)</span>
<span id="cb10-12"></span>
<span id="cb10-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># query test data</span></span>
<span id="cb10-14">index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> VectorstoreIndexCreator(vectorstore_cls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>FAISS).from_loaders([loader])</span>
<span id="cb10-15">index.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"りんご"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>' 私はりんごが好きです。'</code></pre>
</div>
</div>
</section>
<section id="chain" class="level2">
<h2 class="anchored" data-anchor-id="chain">Chain</h2>
<p>ChainはLangChainの中心的な概念です。今まで紹介した複数の部品を組み合わせでChainを作ることができます。インプットが入力された後、Chainの内部で処理し、アウトプットを出す。</p>
<p>例えば、PromptTemplateとLLMをつなぐChainを作ることができます。PromptTemplateはPromptを生成するので、LLMのインプットになります。LLMはPromptを受け取って、それを補完して、アウトプットを生成します。こうしてPromptTemplateとLLMをつなぐChainを作ることができます。</p>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    Input([Input])--&gt;PromptTemplate
    LLM--&gt;Output([Output])
    subgraph Chain
    PromptTemplate--&gt;formattedPrompt([Formatted Prompt])
    formattedPrompt--&gt;LLM
    end
    style PromptTemplate stroke:#333,stroke-width:4px
    style LLM stroke:#333,stroke-width:4px
</pre>
</div>
<p>Chainのダイアグラムの例</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.prompts <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PromptTemplate</span>
<span id="cb12-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.llms <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAI</span>
<span id="cb12-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.chains <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LLMChain</span>
<span id="cb12-4">llm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OpenAI(temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>)</span>
<span id="cb12-5">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PromptTemplate.from_template(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{country}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">の首都は何ですか？"</span>)</span>
<span id="cb12-6">chain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LLMChain(llm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>llm, prompt<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prompt)</span></code></pre></div>
</div>
<p>これで各国の首都は簡単に検索できるようになりました。</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(chain.run({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"country"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"日本"</span>}).strip())</span>
<span id="cb13-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(chain.run({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"country"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"アメリカ"</span>}).strip())</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>東京です。
ワシントンD.C.</code></pre>
</div>
</div>
</section>
<section id="agent" class="level2">
<h2 class="anchored" data-anchor-id="agent">Agent</h2>
<p>AgentはChainよりも高いレベルの概念です。Agentはツールを使うことができます。それにより、Agentは内部環境にとどまらず、外部環境ともやり取りできます。</p>
<p>一番シンプルの例としてはBingChatがあげられます。ユーザーのクエリーを受けた後、BingChatはインタネットから情報を検索し、それをサマリーして、ユーザーのクエリに答えます。</p>
<p>Agentの中身は複雑でドキュメントに書いていないので、今回は挙動だけ見せます。ここでBingChatに似ている機能を実現するAgentを作ります。このAgentはユーザーのクエリーを受け取って、それをインタネットで検索し、その答えを返すことができます。また、外部の電卓ツールを利用して計算もできます。</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.agents <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_tools</span>
<span id="cb15-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.agents <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> initialize_agent</span>
<span id="cb15-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.agents <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AgentType</span>
<span id="cb15-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.llms <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAI</span>
<span id="cb15-5"></span>
<span id="cb15-6">llm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OpenAI(temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb15-7">tools <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_tools([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"serpapi"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"llm-math"</span>], llm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>llm)</span>
<span id="cb15-8">agent <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> initialize_agent(tools, llm, agent<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">agent.run(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"今日の気温は何度ですか？その2乗は何ですか？"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>

&gt; Entering new AgentExecutor chain...
 I need to find out the temperature and then calculate its square.
Action: Search
Action Input: 今日の気温
Observation: ニューヨーク, NY, アメリカ合衆国 の天気. 4. 今日 · 1時間ごと · 10日間 · レーダー. 1時間ごとの天気-ニューヨーク, NY, アメリカ合衆国. 13:48 EDT時点 ...
Thought: I need to find the temperature from the search results
Action: Search
Action Input: 今日の気温 ニューヨーク
Observation: 16:00 · 体感温度16° · 風南東 8 km/h · 湿度47% · 紫外線指数2/10 · 雲量78% · 雨量0 cm ...
Thought: I now have the temperature, I need to calculate its square
Action: Calculator
Action Input: 16^2
Observation: Answer: 256
Thought: I now know the final answer
Final Answer: 今日の気温は16度で、その2乗は256です。

&gt; Finished chain.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>'今日の気温は16度で、その2乗は256です。'</code></pre>
</div>
</div>
<p>「今日の気温は何度ですか？その2乗は何ですか？」のクエリーを投げた後、Agentのほうはまずやるべきことを決めました。やるべきことをプランニングしながら、自分が持っているツールを駆使し、クエリーに答えました。</p>
</section>
<section id="まとめ" class="level2">
<h2 class="anchored" data-anchor-id="まとめ">まとめ</h2>
<p>これでLangChainの中にあるMemory以外のものをひと通り浅く紹介しました。LangChainの開発はまだ初期の段階なので、APIの設計や、ドキュメントの充実さなどの問題があります。今後は各概念を解剖する記事を書いていきます。</p>


</section>

 ]]></description>
  <category>NLP</category>
  <category>LLMs</category>
  <category>LangChain</category>
  <guid>https://jiang.jp/posts/20230505_LangChain_basic/index.html</guid>
  <pubDate>Fri, 05 May 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>OpenAIのGPTのAPIのToken数に関する調査</title>
  <link>https://jiang.jp/posts/20230505_tiktoken/index.html</link>
  <description><![CDATA[ 



<section id="結論" class="level2">
<h2 class="anchored" data-anchor-id="結論">結論</h2>
<p>OpenAIのGPTモデルでドキュメントを処理する際に、日本語の1文字は大よそ1Tokenに等しいです。千文字のドキュメントを処理するためには、概算で、スピード重視の<code>gpt-3.5-turbo</code>を使う場合は0.59円かかります。性能重視の<code>gpt-4-32k</code>を利用する場合は、9.7円かかります。</p>
</section>
<section id="目的" class="level2">
<h2 class="anchored" data-anchor-id="目的">目的</h2>
<p>GPT3を用いた提案をする際によく聞かれることとしては、コストいくらかのことです。GPT3のAPIの課金は下記のように文字数ではなく、<code>token</code>を単位としているため、説明するのは簡単ではないです。</p>
<p>本文は値段の説明をしやすいように、実際のデータで実験してみます。ついてにTicTokenの挙動についても掘り下げてみます。 実験のステップは下記の通りです。</p>
<ol type="1">
<li>livedoor ニュースコーパスをダウンロードする<br>
</li>
<li>ニュースコーパスを<code>tiktoken</code>でトークナイズする</li>
<li>Token数/文字数で、千文字あたりの値段を計算する</li>
</ol>
<strong>テキスト補完モデル</strong>

<table style="width:100%">
<tbody><tr>
<th>
モデル名
</th>
<th>
値段(<strong>Prompt</strong>)
</th>
<th>
値段(<strong>補完</strong>)
</th>
<th>
最大トークン数
</th>
<th>
モデルサイズ(推測)
</th>
</tr>
<tr>
<td>
gpt-3.5-turbo
</td>
<td>
$0.002
</td>
<td>
$0.002
</td>
<td>
4,096
</td>
<td>
6.7B
</td>
</tr>
<tr>
<td>
gpt-4
</td>
<td>
$0.03
</td>
<td>
$0.06
</td>
<td>
8,192
</td>
<td>
不明
</td>
</tr>
<tr>
<td>
gpt-4-32k
</td>
<td>
$0.06
</td>
<td>
$0.12
</td>
<td>
32,768
</td>
<td>
不明
</td>
</tr>

</tbody></table>
</section>
<section id="前準備" class="level2">
<h2 class="anchored" data-anchor-id="前準備">前準備</h2>
<p>GPT3のTokenizerは<a href="https://github.com/openai/tiktoken"><code>tiktoken</code></a>というライブラリを利用しているので、検証するためには<code>tiktoken</code>をインストールする必要があります。</p>
<p>今回利用するデータは、<a href="https://www.rondhuit.com/download.html">livedoor ニュースコーパス</a>です。livedoor ニュースコーパスは、9つのカテゴリに分類された、記事のデータセットです。</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>pip install tiktoken</span>
<span id="cb1-2"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>curl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>O https:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span>www.rondhuit.com<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>download<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>ldcc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">20140209.</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">tar</span>.gz</span>
<span id="cb1-3"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>tar <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>zxvf ldcc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">20140209.</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">tar</span>.gz</span></code></pre></div>
</div>
<p>次に文字数とトークン数の関係を計算します。</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> glob</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> matplotlib <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> tiktoken</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># load data</span></span>
<span id="cb2-7">path_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> glob.glob(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'./text/*/*.txt'</span>)</span>
<span id="cb2-8">txt_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[]</span>
<span id="cb2-9"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> path <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> path_list:</span>
<span id="cb2-10">    category <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> path.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'/'</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]</span>
<span id="cb2-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(path) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> f:</span>
<span id="cb2-12">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># skip first 2 lines</span></span>
<span id="cb2-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>):</span>
<span id="cb2-14">            f.readline()</span>
<span id="cb2-15">        txt_list.append(( category, f.read()))</span>
<span id="cb2-16">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  pd.DataFrame( txt_list, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'text'</span>])</span>
<span id="cb2-17">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"word_count"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> x: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x))</span>
<span id="cb2-18"></span>
<span id="cb2-19"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># cl100k_base is for gpt-4, gpt-3.5-turbo, text-embedding-ada-002</span></span>
<span id="cb2-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb</span></span>
<span id="cb2-21">encoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tiktoken.get_encoding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cl100k_base"</span>)</span>
<span id="cb2-22">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"token_ids"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> x: encoder.encode(x))</span>
<span id="cb2-23">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"token_count"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"token_ids"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> x: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x))</span>
<span id="cb2-24">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tokens"</span>]  <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"token_ids"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> x: encoder.decode_tokens_bytes(x))</span>
<span id="cb2-25">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"word_token_ratio"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"token_count"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"word_count"</span>]</span></code></pre></div>
</div>
</section>
<section id="計算" class="level2">
<h2 class="anchored" data-anchor-id="計算">計算</h2>
<p>まず、処理するデータの様子を実際に見てみましょう。</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ドキュメントのサンプル："</span>)</span>
<span id="cb3-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> txt_list[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]:</span>
<span id="cb3-3">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(i)</span>
<span id="cb3-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"..."</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ドキュメントのサンプル：
【DVDエンター！】誘拐犯に育てられた女が目にした真実は、孤独か幸福か
　2005年11月から翌2006年7月まで読売新聞にて連載された、直木賞作家・角田光代による初の長編サスペンス『八日目の蝉』。2010年に檀れいと北乃きいの出演によりテレビドラマ化された同作が、2011年4月に永作博美と井上真央の出演によって映画化。そして、劇場公開から半年が過ぎた10月28日、DVD＆ブルーレイとなって発売されました。

八日目の蝉
　妻子ある男と愛し合い、その子を身ごもりながら、あきらめざるをえなかった女。彼女は同時に、男の妻が子供を産んだことを知る。その赤ん坊を見に行った女は、突発的にその子を連れ去り、逃避行を続けた挙句、小豆島に落ち着き、母と娘として暮らしはじめる。


不倫相手の子供を誘拐し、4年間育てた女
　永作博美が演じる野々宮希和子は、不倫相手の子を宿しながらも、彼の「いずれ妻と別れるから、それまで待ってくれ」という常套句を信じて、中絶。後遺症により、二度と子供を産めない身体となってしまいます。その後、不倫相手から彼の妻が出産したことを知らされ、別れを決意。最後に諦めをつけるため、彼らの生後6ヶ月の赤ん坊・恵理菜の顔を見た希和子でしたが、自分に笑顔で向けた恵理菜を見て、思わず誘拐。名前を変えて恵理菜を薫と名付けると、人目を避けて各地を転々とし、二人で幸せな時間を過ごしますが、辿り着いた最後の場所・小豆島で4年の逃避行に終止符を打ちます。

...</code></pre>
</div>
</div>
<p>合計7,376件のドキュメントがあり、平均文字数は1,200文字程度です。</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df.word_count.describe().astype(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>count     7376
mean      1259
std        763
min         37
25%        730
50%       1069
75%       1602
max      12163
Name: word_count, dtype: int64</code></pre>
</div>
</div>
<p><span style="background-color: #FFFF00"> 文字数とトークン数の割合を見ると、以外に1文字が1トークンになっていることがわかります。 </span> また、この傾向が記事の種類によりますが、大きな違いはありません。</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">df.word_token_ratio.mean()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>1.008244127016698</code></pre>
</div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">df.groupby(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"category"</span>).word_token_ratio.mean().sort_values().plot.barh(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>&lt;Axes: ylabel='category'&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://jiang.jp/posts/20230505_tiktoken/index_files/figure-html/cell-7-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>これで文字数とトークン数の関係がわかることによって、ドキュメントを処理する課金を概算計算することができます。概算ロジックは以下と仮定です。</p>
<ul>
<li><p>インプットの長さは2000字とする。内訳は以下の通り。</p>
<ul>
<li><p>処理するドキュメントの長さは1000字とする。</p></li>
<li><p>タスクの説明や、処理の例は1000字とする。</p></li>
</ul></li>
<li><p>アウトプットは200字とする。</p></li>
<li><p>為替レートは1ドル=135円とする。</p></li>
</ul>
<p>これで計算すると1ドキュメントを処理するためには:</p>
<ul>
<li><p>スピードを求める<code>gpt-3.5-turbo</code>の場合は、0.002 * 2200 / 1000 * 135 = 0.59円 かかります。</p></li>
<li><p>性能を重視する<code>gpt-4-32k</code>を利用する場合は(0.03 * 2000 + 0.06 * 200) / 1000 * 135 = 9.7円 かかります。</p></li>
</ul>
</section>
<section id="tictokenの挙動" class="level2">
<h2 class="anchored" data-anchor-id="tictokenの挙動">TicTokenの挙動</h2>
<section id="bpeモデルが違う" class="level3">
<h3 class="anchored" data-anchor-id="bpeモデルが違う">BPEモデルが違う</h3>
<p>日本語は英語よりトークン数が多いと話している投稿は過去Twitterで見たことがあります。今回実際に計算してみると、日本語の1文字は大よそ1Tokenに等しいことがわかりました。それはDecodingするモデルが違うためです。</p>
<p>ここからはちょっと深い話をします。<code>TikToken</code>はBPE(Byte Pair Encoding)というデータ圧縮法に基づいて開発しました。コンピューターは文字を扱うことができないので、文字を数値に変換する必要があります。BPEは文字列をシンボルに置き換えることで、文字列を数値に変換します。BPEは頻繁に現れる文字のペアや、複数の文字を組み合わせたシンボルを生成します。それにより、入力するシーケンスの長さを短くすることができます。</p>
<p>例えば、“ab ab b”の文字列について、“ab”を0に、“b”を1に置き換えると、“0 0 1”という文字列になります。このように、BPEは文字列をシンボルに置き換えることで、もともと長さが7の文字列を長さが5のシーケンスに変換できました。</p>
<p>また、どの組み合わせをシンボルにするかはデータから学習することによって決められています。<code>gpt-3.5-turbo</code>と<code>gpt-4-32k</code>のモデルは以前のGPT3のモデルが違うので、Tokenizeした結果も違います。</p>
<p>実際の例を見ましょう。</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">gpt4_encoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tiktoken.encoding_for_model(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-4-32k"</span>)</span>
<span id="cb11-2">gpt3_encoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tiktoken.encoding_for_model(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text-davinci-003"</span>)</span>
<span id="cb11-3"></span>
<span id="cb11-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"GPT3のトークン数："</span>)</span>
<span id="cb11-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"こんにちは: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(gpt3_encoder.encode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'こんにちは'</span>))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>()</span>
<span id="cb11-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"GPT4のトークン数："</span>)</span>
<span id="cb11-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"こんにちは: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(gpt4_encoder.encode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'こんにちは'</span>))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GPT3のトークン数：
こんにちは: 6

GPT4のトークン数：
こんにちは: 1</code></pre>
</div>
</div>
</section>
<section id="gptのbpeモデルは日本語をバイト化してからトークン化している" class="level3">
<h3 class="anchored" data-anchor-id="gptのbpeモデルは日本語をバイト化してからトークン化している">GPTのBPEモデルは日本語をバイト化してからトークン化している</h3>
<p>GPT3のToken数がGPT4より多いことがわかります。例えば、「こんにちは」はGPT3で6Tokenになりますが、GPT4では1Tokenになります。</p>
<p>「こんにちは」については5文字はしかないですが、なぜ6Tokenになっているかに疑問を思うかもしれません。それはGPT3が多言語に対応するために、直接テキストで切っていなくて、日本語をまずバイトに変換して切っているからです。バイト化することにより違う言語でも共通のTokenで表現することができます。</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">tokeinzer_result_byte <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gpt3_encoder.decode_tokens_bytes(gpt3_encoder.encode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'こんにちは'</span>))</span>
<span id="cb13-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Tokenize結果:"</span>, tokeinzer_result_byte)</span>
<span id="cb13-3">tokeinzer_result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [i.decode() <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(i) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span> i <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> tokeinzer_result_byte ]</span>
<span id="cb13-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Decoding結果:"</span>, tokeinzer_result)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tokenize結果: [b'\xe3\x81\x93', b'\xe3\x82\x93', b'\xe3\x81\xab', b'\xe3\x81', b'\xa1', b'\xe3\x81\xaf']
Decoding結果: ['こ', 'ん', 'に', b'\xe3\x81', b'\xa1', 'は']</code></pre>
</div>
</div>
<p>上記の結果からわかることとしては、日本語1キャラクターは3バイトで表示しています。「こんにちは」の中の「ち」のみ2Tokenに分解されました。</p>
</section>
<section id="実際の比較" class="level3">
<h3 class="anchored" data-anchor-id="実際の比較">実際の比較</h3>
<p>つぎに、実際にデータでGPT3とGPT4のTokenizeの結果を比較してみましょう。</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"token_ids_gpt3"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> x: gpt3_encoder.encode(x))</span>
<span id="cb15-2">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"token_count_gpt3"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"token_ids_gpt3"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> x: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x))</span>
<span id="cb15-3">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"word_token_ratio_gpt3"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"token_count_gpt3"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"word_count"</span>]</span>
<span id="cb15-4">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"word_token_ratio_gpt3"</span>].mean()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>1.317645208825121</code></pre>
</div>
</div>
</section>
</section>
<section id="まとめ" class="level2">
<h2 class="anchored" data-anchor-id="まとめ">まとめ</h2>
<p>過去にGPT3を使う場合は日本語のToken数は英語の2倍になる噂があります。GPTモデルで日本語のドキュメントを処理する際、1文字はおおよそ1トークンに等しいことがわかりました。千文字のドキュメントを処理するための概算コストは、スピード重視のgpt-3.5-turboを使う場合は0.59円、性能重視のgpt-4-32kを利用する場合は9.7円です。</p>
<p>また、GPT3とGPT4が使うTokenizerが違い、GPT3のトークン数はおおよそGPT4の1.3倍になります。</p>


</section>

 ]]></description>
  <category>NLP</category>
  <category>LLMs</category>
  <category>LangChain</category>
  <guid>https://jiang.jp/posts/20230505_tiktoken/index.html</guid>
  <pubDate>Thu, 04 May 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>InPars light 論文解読</title>
  <link>https://jiang.jp/posts/20230502_inpars_light/index.html</link>
  <description><![CDATA[ 



<p>InPars-lightは、無料で利用可能な言語モデルBLOOMをランキングモデルを使用し、1000個ではなく100個の候補レコードを再ランクしした。 先行研究の主要な知見を再現するだけでなく、Consistency checkingとAll-domain pre-trainingを組み合わせることで、非常に効率的で小型なモデルMiniLM-L6-30Mを訓練し、すべてのデータセットでBM25を上回る性能を達成した。最後に、大きなDeBERTA-v3-435Mモデルを使用して、7倍大きなMonoT5-3Bの性能をほぼマッチさせることができた。</p>
<p>論文URL：&lt;https://arxiv.org/abs/2301.02998&gt;</p>
<section id="introduction" class="level1">
<h1>1 Introduction</h1>
<p>IR領域でのニューラルモデルを学習させるためには、大量なラベリングしたデータが必要である。データラベリングのコストが非常に高い：Document-Queryのペアが関連するかを判断するには1分以上かかる。一個のQueryについては通常50件以上のドキュメントを見る必要がある。そのため、最近の研究は主にラベリングデータを生成することに集中している。</p>
<p>一方、今までの研究はお主にLLMsを使っていて、費用対効果が良くない。また、GPT3のようなLLMsはAPIのみアクセスしかできない。その2つの問題を解決するためには、この論文はInParsを再現し、改善を行った。</p>
<p>InParsがmonoT5-3Bとmonot5-220Mを使ったが、この論文は30MのLMと435MのDebertaを使って同等レベルの結果を得られた。Inparsは上位1000件のドキュメントをRerankしたが、この論文は100件のみRerankしている。</p>
<p>この論文は以下のResearch Questionを提起した：</p>
<ol type="1">
<li>情報検索（IR）能力は、単に大規模なnext-token-prediction学習から生まれるか。</li>
<li>データ生成においてOpen sourceのモデルは同じサイズのGPT3より劣るか。</li>
<li>一致性検査(Consistency checking)はほんとに有用か。</li>
<li>より小さいBertモデルでMonoT5-3Bを置換する場合は同じ性能を出せるか</li>
<li>30Mの小さいLMを使う場合はBM25に勝てるか。</li>
</ol>
<p>結果：</p>
<p>1&amp;2: BLOOMやGPT-JのようなOpen source LLMは同等サイズなGPT3より高性能の同時に、コストが1/10のみ。</p>
<p>3: 一致性検査はいつも有効である。</p>
<p>4&amp;5：InParsのやり方だと小さいモデルは使えない。一方、全部のデータゼットで前学習し、さらに生成したデータでFine-Tuningした30MのモデルがいつもBM25よりよい結果を出した。</p>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">2 Related Work</h2>
<p>UPR: 3BのLLMをRerankとして使った。第一段階で取り出したドキュメントに対して”please write a question for this document”でQueyrを生成するLog probabiltyを計算し、それでRerankをしている。(LLMを学習させる際にLossの計算と同じやり方)</p>
<p>その他、InPars-v1、InPars-v2、Promptagator、HyDEが紹介された。以前の論文紹介で詳細を書いたため、今回は割愛する。</p>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">3 Methods</h2>
<p>この論文も2段階の検索を使った。まずBM25で大量なDocumentから関連するDocumentをフィルタリングする。そのつぎにニューラルモデルでRerankする。</p>
<p>RerankはCross-encoderを利用した。具体的に以下の3種類なものがある。</p>
<ol type="1">
<li>MiniLM-L6(30M)</li>
<li>ERNIE-v2(335M)</li>
<li>DeBERTA-v3(435M)</li>
</ol>
<p>ERNIEとDeBERTAを利用した理由としては、今2つのモデルはMS MARCOで強い結果を出したことがある。</p>
<p>Inparsと同じように各データセットに対して100kのQueryを生成した。生成したQueryとDocumentのペアでRerankerを学習させ、それを使ってConsistency checkingをした。Consistency checkingをする時に、生成したQueryで検索をかけて、生成元のDocumentがTop-Kにないとそれを捨てる。Kについては、1でも良いが、3のほうが精度が高かった。</p>
<p>また、面白いのは、Consistency checkingでフィルタリングしたデータとLog Probabilityでフィルタリングしたデータは20〜30%のみ共通している。</p>
<p>Rerankerを学習させる際に、まず生成した全データで学習させ、その上で、フィルタリングしたデータでFine-Tuningを行った。</p>
<p>この研究でMiniMLに対して、まずすべてのデータセットで生成したすべてのデータで学習し、さらにすべてのデータセットのフィルタリングしたデータでFine-Tuningしたが、過学習した。</p>
<p>実装する際に、FlexNeuARTのフレームワークを使った。モデルを学習させる際にInfoNCE Lossを使った。各Queryに対してNegative sampleを、BM25で検索できた上位1000件の中から3つサンプリングした。</p>
<p>各モデルについて、3つのSeedで3回学習し、結果の平均値をとった。結果の有意性のチェックはpaired two-sided t-testを使った。大きいデータセットだと0.01の閾値を使った。小さいデータセットだと、0.05の閾値を使った。</p>
<p>Promptの作り方はInParsが使った一般的なやり方と同じ。Queryを生成する際に、最大Token数を32に設定した。</p>
</section>
<section id="データセット" class="level2">
<h2 class="anchored" data-anchor-id="データセット">4 データセット</h2>
<p>InPars[4]の主要結果を再現するために、同じクエリとデータセットを使用した。MS MARCO以外のデータセットは「ir_datasets」というツールを利用して処理した。</p>
<p>InParsの論文で提供したGithubにGPT-3 Curieモデルで生成されたクエリと、それを生成するための文書が提供されている。これにより、GPT-3 CurieとオープンソースモデルGPT-J、BLOOMで生成されたQueryの品質を比較できる。クエリの生成コストがまだ高いため、他のオープンソースモデルの検討は将来の課題である。</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">5 Results</h2>
<section id="main-results" class="level3">
<h3 class="anchored" data-anchor-id="main-results">5.1 Main Results</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230502_inpars_light/images/paste-1.png" class="img-fluid figure-img" alt="main results"></p>
<figcaption class="figure-caption">main results</figcaption>
</figure>
</div>
<p><strong>BM25</strong> この論文は使うフィールドについて少し調整したが、InParsの結果と大きく変わらない。</p>
<p><strong>教師なし学習</strong> 今回使ったDeBERTA-v3-435Mは以前のMonoT5-3Bの性能と同じ。また今回提案したMiniLM-L6-30MはInParsのものT5-220M相当な性能を出している。</p>
<p><strong>Consistency checkingとall-domain pre-training</strong> 両方とも良い影響を与えることがわかる。Deberta-v3-435Mに対してAll-domain pre-trainingが逆効果があるが、理由が不明。</p>
<p><strong>教師あり学習</strong> 今回提案した2つのモデルの性能がいまいち。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230502_inpars_light/images/paste-3.png" class="img-fluid figure-img" alt="model performance"></p>
<figcaption class="figure-caption">model performance</figcaption>
</figure>
</div>
<p>Queryを生成するLLMモデルの比較について、オープンソースのGPT-JとBLOOMはOpen AI Curieよりよい性能を出している。</p>
<p>また、Rerankerについては、Deberta-v3-435MはERNIE-v2-335Mよりよいことがわかる。</p>
</section>
<section id="cost-and-efficiency" class="level3">
<h3 class="anchored" data-anchor-id="cost-and-efficiency">5.2 Cost and Efficiency</h3>
<p>RTX3039を使う場合は：</p>
<ol type="1">
<li>MiniLM-L6-30Mの推論のThroughputは1秒500ドキュメント(LLM各ドキュメントの長さは477キャラクター以下)、そのため、100ドキュメントをRerankする場合は1秒かからない。</li>
<li>MiniLM-L6-30Mを全データセットで前学習しても2時間しかかからない。一方、Deberta-v3-435Mは28時間かかる。</li>
<li>all-domain pre-trainingをする際に、一番時間がかかる操作はMS MARCOのような大きいなデータセットのバリデーションとConsistency checking。Deberta-v3-435MでMS MARCOでのバリデーション時間は6時間、Consistency checkingだと48時間かかった。</li>
<li>Query生成の時間：100kのQueryを生成するためには15時間がかかる。</li>
</ol>
</section>
</section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section"></h2>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Information_retrieval</category>
  <category>paper</category>
  <guid>https://jiang.jp/posts/20230502_inpars_light/index.html</guid>
  <pubDate>Mon, 01 May 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>InPars V2 論文解読</title>
  <link>https://jiang.jp/posts/20230501_inpars_v2/index.html</link>
  <description><![CDATA[ 



<p>InPars V2論文では、Query生成に使用するLLMがGPT3からオープンソースのGPT-J(6B)に変更され、生成したQueryのフィルタリング方法がLog Probabilityからmonot5(3B)をRerankerとして利用する方法に変更された点を挙げている。実験結果としては、V2の精度がV1と比べてわずかに向上したことが報告されている。</p>
<p>論文URL：<a href="https://arxiv.org/abs/2301.01820" class="uri">https://arxiv.org/abs/2301.01820</a></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1 Introduction</h2>
<p>InPars v1とv2の違いは、主に以下の2点：</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Difference</th>
<th>InPars v1</th>
<th>InPars v2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Queryを生成するLLM</td>
<td>GPT3</td>
<td>GPT-J(6B) (オープンソース)</td>
</tr>
<tr class="even">
<td>生成したQueryのフィルタリング方法</td>
<td>生成時のLog Probabilityでフィルタリング</td>
<td>monot5(3B)をRerankerとしてフィルタリング</td>
</tr>
</tbody>
</table>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">2 Methodology</h2>
<p>BEIRの各データセットに対して100kのドキュメントをサンプリングする。MS MARCOからの3つの例を利用してGBQの形式でPromptを作成し、各ドキュメントに対して一個のQueryを生成する。GPT-J(6B)を利用してQueryを生成した。A100一枚で100kのQueryを生成するためには30時間かかる。</p>
<p>フィルタリングについては以前は生成時のLog Probabilityが上位の10kのペアを選んだが、今回はMS-MARCOでFine-tuningしたものT5-3BをRerankerとして使った。100kのQueryとDocumentのペアについて相関度を出して、上位の10kペアを利用した。</p>
<p>Negative sampleはまた各QueryについてBM25で上位1000ドキュメント中で1個ランダム選んだ。</p>
</section>
<section id="result" class="level2">
<h2 class="anchored" data-anchor-id="result">3 Result</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230501_inpars_v2/images/paste-1.png" class="img-fluid figure-img" alt="result" width="500"></p>
<figcaption class="figure-caption">result</figcaption>
</figure>
</div>
<p>実験結果を見ると、v2はv1と比べて精度が少し良くなった(0.006)。</p>


</section>

 ]]></description>
  <category>NLP</category>
  <category>Information_retrieval</category>
  <category>paper</category>
  <guid>https://jiang.jp/posts/20230501_inpars_v2/index.html</guid>
  <pubDate>Sun, 30 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>HyDE 論文解読</title>
  <link>https://jiang.jp/posts/20230430_HyDE/index.html</link>
  <description><![CDATA[ 



<p>HyDE論文では、教師なしのZero-shot dense retrievalシステムを提案。従来のDense retrieverとは異なり、HyDEはQueryから仮想的なDocumentを生成し、その類似度でランキング。InstructGPTで仮想ドキュメントを生成し、Contrieverを使ってEmbeddingに変換。様々なデータセットでテストした結果、HyDEは教師なし領域で従来のContrieverを凌駕し、教師ありモデルとも遜色ない精度を示した。実装はLangChainで利用可能。</p>
<p>論文URL：<a href="https://arxiv.org/abs/2204.07496" class="uri">https://arxiv.org/abs/2204.07496</a></p>
<section id="introduction" class="level1">
<h1>1 Introduction</h1>
<p>Dense retrievalについて様々な研究が行われているが、Zero-shot dense retrievalはまだ難しい。多くの研究はMS-MARCOのような大規模なデータセットを使って転移学習をしているが、MS-MARCOが商用不可の制限があるし、他のドメインに汎化が難しい課題がある。一方、新たなデータをラベリングするためには莫大なコストがかかる。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230430_HyDE/images/paste-1.png" class="img-fluid figure-img" alt="HyDE"></p>
<figcaption class="figure-caption">HyDE</figcaption>
</figure>
</div>
<p>この論文では教師なしのZero-shot dense retrievalの仕組HyDEを提案した。 従来のDense retrieverはQueryとDocumentとの類似度でランクを決めている。HyDEはQueryを利用して、まずLLMでそのQueryを答える仮想なDocumentを生成する。生成したDocumentとDocumentの類似度でランキングしている。</p>
<section id="related-works" class="level2">
<h2 class="anchored" data-anchor-id="related-works">2 Related works</h2>
</section>
<section id="methology" class="level2">
<h2 class="anchored" data-anchor-id="methology">3 Methology</h2>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">4 Experiments</h2>
<p>仮想なDocumentはInstructGPTで生成した。生成したDocumentをContrieverを用いてEmbeddingに変換した。</p>
<p>テストのデータとしては、MS-MARCOをベースとしたTREC DL19 DL20があり、BEIRからもLow-resourceのデータセットをいくつ利用した。また、英語以外、韓国語、日本語等データセットも使った。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230430_HyDE/images/paste-2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">web search query sets</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230430_HyDE/images/paste-4.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">low-resource datasets</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230430_HyDE/images/paste-5.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">non-English retrieval</figcaption>
</figure>
</div>
<p>結果を見ると、教師なしの領域でHyDEは全面的に以前のContrieverを超えた。また、教師あるのモデルから比較しても遜色しない精度を出した。</p>
</section>
<section id="analysis" class="level2">
<h2 class="anchored" data-anchor-id="analysis">5 Analysis</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230430_HyDE/images/paste-6.png" class="img-fluid figure-img" alt="LLM difference" width="300"></p>
<figcaption class="figure-caption">LLM difference</figcaption>
</figure>
</div>
<p>当たり前だが、仮想なドキュメントを生成するLLMによって最終の精度が違う。また、HyDEは教師なしの手法だが、教師ありのRetrieverの精度も向上できる。</p>
</section>
<section id="実装" class="level2">
<h2 class="anchored" data-anchor-id="実装">実装</h2>
<p>HyDEはすでに<a href="https://python.langchain.com/en/latest/modules/chains/index_examples/hyde.html">LangChain</a>で実装されている。</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.llms <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAI</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.embeddings <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAIEmbeddings</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.chains <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> HypotheticalDocumentEmbedder</span>
<span id="cb1-4"></span>
<span id="cb1-5">base_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OpenAIEmbeddings()</span>
<span id="cb1-6">llm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OpenAI()</span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load with `web_search` prompt</span></span>
<span id="cb1-9">embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> HypotheticalDocumentEmbedder.from_llm(llm, base_embeddings, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"web_search"</span>)</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Now we can use it as any embedding class!</span></span>
<span id="cb1-12">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> embeddings.embed_query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Where is the Taj Mahal?"</span>)</span></code></pre></div>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Information_retrieval</category>
  <category>paper</category>
  <guid>https://jiang.jp/posts/20230430_HyDE/index.html</guid>
  <pubDate>Sat, 29 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Promptagator 論文解読</title>
  <link>https://jiang.jp/posts/20230429_promptagator/index.html</link>
  <description><![CDATA[ 



<p>Promptagator論文では、Few-shot Retrieval settingを提案し、異なる検索タスクに対応するために専用のPromptを作成してLLMでDocumentに関連するQueryを生成。生成されたQueryをフィルタリングし、Retrieverを学習させる。実験結果から、Promptagatorは既存の50万件以上の学習データを使ったモデルより高い精度を達成し、教師ありのRetrieverよりも優れていることが分かった。</p>
<p>論文URL：<a href="https://arxiv.org/abs/2209.11755" class="uri">https://arxiv.org/abs/2209.11755</a></p>
<section id="introduction" class="level1">
<h1>1 Introduction</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230429_promptagator/images/paste-7.png" class="img-fluid figure-img" alt="Few-shot retrieval with PROMPTAGATOR" width="700"></p>
<figcaption class="figure-caption">Few-shot retrieval with PROMPTAGATOR</figcaption>
</figure>
</div>
<p>BEIRのような異なる種類のデータセットによって構成されるベンチマークがある。特定のデータセットで学習したモデルは他領域への汎化が難しい。その理由は主に2つある。</p>
<p>まず、異なるタスクはそれぞれ検索意図が違う。例えば、図1の左側に示されているように、Dbpedia-Entityはクエリで言及されたエンティティを検索するタスクであり、FEVERは与えられた声明を支持または反証する証拠を検索するタスクである。</p>
<p>また、検索意図が類似していても、データの分布が異なることがある。例えば、HotpotQAのように違う領域の質問もあるし、FiQAのようにファイナンシャル専門の質問もある。</p>
<p>これまでに、情報検索領域でのLLMの活用に関する論文が複数ある。例えば、GPT3のEmbedingをDual encoderモデルに適用する論文がある。ただし、GPT3のEmbeddingの次元数が12ｋとなっており、推論時の計算コストが高い。また、以前紹介したGPT3でQueryを作って、T5のRerankerを学習したInParsもある。一方、その論文はRetrieverをBM25を使っている。</p>
<p>この論文は以下の貢献がある：</p>
<ol type="1">
<li>異なる検索タスクの意図とQueryの分布を分析し、Few-shot Retrieval settingを提案した。</li>
<li>Promptgatorを提案した。2~8個の例を使えば既往の50万以上の例で学習したモデルより高い精度を達成できている。</li>
</ol>
</section>
<section id="few-shot-retrieval-task" class="level1">
<h1>2 Few-shot retrieval task</h1>
<p>この節でFew-shot retrievalを紹介した。</p>
</section>
<section id="promptagator" class="level1">
<h1>3 Promptagator</h1>
<p>Promptagatorは3つのステップで実現される。</p>
<ol type="1">
<li>各タスクに対して専用なPromptを作成し、LLMでDocumentに関連するQueryを作る</li>
<li>Retrieverを用いて、生成したQueryと生成元のDocumentを検索できない場合は、それを除外する</li>
<li>除外されていないQueryとDocumentのペアでRetrieverを学習させる。</li>
</ol>
<section id="prompt-base-query-generation" class="level2">
<h2 class="anchored" data-anchor-id="prompt-base-query-generation">3.1 Prompt-base query generation</h2>
<p>LLMはFLANを利用した。Promptの形式としては、HotpotQAを例として説明すると以下になる。</p>
<pre class="raw text"><code>Evidence: passage 1 
Vexed question: query 1
...
Evidence: passage k
Vexed question: query k
Evidence: target passage</code></pre>
<p>下表のようにタスクごとに違うPromptを設定した。（Promptの中の0と 1の意味が不明）</p>
<p><img src="https://jiang.jp/posts/20230429_promptagator/images/paste-8.png" class="img-fluid" alt="Prompt template for each dataset"><br>
Promptで使用した例は最大8個にし、例の長さによって調整している。文書が長い場合は、必要に応じて切断している。</p>
<p>各コーパスから最大100万のドキュメントを抽出し、各ドキュメントで8個のQueryを生成している。LLMはFLAN137Bを使った。生成する際に0.7の温度を使った。</p>
</section>
<section id="consistency-filtering-using-only-generated-data" class="level2">
<h2 class="anchored" data-anchor-id="consistency-filtering-using-only-generated-data">3.2 Consistency filtering using only generated data</h2>
<p>生成したQueryに対して、生成元がその答案を含む必要がある。今までの研究で、その原則で生成したQueryをフィルタリングすることは重要であることがしめされている。</p>
<p>過去の研究の中で外部の質問応答モデルを用いて実現していたが、この研究では生成したデータで初期のRetrieverを学習されている。各生成したQueryに対して、Retrieverが検索したTopKの中に生成元のドキュメントが含まれていない場合は、そのQueryを除外する。</p>
</section>
<section id="few-shot-promptagator-retriever" class="level2">
<h2 class="anchored" data-anchor-id="few-shot-promptagator-retriever">3.3 Few-shot promptagator retriever</h2>
<p>Dural EncoderのRetrieverを利用している。ベースモデルはT5で、それをC4（Common Crawlのweb crawlコーパス）データセットを使って、Contriverが使用したindependent cropping taskでさらに学習させた。（independent cropping taskとは、同じ文書の異なる部分のペアをPositive example、異なる文書のテキストのペアをNegative exampleとして、教師なしでRetrieverを学習する手法）</p>
<p>その後、生成されたQueryとDocumentのペアを使って継続的に学習させる。学習時にBatch内のQueryとDocumentのペアをシャッフルしてNegative exampleとする。また、一定のStep数を学習した後、それを初期のRetrieverとして生成されたQueryのフィルタリングを行う。フィルタリングした後、継続的に学習させる。</p>
<p>また、Promptagator++というRerankerも提案した。学習データがRetrieverと同じだが、モデルはもっと精度が高く、推論時間が長いCross-attention modelを使った。Retrieverから取得した上位の200件のDocumentから31個のDocumentをサンプリングして、Negative exampleとして使っている。</p>
</section>
<section id="zero-shot-promptagator-retriever" class="level2">
<h2 class="anchored" data-anchor-id="zero-shot-promptagator-retriever">3.4 Zero-shot promptagator retriever</h2>
<p>Zero-shotでQueryを生成する場合は以下の形式でPromptを書いた：</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>d<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> Read the passage and generate a query.'</span></span></code></pre></div>
</section>
</section>
<section id="experiments" class="level1">
<h1>4 Experiments</h1>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">4.1 Implementation</h2>
<p>Queryを生成する際に温度を0.7にした。</p>
<p>生成したQueryをフィルタリングする際にKを1にした。</p>
<p>Dual Encodersは同じT5-base v1.1 のEncoder(110M)を使っている。Encoderのトップの層を平均し、768次元のEmbeddingへ投影した。</p>
<p>Promptagator++のRerankerもT5-base v1.1 のEncoder(110M)を使っているが、Cross AttentionのEncoderにしている。</p>
<p>Fine-tuningする際にの具体的なBatch sizeとStepsが下表の通り：</p>
<table class="table">
<thead>
<tr class="header">
<th>Model type</th>
<th>Dataset size</th>
<th>Batch size</th>
<th>Fine tune steps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dual encoder</td>
<td>Big(&gt;500k)</td>
<td>6k</td>
<td>5k</td>
</tr>
<tr class="even">
<td>Dual encoder</td>
<td>Small(&lt;=500k)</td>
<td>128</td>
<td>1k</td>
</tr>
<tr class="odd">
<td>Reranker</td>
<td>Big(&gt;500k)</td>
<td>64</td>
<td>20k</td>
</tr>
<tr class="even">
<td>Reranker</td>
<td>Small(&lt;=500k)</td>
<td>64</td>
<td>5k</td>
</tr>
</tbody>
</table>
</section>
<section id="main-results" class="level2">
<h2 class="anchored" data-anchor-id="main-results">4.2 Main Results</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230429_promptagator/images/paste-9.png" class="img-fluid figure-img" alt="Main result"></p>
<figcaption class="figure-caption">Main result</figcaption>
</figure>
</div>
<p>表の前半ではRetrieverの比較が行われている。Zero-shotのPromptagatorはすでに大多数のMS MARCOでFine-tuningした教師ありのRetrieverと同等な精度を出している。Few-shotのPromptagatorはさらに教師ありのRetrieverより高い精度を出している。</p>
<p>後半ではRetriever+Rerankerの組み合わせの比較になる。Retrieverと同じ傾向で、Zero-shotでかなり良い精度を出している。Few-shotになるとさらに更に精度が3%向上し、Sotaになっている。</p>
<p>また、Promptagatorのもう一つ優れている点はモデルのサイズである。他のモデルは大体3Bの大きさだが、Promptagatorはわずか110Mのみである。</p>
</section>
<section id="abalation-study" class="level2">
<h2 class="anchored" data-anchor-id="abalation-study">4.3 Abalation Study</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230429_promptagator/images/paste-10.png" class="img-fluid figure-img" alt="Abalation study"></p>
<figcaption class="figure-caption">Abalation study</figcaption>
</figure>
</div>
<p><strong>Queryフィルタリングの効果</strong> Figure2の左図は、Queryを一回フィルタリングした効果をしめしている。大多数のデータセットにとって、Queryフィルタリングが有効だが、逆効果のものも存在する。NFCorpus and SciFactは小さいデータセットなので、フィルタリングで過学習している可能性がある。</p>
<p>また、詳細にフィルタリングされた例をみると、多くケースはQueryは一般化過ぎて多数のドキュメントにマッチングされていること、もしくは単純にQueryが間違っていることがわかる。</p>
<p><strong>生成したQueryで人間のデータを代替することができるか？</strong> Figure2の真ん中の図は、8-shotのPromptagatorは5万件の人間がラベリングしたデータと同じ効果であることを示している。</p>
<p><strong>PromptagatorのQuery生成が効いているか？</strong> Figure2の右図のGenQはBEIR論文の中で提案されたモデル、NQ-QGenはこの論文提案した方法でNQデータセット学習したモデル、NQ-QGenとGenQの違いはQuery生成の部分のみ。NQ-QGenの精度は2.7%高いため、提案したQuery生成の方法が有効だと言える。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230429_promptagator/images/paste-6.png" class="img-fluid figure-img" alt="Impact of FLAN"></p>
<figcaption class="figure-caption">Impact of FLAN</figcaption>
</figure>
</div>
<p><strong>FLANの影響</strong> PromptagatorのLLMはFLANを利用している。FLANの学習データの中にNQとQuaroデータが含まれいている。その影響を検証するため､それらを除いたデータセットでFLANを学習し、その結果を比較した(さすがGoogle Research、金ならあるの感じ)。その結果、精度は若干低下したが、以前の研究よりは高い精度を達成している。</p>
</section>
<section id="qualitative-analysis" class="level2">
<h2 class="anchored" data-anchor-id="qualitative-analysis">4.4 Qualitative Analysis</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230429_promptagator/images/paste-5.png" class="img-fluid figure-img" alt="Top word distribution"></p>
<figcaption class="figure-caption">Top word distribution</figcaption>
</figure>
</div>
<p>Queryの最初のWordの分布を調査した。Few-shotが生成したQueryの分布は実際のQueryの分布と同じであることがわかる。</p>
</section>
</section>
<section id="related-work" class="level1">
<h1>5 Related Work</h1>
<section id="neural-retrieval-models" class="level2">
<h2 class="anchored" data-anchor-id="neural-retrieval-models"><strong>Neural retrieval models</strong></h2>
<p>Neural retrieval modelはrepresentation based modelとinteraction based modelの2つに分類できる。</p>
<p>Representation based modelはQueryとDocument両方とも分散表現に変換し、分散表現の類似度で相関性を測っている。最近の研究は以下のことにフォーカスしている：</p>
<ol type="1">
<li>より良い前学習するタスクやモデルのアーキテクチャを開発する</li>
<li>Multi-Vectorでより分散表現の表現性能を向上させる</li>
<li>よりよいNegative sample手法を開発する</li>
<li>ドメイン横断での汎化性能を向上させる</li>
</ol>
<p>Interaction based modelだと、QueryとDocumentを一緒に処理するため、精度が高い。一方、計算コストが高いため、Rerankerとして使われることが多い。それに関する研究は以下のものがある：</p>
<ol type="1">
<li>Interaction based modelを蒸留し、Representation based modelとして使う。</li>
<li>Interactionを最後にさせること。</li>
</ol>
</section>
<section id="prompt-based-query-generation" class="level2">
<h2 class="anchored" data-anchor-id="prompt-based-query-generation">Prompt-based query generation</h2>
<p>UPR：直接LLMSを使ってRerankを行う</p>
<p>InPars：GPT3でQueryを生成し、T5のRerankerを学習する</p>
</section>
<section id="retrievers-with-late-interactions" class="level2">
<h2 class="anchored" data-anchor-id="retrievers-with-late-interactions">Retrievers with late interactions</h2>
<p>Dual encoder modelの効率が良いが、QueryとDocumentの相互作用は最後の内積のみであるため、性能が弱い。ColBERTとSPLADEは最後のToken-level interactionを使ったため、計算コストを少し犠牲して性能を向上させた。</p>
</section>
</section>
<section id="conclution-and-discussions" class="level1">
<h1>6 Conclution and discussions</h1>
<p>この論文はPromptagatorを提案した。一方、未解決の問題はいかのように存在する：</p>
<ol type="1">
<li>効率的に生成したデータを利用する手法が必要。</li>
<li>PromptとRetrieverの性能との関係。</li>
<li>LLMの情報を小さいモデルを転移すること。</li>
</ol>


</section>

 ]]></description>
  <category>NLP</category>
  <category>Information_retrieval</category>
  <category>paper</category>
  <guid>https://jiang.jp/posts/20230429_promptagator/index.html</guid>
  <pubDate>Fri, 28 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>InPars 論文解読</title>
  <link>https://jiang.jp/posts/20230428_inpars/index.html</link>
  <description><![CDATA[ 



<p>InPars論文では、擬似データ（Pseudo data）生成によるランキングモデル学習手法を提案。LLMを用いて擬似データを生成し、それを使ってモデルを学習させることで、情報検索（IR）の精度を向上させる。実験では、生成された擬似データでMonoT5をFine-tuningし、結果として従来のUnsupervisedモデルより優れた性能を示した。</p>
<p>論文URL：<a href="https://arxiv.org/abs/2202.05144" class="uri">https://arxiv.org/abs/2202.05144</a></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1. Introduction</h2>
<p>LLMは高性能を誇るものの、情報検索（IR）への応用が制限されている理由として、大規模な計算量が必要であることと、コストが高いことが挙げられる。GPT-3の埋め込みサービスを利用する場合、すべてのテキストを少なくとも1回は処理する必要があり、件数が多い場合、コストが膨大になることが問題となる。</p>
<p>また、学習データにも課題が存在し、現存するデータが商用利用に適さないものが多く、また、既存のデータを用いて学習したモデルが他の領域に汎用性を持たないという問題がある。</p>
<p>この論文では、検索推論にLLMを直接使用するのではなく、LLMを用いて擬似データ（Pseudo data）を生成し、そのデータを使ってランキングモデルを学習する手法を提案している。</p>
</section>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">2. Related work</h2>
<p>これまで情報検索（IR）領域におけるデータ生成の研究では、BM25を用いて類似度が高いドキュメントをペアとしてモデルを学習する研究が存在する。また、教師ありのモデルでテキストからQueryを生成し、それとテキストのペアを学習データとする研究もあった。</p>
<p>この研究の特徴として、モデルを学習させなく、LLMからFew-shotでデータを生成したこと（ちょっと新奇性が足りていない気がする）。</p>
</section>
<section id="our-method-inpars" class="level2">
<h2 class="anchored" data-anchor-id="our-method-inpars">3. Our Method: InPars</h2>
<p>以下はInParsのステップ：</p>
<ol type="1">
<li><p>複数のドキュメントとクエリーのペアが存在すると仮定する。それを例として、Few-shotでLLMでドキュメントに関連するクエリーを生成してもらう。(</p>
<p>詳細は4.2で紹介する）</p></li>
<li><p>生成したドキュメントとクエリーのペアについて、LLMが出力する際に出している結果のLog probabilityが上位のものを選択する。ちなみに、このステップが大きく精度を改善した。</p></li>
<li><p>生成したqとdのペアを学習データとしてRerankのモデルをFine-tuningする。（詳細は4.3で紹介する）</p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230428_inpars/images/paste-1.png" class="img-fluid figure-img" alt="InPars "></p>
<figcaption class="figure-caption">InPars</figcaption>
</figure>
</div>
</section>
<section id="experimental-setup" class="level2">
<h2 class="anchored" data-anchor-id="experimental-setup">4. Experimental Setup</h2>
<section id="datasets" class="level3">
<h3 class="anchored" data-anchor-id="datasets">4.1 Datasets</h3>
<p>今回使用したデータセットは以下：</p>
<ul>
<li><p>MS MARCO：Microsoftが出したBingの実際のユーザログをベースとした大型データセット。880万のドキュメントと50万のドキュメントとクエリーのペアがある。各クエリーが平均的に1ドキュメントに対応する。</p></li>
<li><p>TREC-DL：MS MARCOと同じドキュメントを持っているが、クエリーは54件のみである。また、各クエリーについてアノテーションしたドキュメントが多い。</p></li>
<li><p>Robust04：新聞領域のデータセット。52万のドキュメントがあり、249クエリーがある。各クエリーに対して、平均的に1250件のドキュメントをアノテーションした。</p></li>
<li><p>Natural Questions：260万件のWikipediaテキストをベースとしたQuestion Answer データセット。QuestionはGoogleの検索エンジンのログから作ったもの。</p></li>
<li><p>TREC-COVID：コロナの情報に関するデータセット</p></li>
</ul>
</section>
<section id="training-data-generation" class="level3">
<h3 class="anchored" data-anchor-id="training-data-generation">4.2 Training Data Generation</h3>
<p>各学習データは（Query, Positive document, Negative document）のTripleによって構成させる。その生成のステップは以下：</p>
<ul>
<li><p>10万のドキュメントをサンプリングし、GPT3のCurieでQueryを生成させる。</p></li>
<li><p>最終的にLog probabilityが上位の1万件のペアのみ学習データとして使う。</p></li>
<li><p>BM25で検索した1000件の中でランダムに1件を抽出し、それをNegative Documentとする。（このやり方で多くのノイズを入れてしまうのでは？）</p></li>
</ul>
<p>以下は２点の補足：</p>
<ul>
<li><p>生成する際に温度とTopーPのパラメータ設定は結果に有意の影響しない。</p></li>
<li><p>長さが300文字のドキュメントは捨てられる。</p></li>
</ul>
<p>Query生成する際にPromptの書き方は2つを利用した（Figure２)：</p>
<ol type="1">
<li>一般方法（Vanilla)：MS MARCOからランダムに3つのデータを抽出し、それを例として、FewーshotでQueryを生成させる。</li>
<li>GBQ（Guided by Bad Questions）：一般方法と同じように、MS MARCOからランダムに3つのデータを抽出する。しかし、MS MARCOのQueryが漠然すぎるため、それをBad questionとして提示する。Documentを読んで、より関連するGood Questionを手動で作った。（Document, Good Question, Bad Question)で例を提示し、生成したGood QuestionをQueryとする。</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230428_inpars/images/paste-2.png" class="img-fluid figure-img" alt="Prompt example"></p>
<figcaption class="figure-caption">Prompt example</figcaption>
</figure>
</div>
</section>
<section id="retrieval-methods" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-methods">4.3 Retrieval Methods</h3>
<p>２段階の検索を採用している。まずBM25で上位1000のドキュメントを取り出す。その次、MonoT5を使ってRerankingをする。</p>
<p>MonoT5はTransformerのEncoderとDecoder両方とも使っているモデルで、Cross-Encoderモデルである。今回の実験では、サイズは220Mと3Bのモデルでテストした。</p>
<p>各データセットにおいて作成された擬似データでMonoT5をFine-tuningした。</p>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">5 Results</h2>
<p><img src="https://jiang.jp/posts/20230428_inpars/images/Snipaste_2023-05-01_09-49-20.png" class="img-fluid"></p>
<p>7,8行目を見ると、BM25やContriever等の以前のUnsupervised結果より優れていることがわかる。また、16行目はMS MARCOでFine-tuningした後さらに擬似データでFine-tuningした結果。幾つかのデータセットで単純にMS MARCOでFine-tuningするより良い結果が出ている。</p>
</section>
<section id="ablation-study-and-analysis" class="level2">
<h2 class="anchored" data-anchor-id="ablation-study-and-analysis">6 Ablation Study and Analysis</h2>
<p>6.1 Prompt Selection and Source Corpus</p>
<p>比較対象が混乱のため、何が言いたいかがわからなかった。</p>
<p>6.2 Model Size Impact on IR Metrics</p>
<p>当たり前だけど、Questionを生成するモデルのサイズが大きいほど結果がよくなる。</p>
<p>6.3 Filtering by the Most Likely Questions</p>
<p>Top1万件のデータを利用することにより精度が向上した。</p>
<p>6.4 Was GPT-3 Trained on Supervised IR Data?</p>
<p>生成したQuestionがどのぐらい実際のMS MARCOデータにあるかで、GPT3の学習データにMS MARCOのデータを含まれているかを検証した。結果としては多くても10％前後のため影響がない。（検証のやり方が正しいかが疑問）</p>
</section>
<section id="conclusion-and-future-work" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-and-future-work">7 Conclusion and Future Work</h2>
<p>本文は、LLMを利用してQueryを生成して、生成したQueryとドキュメントのペアを学習データでモデルを学習する手法を提案した。</p>
<p>今後の改善点としては、</p>
<ol type="1">
<li>擬似データでDense RetrieverをFine-tuningする（今回はRerankerのみ）</li>
<li>データを生成する際に作った”BAD question”をNegative exampleとして使用する</li>
<li>擬似データの数を増やす</li>
<li>（Query, Document)のペアを探すもっと良い手法を開発する</li>
</ol>


</section>

 ]]></description>
  <category>NLP</category>
  <category>Information_retrieval</category>
  <category>paper</category>
  <guid>https://jiang.jp/posts/20230428_inpars/index.html</guid>
  <pubDate>Fri, 28 Apr 2023 15:00:00 GMT</pubDate>
</item>
</channel>
</rss>
