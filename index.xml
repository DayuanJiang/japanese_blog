<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>blog</title>
<link>https://jiang.jp/index.html</link>
<atom:link href="https://jiang.jp/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.3.340</generator>
<lastBuildDate>Sat, 29 Apr 2023 15:00:00 GMT</lastBuildDate>
<item>
  <title>HyDE 論文解読</title>
  <link>https://jiang.jp/posts/20230430_HyDE/index.html</link>
  <description><![CDATA[ 



<p>論文URL：<a href="https://arxiv.org/abs/2204.07496" class="uri">https://arxiv.org/abs/2204.07496</a></p>
<section id="introduction" class="level1">
<h1>1 Introduction</h1>
<p>Dense retrievalについて様々な研究が行われているが、Zero-shot dense retrievalはまだ難しい。多くの研究はMS-MARCOのような大規模なデータセットを使って転移学習をしているが、MS-MARCOが商用不可の制限があるし、他のドメインに汎化が難しい課題がある。一方、新たなデータをラベリングするためには莫大なコストがかかる。</p>
<p><img src="https://jiang.jp/posts/20230430_HyDE/images/paste-1.png" class="img-fluid"></p>
<p>この論文では教師なしのZero-shot dense retrievalの仕組HyDEを提案した。 従来のDense retrieverはQueryとDocumentとの類似度でランクを決めている。HyDEはQueryを利用して、まずLLMでそのQueryを答える仮想なDocumentを生成する。生成したDocumentとDocumentの類似度でランキングしている。</p>
<section id="related-works" class="level2">
<h2 class="anchored" data-anchor-id="related-works">2 Related works</h2>
</section>
<section id="methology" class="level2">
<h2 class="anchored" data-anchor-id="methology">3 Methology</h2>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">4 Experiments</h2>
<p>仮想なDocumentはInstructGPTで生成した。生成したDocumentをContrieverを用いてEmbeddingに変換した。</p>
<p>テストのデータとしては、MS-MARCOをベースとしたTREC DL19 DL20があり、BEIRからもLow-resourceのデータセットをいくつ利用した。また、英語以外、韓国語、日本語等データセットも使った。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230430_HyDE/images/paste-2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">web search query sets</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230430_HyDE/images/paste-4.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">low-resource datasets</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jiang.jp/posts/20230430_HyDE/images/paste-5.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">non-English retrieval</figcaption>
</figure>
</div>
<p>結果を見ると、教師なしの領域でHyDEは全面的に以前のContrieverを超えた。また、教師あるのモデルから比較しても遜色しない精度を出した。</p>
</section>
<section id="analysis" class="level2">
<h2 class="anchored" data-anchor-id="analysis">5 Analysis</h2>
<p><img src="https://jiang.jp/posts/20230430_HyDE/images/paste-6.png" class="img-fluid" width="300"></p>
<p>当たり前だが、仮想なドキュメントを生成するLLMによって最終の精度が違う。また、HyDEは教師なしの手法だが、教師ありのRetrieverの精度も向上できる。</p>
</section>
<section id="実装" class="level2">
<h2 class="anchored" data-anchor-id="実装">実装</h2>
<p>HyDEはすでに<a href="https://python.langchain.com/en/latest/modules/chains/index_examples/hyde.html">LangChain</a>で実装されている。</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.llms <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAI</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.embeddings <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAIEmbeddings</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.chains <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> HypotheticalDocumentEmbedder</span>
<span id="cb1-4"></span>
<span id="cb1-5">base_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OpenAIEmbeddings()</span>
<span id="cb1-6">llm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OpenAI()</span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load with `web_search` prompt</span></span>
<span id="cb1-9">embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> HypotheticalDocumentEmbedder.from_llm(llm, base_embeddings, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"web_search"</span>)</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Now we can use it as any embedding class!</span></span>
<span id="cb1-12">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> embeddings.embed_query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Where is the Taj Mahal?"</span>)</span></code></pre></div>


</section>
</section>

 ]]></description>
  <category>NLP</category>
  <category>Information_retrieval</category>
  <category>paper</category>
  <guid>https://jiang.jp/posts/20230430_HyDE/index.html</guid>
  <pubDate>Sat, 29 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Promptagator 論文解読</title>
  <link>https://jiang.jp/posts/20230429_promptagator/index.html</link>
  <description><![CDATA[ 



<p>論文URL：<a href="https://arxiv.org/abs/2209.11755" class="uri">https://arxiv.org/abs/2209.11755</a></p>
<section id="introduction" class="level1">
<h1>1 Introduction</h1>
<p><img src="https://jiang.jp/posts/20230429_promptagator/images/Untitled.png" class="img-fluid"></p>
<p>BEIRのような異なる種類のデータセットによって構成されるベンチマークがある。特定のデータセットで学習したモデルは他領域への汎化が難しい。その理由は主に2つある。</p>
<p>まず、異なるタスクはそれぞれ検索意図が違う。例えば、図1の左側に示されているように、Dbpedia-Entityはクエリで言及されたエンティティを検索するタスクであり、FEVERは与えられた声明を支持または反証する証拠を検索するタスクである。</p>
<p>また、検索意図が類似していても、データの分布が異なることがある。例えば、HotpotQAのように違う領域の質問もあるし、FiQAのようにファイナンシャル専門の質問もある。</p>
<p>これまでに、情報検索領域でのLLMの活用に関する論文が複数ある。例えば、GPT3のEmbedingをDual encoderモデルに適用する論文がある。ただし、GPT3のEmbeddingの次元数が12ｋとなっており、推論時の計算コストが高い。また、以前紹介したGPT3でQueryを作って、T5のRerankerを学習したInParsもある。一方、その論文はRetrieverをBM25を使っている。</p>
<p>この論文は以下の貢献がある：</p>
<ol type="1">
<li>異なる検索タスクの意図とQueryの分布を分析し、Few-shot Retrieval settingを提案した。</li>
<li>Promptgatorを提案した。2~8個の例を使えば既往の50万以上の例で学習したモデルより高い精度を達成できている。</li>
</ol>
</section>
<section id="few-shot-retrieval-task" class="level1">
<h1>2 Few-shot retrieval task</h1>
<p>この節でFew-shot retrievalを紹介した。</p>
</section>
<section id="promptagator" class="level1">
<h1>3 Promptagator</h1>
<p>Promptagatorは3つのステップで実現される。</p>
<ol type="1">
<li>各タスクに対して専用なPromptを作成し、LLMでDocumentに関連するQueryを作る</li>
<li>Retrieverを用いて、生成したQueryと生成元のDocumentを検索できない場合は、それを除外する</li>
<li>除外されていないQueryとDocumentのペアでRetrieverを学習させる。</li>
</ol>
<section id="prompt-base-query-generation" class="level2">
<h2 class="anchored" data-anchor-id="prompt-base-query-generation">3.1 Prompt-base query generation</h2>
<p>LLMはFLANを利用した。Promptの形式としては、HotpotQAを例として説明すると以下になる。</p>
<pre class="raw text"><code>Evidence: passage 1 
Vexed question: query 1
...
Evidence: passage k
Vexed question: query k
Evidence: target passage</code></pre>
<p>下表のようにタスクごとに違うPromptを設定した。（Promptの中の0と 1の意味が不明）</p>
<p><img src="https://jiang.jp/posts/20230429_promptagator/images/Untitled 1.png" class="img-fluid"></p>
<p>Promptで使用した例は最大8個にし、例の長さによって調整している。文書が長い場合は、必要に応じて切断している。</p>
<p>各コーパスから最大100万のドキュメントを抽出し、各ドキュメントで8個のQueryを生成している。LLMはFLAN137Bを使った。生成する際に0.7の温度を使った。</p>
</section>
<section id="consistency-filtering-using-only-generated-data" class="level2">
<h2 class="anchored" data-anchor-id="consistency-filtering-using-only-generated-data">3.2 Consistency filtering using only generated data</h2>
<p>生成したQueryに対して、生成元がその答案を含む必要がある。今までの研究で、その原則で生成したQueryをフィルタリングすることは重要であることがしめされている。</p>
<p>過去の研究の中で外部の質問応答モデルを用いて実現していたが、この研究では生成したデータで初期のRetrieverを学習されている。各生成したQueryに対して、Retrieverが検索したTopKの中に生成元のドキュメントが含まれていない場合は、そのQueryを除外する。</p>
</section>
<section id="few-shot-promptagator-retriever" class="level2">
<h2 class="anchored" data-anchor-id="few-shot-promptagator-retriever">3.3 Few-shot promptagator retriever</h2>
<p>Dural EncoderのRetrieverを利用している。ベースモデルはT5で、それをC4（Common Crawlのweb crawlコーパス）データセットを使って、Contriverが使用したindependent cropping taskでさらに学習させた。（independent cropping taskとは、同じ文書の異なる部分のペアをPositive example、異なる文書のテキストのペアをNegative exampleとして、教師なしでRetrieverを学習する手法）</p>
<p>その後、生成されたQueryとDocumentのペアを使って継続的に学習させる。学習時にBatch内のQueryとDocumentのペアをシャッフルしてNegative exampleとする。また、一定のStep数を学習した後、それを初期のRetrieverとして生成されたQueryのフィルタリングを行う。フィルタリングした後、継続的に学習させる。</p>
<p>また、Promptagator++というRerankerも提案した。学習データがRetrieverと同じだが、モデルはもっと精度が高く、推論時間が長いCross-attention modelを使った。Retrieverから取得した上位の200件のDocumentから31個のDocumentをサンプリングして、Negative exampleとして使っている。</p>
</section>
<section id="zero-shot-promptagator-retriever" class="level2">
<h2 class="anchored" data-anchor-id="zero-shot-promptagator-retriever">3.4 Zero-shot promptagator retriever</h2>
<p>Zero-shotでQueryを生成する場合は以下の形式でPromptを書いた：</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>d<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> Read the passage and generate a query.'</span></span></code></pre></div>
</section>
</section>
<section id="experiments" class="level1">
<h1>4 Experiments</h1>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">4.1 Implementation</h2>
<p>Queryを生成する際に温度を0.7にした。</p>
<p>生成したQueryをフィルタリングする際にKを1にした。</p>
<p>Dual Encodersは同じT5-base v1.1 のEncoder(110M)を使っている。Encoderのトップの層を平均し、768次元のEmbeddingへ投影した。</p>
<p>Promptagator++のRerankerもT5-base v1.1 のEncoder(110M)を使っているが、Cross AttentionのEncoderにしている。</p>
<p>Fine-tuningする際にの具体的なBatch sizeとStepsが下表の通り：</p>
<table class="table">
<thead>
<tr class="header">
<th>Model type</th>
<th>Dataset size</th>
<th>Batch size</th>
<th>Fine tune steps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dual encoder</td>
<td>Big(&gt;500k)</td>
<td>6k</td>
<td>5k</td>
</tr>
<tr class="even">
<td>Dual encoder</td>
<td>Small(&lt;=500k)</td>
<td>128</td>
<td>1k</td>
</tr>
<tr class="odd">
<td>Reranker</td>
<td>Big(&gt;500k)</td>
<td>64</td>
<td>20k</td>
</tr>
<tr class="even">
<td>Reranker</td>
<td>Small(&lt;=500k)</td>
<td>64</td>
<td>5k</td>
</tr>
</tbody>
</table>
</section>
<section id="main-results" class="level2">
<h2 class="anchored" data-anchor-id="main-results">4.2 Main Results</h2>
<p><img src="https://jiang.jp/posts/20230429_promptagator/images/paste-1.png" class="img-fluid"></p>
<p>表の前半ではRetrieverの比較が行われている。Zero-shotのPromptagatorはすでに大多数のMS MARCOでFine-tuningした教師ありのRetrieverと同等な精度を出している。Few-shotのPromptagatorはさらに教師ありのRetrieverより高い精度を出している。</p>
<p>後半ではRetriever+Rerankerの組み合わせの比較になる。Retrieverと同じ傾向で、Zero-shotでかなり良い精度を出している。Few-shotになるとさらに更に精度が3%向上し、Sotaになっている。</p>
<p>また、Promptagatorのもう一つ優れている点はモデルのサイズである。他のモデルは大体3Bの大きさだが、Promptagatorはわずか110Mのみである。</p>
</section>
<section id="abalation-study" class="level2">
<h2 class="anchored" data-anchor-id="abalation-study">4.3 Abalation Study</h2>
<p><img src="https://jiang.jp/posts/20230429_promptagator/images/paste-2.png" class="img-fluid"></p>
<p><strong>Queryフィルタリングの効果</strong> Figure2の左図は、Queryを一回フィルタリングした効果をしめしている。大多数のデータセットにとって、Queryフィルタリングが有効だが、逆効果のものも存在する。NFCorpus and SciFactは小さいデータセットなので、フィルタリングで過学習している可能性がある。</p>
<p>また、詳細にフィルタリングされた例をみると、多くケースはQueryは一般化過ぎて多数のドキュメントにマッチングされていること、もしくは単純にQueryが間違っていることがわかる。</p>
<p><strong>生成したQueryで人間のデータを代替することができるか？</strong> Figure2の真ん中の図は、8-shotのPromptagatorは5万件の人間がラベリングしたデータと同じ効果であることを示している。</p>
<p><strong>PromptagatorのQuery生成が効いているか？</strong> Figure2の右図のGenQはBEIR論文の中で提案されたモデル、NQ-QGenはこの論文提案した方法でNQデータセット学習したモデル、NQ-QGenとGenQの違いはQuery生成の部分のみ。NQ-QGenの精度は2.7%高いため、提案したQuery生成の方法が有効だと言える。</p>
<p><img src="https://jiang.jp/posts/20230429_promptagator/images/paste-6.png" class="img-fluid"></p>
<p><strong>FLANの影響</strong> PromptagatorのLLMはFLANを利用している。FLANの学習データの中にNQとQuaroデータが含まれいている。その影響を検証するため､それらを除いたデータセットでFLANを学習し、その結果を比較した(さすがGoogle Research、金ならあるの感じ)。その結果、精度は若干低下したが、以前の研究よりは高い精度を達成している。</p>
</section>
<section id="qualitative-analysis" class="level2">
<h2 class="anchored" data-anchor-id="qualitative-analysis">4.4 Qualitative Analysis</h2>
<p><img src="https://jiang.jp/posts/20230429_promptagator/images/paste-5.png" class="img-fluid"></p>
<p>Queryの最初のWordの分布を調査した。Few-shotが生成したQueryの分布は実際のQueryの分布と同じであることがわかる。</p>
</section>
</section>
<section id="related-work" class="level1">
<h1>5 Related Work</h1>
<section id="neural-retrieval-models" class="level2">
<h2 class="anchored" data-anchor-id="neural-retrieval-models"><strong>Neural retrieval models</strong></h2>
<p>Neural retrieval modelはrepresentation based modelとinteraction based modelの2つに分類できる。</p>
<p>Representation based modelはQueryとDocument両方とも分散表現に変換し、分散表現の類似度で相関性を測っている。最近の研究は以下のことにフォーカスしている：</p>
<ol type="1">
<li>より良い前学習するタスクやモデルのアーキテクチャを開発する</li>
<li>Multi-Vectorでより分散表現の表現性能を向上させる</li>
<li>よりよいNegative sample手法を開発する</li>
<li>ドメイン横断での汎化性能を向上させる</li>
</ol>
<p>Interaction based modelだと、QueryとDocumentを一緒に処理するため、精度が高い。一方、計算コストが高いため、Rerankerとして使われることが多い。それに関する研究は以下のものがある：</p>
<ol type="1">
<li>Interaction based modelを蒸留し、Representation based modelとして使う。</li>
<li>Interactionを最後にさせること。</li>
</ol>
</section>
<section id="prompt-based-query-generation" class="level2">
<h2 class="anchored" data-anchor-id="prompt-based-query-generation">Prompt-based query generation</h2>
<p>UPR：直接LLMSを使ってRerankを行う</p>
<p>InPars：GPT3でQueryを生成し、T5のRerankerを学習する</p>
</section>
<section id="retrievers-with-late-interactions" class="level2">
<h2 class="anchored" data-anchor-id="retrievers-with-late-interactions">Retrievers with late interactions</h2>
<p>Dual encoder modelの効率が良いが、QueryとDocumentの相互作用は最後の内積のみであるため、性能が弱い。ColBERTとSPLADEは最後のToken-level interactionを使ったため、計算コストを少し犠牲して性能を向上させた。</p>
</section>
</section>
<section id="conclution-and-discussions" class="level1">
<h1>6 Conclution and discussions</h1>
<p>この論文はPromptagatorを提案した。一方、未解決の問題はいかのように存在する：</p>
<ol type="1">
<li>効率的に生成したデータを利用する手法が必要。</li>
<li>PromptとRetrieverの性能との関係。</li>
<li>LLMの情報を小さいモデルを転移すること。</li>
</ol>


</section>

 ]]></description>
  <category>NLP</category>
  <category>Information_retrieval</category>
  <category>paper</category>
  <guid>https://jiang.jp/posts/20230429_promptagator/index.html</guid>
  <pubDate>Fri, 28 Apr 2023 15:00:00 GMT</pubDate>
</item>
<item>
  <title>InPars 論文解読</title>
  <link>https://jiang.jp/posts/20230428_inpars/index.html</link>
  <description><![CDATA[ 



<p>論文URL：<a href="https://arxiv.org/abs/2202.05144" class="uri">https://arxiv.org/abs/2202.05144</a></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1. Introduction</h2>
<p>LLMは高性能を誇るものの、情報検索（IR）への応用が制限されている理由として、大規模な計算量が必要であることと、コストが高いことが挙げられる。GPT-3の埋め込みサービスを利用する場合、すべてのテキストを少なくとも1回は処理する必要があり、件数が多い場合、コストが膨大になることが問題となる。</p>
<p>また、学習データにも課題が存在し、現存するデータが商用利用に適さないものが多く、また、既存のデータを用いて学習したモデルが他の領域に汎用性を持たないという問題がある。</p>
<p>この論文では、検索推論にLLMを直接使用するのではなく、LLMを用いて擬似データ（Pseudo data）を生成し、そのデータを使ってランキングモデルを学習する手法を提案している。</p>
</section>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">2. Related work</h2>
<p>これまで情報検索（IR）領域におけるデータ生成の研究では、BM25を用いて類似度が高いドキュメントをペアとしてモデルを学習する研究が存在する。また、教師ありのモデルでテキストからQueryを生成し、それとテキストのペアを学習データとする研究もあった。</p>
<p>この研究の特徴として、モデルを学習させなく、LLMからFew-shotでデータを生成したこと（ちょっと新奇性が足りていない気がする）。</p>
</section>
<section id="our-method-inpars" class="level2">
<h2 class="anchored" data-anchor-id="our-method-inpars">3. Our Method: InPars</h2>
<p>以下はInParsのステップ：</p>
<ol type="1">
<li><p>複数のドキュメントとクエリーのペアが存在すると仮定する。それを例として、Few-shotでLLMでドキュメントに関連するクエリーを生成してもらう。(</p>
<p>詳細は4.2で紹介する）</p></li>
<li><p>生成したドキュメントとクエリーのペアについて、LLMが出力する際に出している結果のLog probabilityが上位のものを選択する。ちなみに、このステップが大きく精度を改善した。</p></li>
<li><p>生成したqとdのペアを学習データとしてRerankのモデルをFine-tuningする。（詳細は4.3で紹介する）</p></li>
</ol>
<p><img src="https://jiang.jp/posts/20230428_inpars/images/paste-1.png" class="img-fluid"></p>
</section>
<section id="experimental-setup" class="level2">
<h2 class="anchored" data-anchor-id="experimental-setup">4. Experimental Setup</h2>
<section id="datasets" class="level3">
<h3 class="anchored" data-anchor-id="datasets">4.1 Datasets</h3>
<p>今回使用したデータセットは以下：</p>
<ul>
<li><p>MS MARCO：Microsoftが出したBingの実際のユーザログをベースとした大型データセット。880万のドキュメントと50万のドキュメントとクエリーのペアがある。各クエリーが平均的に1ドキュメントに対応する。</p></li>
<li><p>TREC-DL：MS MARCOと同じドキュメントを持っているが、クエリーは54件のみである。また、各クエリーについてアノテーションしたドキュメントが多い。</p></li>
<li><p>Robust04：新聞領域のデータセット。52万のドキュメントがあり、249クエリーがある。各クエリーに対して、平均的に1250件のドキュメントをアノテーションした。</p></li>
<li><p>Natural Questions：260万件のWikipediaテキストをベースとしたQuestion Answer データセット。QuestionはGoogleの検索エンジンのログから作ったもの。</p></li>
<li><p>TREC-COVID：コロナの情報に関するデータセット</p></li>
</ul>
</section>
<section id="training-data-generation" class="level3">
<h3 class="anchored" data-anchor-id="training-data-generation">4.2 Training Data Generation</h3>
<p>各学習データは（Query, Positive document, Negative document）のTripleによって構成させる。その生成のステップは以下：</p>
<ul>
<li><p>10万のドキュメントをサンプリングし、GPT3のCurieでQueryを生成させる。</p></li>
<li><p>最終的にLog probabilityが上位の1万件のペアのみ学習データとして使う。</p></li>
<li><p>BM25で検索した1000件の中でランダムに1件を抽出し、それをNegative Documentとする。（このやり方で多くのノイズを入れてしまうのでは？）</p></li>
</ul>
<p>以下は２点の補足：</p>
<ul>
<li><p>生成する際に温度とTopーPのパラメータ設定は結果に有意の影響しない。</p></li>
<li><p>長さが300文字のドキュメントは捨てられる。</p></li>
</ul>
<p>Query生成する際にPromptの書き方は2つを利用した（Figure２)：</p>
<ol type="1">
<li>一般方法（Vanilla)：MS MARCOからランダムに3つのデータを抽出し、それを例として、FewーshotでQueryを生成させる。</li>
<li>GBQ（Guided by Bad Questions）：一般方法と同じように、MS MARCOからランダムに3つのデータを抽出する。しかし、MS MARCOのQueryが漠然すぎるため、それをBad questionとして提示する。Documentを読んで、より関連するGood Questionを手動で作った。（Document, Good Question, Bad Question)で例を提示し、生成したGood QuestionをQueryとする。</li>
</ol>
<p><img src="https://jiang.jp/posts/20230428_inpars/images/Untitled.png" class="img-fluid"></p>
</section>
<section id="retrieval-methods" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-methods">4.3 Retrieval Methods</h3>
<p>２段階の検索を採用している。まずBM25で上位1000のドキュメントを取り出す。その次、MonoT5を使ってRerankingをする。</p>
<p>MonoT5はTransformerのEncoderとDecoder両方とも使っているモデルで、Cross-Encoderモデルである。今回の実験では、サイズは220Mと3Bのモデルでテストした。</p>
<p>各データセットにおいて作成された擬似データでMonoT5をFine-tuningした。</p>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">5 Results</h2>
<p><img src="https://jiang.jp/posts/20230428_inpars/images/Untitled 1.png" class="img-fluid"></p>
<p>7,8行目を見ると、BM25やContriever等の以前のUnsupervised結果より優れていることがわかる。また、16行目はMS MARCOでFine-tuningした後さらに擬似データでFine-tuningした結果。幾つかのデータセットで単純にMS MARCOでFine-tuningするより良い結果が出ている。</p>
</section>
<section id="ablation-study-and-analysis" class="level2">
<h2 class="anchored" data-anchor-id="ablation-study-and-analysis">6 Ablation Study and Analysis</h2>
<p>6.1 Prompt Selection and Source Corpus</p>
<p>比較対象が混乱のため、何が言いたいかがわからなかった。</p>
<p>6.2 Model Size Impact on IR Metrics</p>
<p>当たり前だけど、Questionを生成するモデルのサイズが大きいほど結果がよくなる。</p>
<p>6.3 Filtering by the Most Likely Questions</p>
<p>Top1万件のデータを利用することにより精度が向上した。</p>
<p>6.4 Was GPT-3 Trained on Supervised IR Data?</p>
<p>生成したQuestionがどのぐらい実際のMS MARCOデータにあるかで、GPT3の学習データにMS MARCOのデータを含まれているかを検証した。結果としては多くても10％前後のため影響がない。（検証のやり方が正しいかが疑問）</p>
</section>
<section id="conclusion-and-future-work" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-and-future-work">7 Conclusion and Future Work</h2>
<p>本文は、LLMを利用してQueryを生成して、生成したQueryとドキュメントのペアを学習データでモデルを学習する手法を提案した。</p>
<p>今後の改善点としては、</p>
<ol type="1">
<li>擬似データでDense RetrieverをFine-tuningする（今回はRerankerのみ）</li>
<li>データを生成する際に作った”BAD question”をNegative exampleとして使用する</li>
<li>擬似データの数を増やす</li>
<li>（Query, Document)のペアを探すもっと良い手法を開発する</li>
</ol>


</section>

 ]]></description>
  <category>NLP</category>
  <category>Information_retrieval</category>
  <category>paper</category>
  <guid>https://jiang.jp/posts/20230428_inpars/index.html</guid>
  <pubDate>Fri, 28 Apr 2023 15:00:00 GMT</pubDate>
</item>
</channel>
</rss>
